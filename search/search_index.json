{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#triton-augment","title":"Triton-Augment","text":"<p>GPU-Accelerated Image Augmentation with Kernel Fusion</p> <p> </p> <p>Triton-Augment is a high-performance image augmentation library that leverages OpenAI Triton to fuse common transform operations, providing significant speedups over standard PyTorch implementations.</p> <p>Key Idea: Fuse multiple GPU operations into a single kernel \u2192 eliminate intermediate memory transfers \u2192 faster augmentation.</p> <pre><code># Traditional (torchvision Compose): 7 kernel launches\ncrop \u2192 flip \u2192 brightness \u2192 contrast \u2192 saturation \u2192 grayscale \u2192 normalize\n\n# Triton-Augment Ultimate Fusion: 1 kernel launch \ud83d\ude80\n[crop + flip + brightness + contrast + saturation + grayscale + normalize]\n</code></pre>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>One Kernel, All Operations: Fuse crop, flip, color jitter, grayscale, and normalize in a single kernel - significantly faster, scales with image size! \ud83d\ude80</li> <li>Different Parameters Per Sample: Each image in batch gets different random augmentations (not just batch-wide)</li> <li>Transform &amp; Functional APIs: Random parameters (transforms) or fixed parameters (functional) - your choice</li> <li>Zero Memory Overhead: No intermediate buffers between operations</li> <li>Float16 Ready: ~1.3x speedup on large images + 50% memory savings</li> <li>Drop-in Replacement: torchvision-like API, easy migration</li> <li>Auto-Tuning: Optional performance optimization for your GPU</li> </ul>"},{"location":"#quick-start","title":"\ud83d\udce6 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install triton-augment\n</code></pre> <p>Requirements: Python 3.8+, PyTorch 2.0+, CUDA-capable GPU</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Recommended: Ultimate Fusion \ud83d\ude80</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Create batch of images on GPU\nimages = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Replace torchvision Compose (7 kernel launches)\n# With Triton-Augment (1 kernel launch - significantly faster!)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    random_grayscale_p=0.1,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # \ud83d\ude80 Single kernel for entire pipeline!\n</code></pre> <p>Need only some operations? Use <code>TritonFusedAugment</code> with default/no-op values, or use specialized APIs (they all use the same fused kernel internally):</p> <pre><code># Option 1: Ultimate API with partial operations (set unused to 0/default)\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    horizontal_flip_p=0.0,  # No flip\n    brightness=0.0,         # No brightness\n    saturation=0.2,         # Only saturation\n    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n)\n\n# Option 2: Specialized APIs (convenience wrappers, same kernel internally)\ncolor_only = ta.TritonColorJitterNormalize(brightness=0.2, saturation=0.2, ...)\ngeo_only = ta.TritonRandomCropFlip(size=112, horizontal_flip_p=0.5)\n</code></pre> <p>\u2192 More Examples</p>"},{"location":"#input-requirements","title":"\u26a0\ufe0f Input Requirements","text":"<ul> <li>Range: Images must be in <code>[0, 1]</code> range (e.g., use <code>torchvision.transforms.ToTensor()</code>)</li> <li>Device: GPU (CUDA) - CPU tensors automatically moved to GPU</li> <li>Shape: <code>(C, H, W)</code> or <code>(N, C, H, W)</code> - 3D tensors automatically batched</li> <li>Dtype: <code>float32</code> or <code>float16</code></li> </ul>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Full documentation: https://triton-augment.readthedocs.io (or see <code>docs/</code> folder)</p> Guide Description Quick Start Get started in 5 minutes with examples Installation Setup and requirements API Reference Complete API documentation for all functions and classes Float16 Support Use half-precision for ~1.3x speedup (large images) and 50% memory savings Contrast Notes Fused kernel uses fast contrast (different from torchvision). See how to get exact torchvision results Auto-Tuning Optional performance optimization for your GPU and data size (disabled by default). Includes cache warm-up guide Batch Behavior Different parameters per sample (default) vs batch-wide parameters. Understanding <code>same_on_batch</code> flag"},{"location":"#performance","title":"\u26a1 Performance","text":""},{"location":"#benchmark-results","title":"Benchmark Results","text":"<p>Real training scenario with random augmentations on Tesla T4 (Google Colab Free Tier):</p> Image Size Batch Crop Size Torchvision Triton Fused Speedup 256\u00d7256 32 224\u00d7224 2.48 ms 0.56 ms 4.5x 256\u00d7256 64 224\u00d7224 4.51 ms 0.69 ms 6.5x 600\u00d7600 32 512\u00d7512 11.82 ms 1.26 ms 9.4x 1280\u00d71280 32 1024\u00d71024 48.91 ms 4.07 ms 12.0x <p>Average Speedup: 8.1x \ud83d\ude80</p> <p>Operations: RandomCrop + RandomHorizontalFlip + ColorJitter + RandomGrayscale + Normalize</p> <p>Performance scales with image size \u2014 larger images benefit more from kernel fusion:</p> <p> </p> <p>Speedup advantage increases dramatically for larger images (600\u00d7600+). Triton maintains near-constant runtime while Torchvision scales linearly.</p> <p>\ud83d\udcca Additional Benchmarks (NVIDIA A100 on Google Colab):</p> Image Size Batch Crop Size Torchvision Triton Fused Speedup 256\u00d7256 32 224\u00d7224 0.61 ms 0.44 ms 1.4x 256\u00d7256 64 224\u00d7224 0.93 ms 0.43 ms 2.1x 600\u00d7600 32 512\u00d7512 2.19 ms 0.50 ms 4.4x 1280\u00d71280 32 1024\u00d71024 8.23 ms 0.94 ms 8.7x <p>Average: 4.1x (A100's high memory bandwidth makes torchvision already fast, so relative improvement is smaller)</p> <p>\ud83d\udca1 Why better speedup on T4? Kernel fusion reduces memory bandwidth bottlenecks, which matters more on bandwidth-limited GPUs like T4 (320 GB/s) vs A100 (1,555 GB/s). This means greater benefits on consumer and mid-range hardware.</p>"},{"location":"#run-your-own-benchmarks","title":"Run Your Own Benchmarks","text":"<p>Quick Benchmark (Ultimate Fusion only): <pre><code># Simple, clean table output - easy to run!\npython examples/benchmark.py\n</code></pre></p> <p>Note: Benchmarks use <code>torchvision.transforms.v2</code> (not the legacy v1 API) for comparison.</p> <p>Detailed Benchmark (All operations): <pre><code># Comprehensive analysis with visualizations\npython examples/benchmark_triton.py\n</code></pre></p> <p>\ud83d\udca1 Auto-Tuning: The results above use default configurations. Auto-tuning can provide additional speedup on dedicated GPUs (local workstations, cloud instances). On shared cloud services (Colab, Kaggle), auto-tuning benefits may be limited due to variable GPU utilization. See Auto-Tuning Guide for details.</p>"},{"location":"#when-to-use-triton-augment","title":"\ud83c\udfaf When to Use Triton-Augment?","text":"<p>\ud83d\udca1 Use Triton-Augment + Torchvision together: - Torchvision: Data loading, resize, ToTensor, rotation, affine, etc. - Triton-Augment: Replace supported operations (currently: crop, flip, color jitter, grayscale, normalize; more coming) with fused GPU kernels</p> <p>Best speedup when: - Large images (600\u00d7600+) or large batches - These operations are your bottleneck: crop, flip, brightness, contrast, saturation, normalize</p> <p>Stick with Torchvision only if: - Small images (&lt; 256\u00d7256) on high-end GPUs (A100+) - CPU-only training</p> <p>\ud83d\udca1 TL;DR: Use both! Triton-Augment replaces Torchvision's fusible ops for 8-12x speedup.</p>"},{"location":"#training-integration","title":"\ud83c\udf93 Training Integration","text":"<p>Want to use Triton-Augment in your training pipeline? See the Quick Start Guide for:</p> <ul> <li>Complete training examples (MNIST, CIFAR-10)</li> <li>DataLoader integration patterns</li> <li>Best practices for CPU data loading + GPU augmentation</li> <li>Why this architecture is fast</li> </ul> <p>Quick snippet:</p> <pre><code># Step 1: Load data on CPU with workers\ntrain_loader = DataLoader(..., num_workers=4)\n\n# Step 2: Create GPU augmentation (once)\naugment = ta.TritonFusedAugment(crop_size=28, ...)\n\n# Step 3: Apply in training loop on GPU batches\nfor images, labels in train_loader:\n    images = images.cuda()\n    images = augment(images)  # \ud83d\ude80 1 kernel for all ops!\n    outputs = model(images)\n</code></pre> <p>\u2192 Full Training Guide</p>"},{"location":"#roadmap","title":"\ud83d\udccb Roadmap","text":"<ul> <li>[x] Phase 1: Fused color operations (brightness, contrast, saturation, normalize)</li> <li>[x] Phase 1.5: Grayscale, float16 support, auto-tuning</li> <li>[x] Phase 2: Basic Geometric operations (crop, flip) + Ultimate fusion \ud83d\ude80</li> <li>[ ] Phase 3: Extended operations (resize, rotation, blur, erasing, mixup)</li> </ul> <p>\u2192 Detailed Roadmap</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions welcome! Please see CONTRIBUTING.md for guidelines.</p> <pre><code># Development setup\npip install -e \".[dev]\"\n\n# Useful commands\nmake help        # Show all available commands\nmake test        # Run tests\n</code></pre> <p>\u2192 Complete Contributing Guide</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>Apache License 2.0 - see LICENSE file.</p>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>OpenAI Triton - GPU programming framework</li> <li>PyTorch - Deep learning foundation</li> <li>torchvision - API inspiration</li> </ul>"},{"location":"#author","title":"\ud83d\udc64 Author","text":"<p>Yuhe Zhang</p> <ul> <li>\ud83d\udcbc LinkedIn: Yuhe Zhang</li> <li>\ud83d\udce7 Email: yuhezhang.zju@gmail.com</li> </ul> <p>Research interests: Applied ML, Computer Vision, Efficient Deep Learning, GPU Acceleration</p>"},{"location":"#project","title":"\ud83d\udce7 Project","text":"<ul> <li>Issues and feature requests: GitHub Issues</li> <li>PyPI Package: pypi.org/project/triton-augment</li> </ul> <p>\u2b50 If you find this library useful, please consider starring the repo! \u2b50</p>"},{"location":"DOCUMENTATION/","title":"Documentation Guide","text":"<p>This document explains how to build, preview, and deploy the Triton-Augment documentation.</p>"},{"location":"DOCUMENTATION/#structure","title":"Structure","text":"<pre><code>README.md                 # Home page (shared with GitHub)\ndocs/\n\u251c\u2500\u2500 installation.md       # Installation guide\n\u251c\u2500\u2500 quickstart.md        # Quick start tutorial\n\u251c\u2500\u2500 float16.md           # Float16 support\n\u251c\u2500\u2500 batch-behavior.md    # Batch behavior guide\n\u251c\u2500\u2500 contrast.md          # Contrast implementation details\n\u251c\u2500\u2500 auto-tuning.md       # Auto-tuning guide\n\u251c\u2500\u2500 api-reference.md     # API reference\n\u251c\u2500\u2500 requirements.txt     # Docs dependencies\n\u2514\u2500\u2500 stylesheets/\n    \u2514\u2500\u2500 extra.css        # Custom CSS\n</code></pre> <p>Note: The home page uses the root <code>README.md</code> to avoid duplication between GitHub and documentation site.</p>"},{"location":"DOCUMENTATION/#building-the-documentation","title":"Building the Documentation","text":""},{"location":"DOCUMENTATION/#install-dependencies","title":"Install Dependencies","text":"<pre><code># From project root\npip install -r docs/requirements.txt\n</code></pre>"},{"location":"DOCUMENTATION/#preview-locally","title":"Preview Locally","text":"<pre><code># Start local server (with auto-reload)\nmkdocs serve\n\n# Open in browser\n# http://127.0.0.1:8000\n</code></pre> <p>The server will auto-reload when you edit documentation files.</p>"},{"location":"DOCUMENTATION/#build-static-site","title":"Build Static Site","text":"<pre><code># Build HTML files\nmkdocs build\n\n# Output is in site/ directory\n# Open site/index.html in browser to preview\n</code></pre>"},{"location":"DOCUMENTATION/#deploying-to-github-pages","title":"Deploying to GitHub Pages","text":""},{"location":"DOCUMENTATION/#option-1-automatic-recommended","title":"Option 1: Automatic (Recommended)","text":"<pre><code># Build and deploy in one command\nmkdocs gh-deploy\n\n# This will:\n# 1. Build the site\n# 2. Push to gh-pages branch\n# 3. GitHub will serve it automatically\n</code></pre> <p>Your docs will be available at: <code>https://&lt;username&gt;.github.io/&lt;repo&gt;/</code></p>"},{"location":"DOCUMENTATION/#option-2-manual","title":"Option 2: Manual","text":"<pre><code># Build locally\nmkdocs build\n\n# Copy site/ contents to your web server\n# Or commit site/ to gh-pages branch manually\n</code></pre>"},{"location":"DOCUMENTATION/#documentation-workflow","title":"Documentation Workflow","text":""},{"location":"DOCUMENTATION/#adding-a-new-page","title":"Adding a New Page","text":"<ol> <li>Create new <code>.md</code> file in <code>docs/</code></li> <li>Add entry to <code>mkdocs.yml</code> navigation</li> <li>Preview with <code>mkdocs serve</code></li> <li>Commit and deploy</li> </ol> <p>Example:</p> <pre><code># mkdocs.yml\nnav:\n  - Home: index.md\n  - Getting Started:\n      - Installation: installation.md\n      - Quick Start: quickstart.md\n      - Your New Page: new-page.md  # Add here\n</code></pre>"},{"location":"DOCUMENTATION/#updating-api-reference","title":"Updating API Reference","text":"<p>The API reference (<code>docs/api-reference.md</code>) is manually written. To add new functions:</p> <ol> <li>Add function documentation to <code>api-reference.md</code></li> <li>Follow the existing format (code examples + parameter descriptions)</li> <li>Add cross-references to relevant guides</li> </ol> <p>For auto-generated API docs (future): - Use <code>mkdocstrings</code> plugin (already configured) - Add docstring references: <code>triton_augment.functional</code></p>"},{"location":"DOCUMENTATION/#writing-style","title":"Writing Style","text":"<ul> <li>Use code examples: Show, don't just tell</li> <li>Add admonitions: <code>!!! note</code>, <code>!!! warning</code>, <code>!!! tip</code></li> <li>Include cross-references: Link related pages</li> <li>Keep it concise: Short paragraphs, clear headings</li> <li>Use tables: For comparisons and parameter lists</li> </ul>"},{"location":"DOCUMENTATION/#markdown-extensions","title":"Markdown Extensions","text":"<p>Available extensions (configured in <code>mkdocs.yml</code>):</p> <pre><code># Admonitions (call-out boxes)\n!!! note \"Optional Title\"\n    This is a note\n\n!!! warning\n    This is a warning\n\n# Code blocks with syntax highlighting\n```python\nimport triton_augment as ta\n</code></pre>"},{"location":"DOCUMENTATION/#tabbed-content","title":"Tabbed content","text":"Tab 1Tab 2 <p>Content 1</p> <p>Content 2</p>"},{"location":"DOCUMENTATION/#tables","title":"Tables","text":"Header 1 Header 2 Cell 1 Cell 2 <pre><code>## Theme Customization\n\n### Colors\n\nEdit `mkdocs.yml`:\n\n```yaml\ntheme:\n  palette:\n    primary: indigo  # Change to: red, pink, purple, etc.\n    accent: indigo\n</code></pre>"},{"location":"DOCUMENTATION/#logo-and-favicon","title":"Logo and Favicon","text":"<p>Add files: - <code>docs/images/logo.png</code> - <code>docs/images/favicon.ico</code></p> <p>Update <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  logo: images/logo.png\n  favicon: images/favicon.ico\n</code></pre>"},{"location":"DOCUMENTATION/#custom-css","title":"Custom CSS","text":"<p>Edit <code>docs/stylesheets/extra.css</code> for custom styles.</p>"},{"location":"DOCUMENTATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DOCUMENTATION/#build-errors","title":"Build Errors","text":"<pre><code># Clear cache\nrm -rf site/\n\n# Rebuild\nmkdocs build\n</code></pre>"},{"location":"DOCUMENTATION/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code># Reinstall\npip install -r docs/requirements.txt --upgrade\n</code></pre>"},{"location":"DOCUMENTATION/#gh-deploy-issues","title":"gh-deploy Issues","text":"<pre><code># Check git status\ngit status\n\n# Ensure you're on main branch\ngit checkout main\n\n# Try force deploy\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"DOCUMENTATION/#best-practices","title":"Best Practices","text":"<ol> <li>Test locally before deploying (<code>mkdocs serve</code>)</li> <li>Keep README short - link to full docs</li> <li>Update docs when adding features</li> <li>Use examples - code speaks louder than words</li> <li>Cross-reference - link related pages</li> <li>Version docs - note when features were added</li> </ol>"},{"location":"DOCUMENTATION/#continuous-integration-optional","title":"Continuous Integration (Optional)","text":"<p>Add GitHub Actions to auto-deploy docs:</p> <pre><code># .github/workflows/docs.yml\nname: Deploy Documentation\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.x\n      - run: pip install -r docs/requirements.txt\n      - run: mkdocs gh-deploy --force\n</code></pre>"},{"location":"DOCUMENTATION/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>mkdocstrings</li> <li>Markdown Guide</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete reference for all Triton-Augment operations.</p>"},{"location":"api-reference/#transform-classes","title":"Transform Classes","text":"<p>Transform classes provide stateful, random augmentations similar to torchvision.</p>"},{"location":"api-reference/#tritoncolorjitternormalize","title":"TritonColorJitterNormalize","text":"<p>Fused color jitter and normalization in a single kernel. Recommended for best performance.</p> <pre><code>ta.TritonColorJitterNormalize(\n    brightness=0,\n    contrast=0,\n    saturation=0,\n    random_grayscale_p=0.0,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>brightness</code> (float or tuple): Brightness jitter range. If float, range is <code>[max(0, 1-brightness), 1+brightness]</code>. Default: 0</li> <li><code>contrast</code> (float or tuple): Contrast jitter range. Default: 0</li> <li><code>saturation</code> (float or tuple): Saturation jitter range. Default: 0</li> <li><code>random_grayscale_p</code> (float): Probability of converting to grayscale. Default: 0.0</li> <li><code>mean</code> (tuple): Normalization means for R, G, B channels</li> <li><code>std</code> (tuple): Normalization stds for R, G, B channels</li> </ul> <p>Note: Uses fast contrast (centered scaling), not torchvision-exact. See Contrast Guide.</p>"},{"location":"api-reference/#tritoncolorjitter","title":"TritonColorJitter","text":"<p>Randomly change brightness, contrast, and saturation.</p> <pre><code>ta.TritonColorJitter(\n    brightness=0,\n    contrast=0,\n    saturation=0\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>brightness</code> (float or tuple): Brightness jitter range</li> <li><code>contrast</code> (float or tuple): Contrast jitter range (uses exact torchvision algorithm)</li> <li><code>saturation</code> (float or tuple): Saturation jitter range</li> </ul> <p>Note: Applies brightness, contrast, saturation transformations sequentially to match torchvision's behavior. Uses exact torchvision contrast algorithm (blend with mean). For fused operations with better performance, use <code>TritonColorJitterNormalize</code>.</p>"},{"location":"api-reference/#tritonnormalize","title":"TritonNormalize","text":"<p>Normalize image with mean and standard deviation.</p> <pre><code>ta.TritonNormalize(\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>mean</code> (tuple): Per-channel means</li> <li><code>std</code> (tuple): Per-channel standard deviations</li> </ul>"},{"location":"api-reference/#tritongrayscale","title":"TritonGrayscale","text":"<p>Convert RGB image to grayscale.</p> <pre><code>ta.TritonGrayscale(num_output_channels=1)\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_output_channels</code> (int): 1 for single-channel grayscale, 3 to replicate across channels</li> </ul>"},{"location":"api-reference/#tritonrandomgrayscale","title":"TritonRandomGrayscale","text":"<p>Randomly convert image to grayscale.</p> <pre><code>ta.TritonRandomGrayscale(p=0.1, num_output_channels=3)\n</code></pre> <p>Parameters:</p> <ul> <li><code>p</code> (float): Probability of grayscale conversion</li> <li><code>num_output_channels</code> (int): 1 or 3</li> </ul>"},{"location":"api-reference/#functional-api","title":"Functional API","text":"<p>Low-level functional interface for fine-grained control.</p>"},{"location":"api-reference/#fused_color_normalize","title":"fused_color_normalize","text":"<p>Fuse multiple operations into a single kernel.</p> <pre><code>ta.fused_color_normalize(\n    image,\n    brightness_factor=1.0,\n    contrast_factor=1.0,\n    saturation_factor=1.0,\n    random_grayscale_p=0.0,\n    mean=None,\n    std=None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>brightness_factor</code> (float): Brightness multiplier (1.0 = no change)</li> <li><code>contrast_factor</code> (float): Contrast multiplier (1.0 = no change, uses fast contrast)</li> <li><code>saturation_factor</code> (float): Saturation multiplier (1.0 = no change)</li> <li><code>random_grayscale_p</code> (float): Probability of grayscale</li> <li><code>mean</code> (tuple): Normalization means (None = skip normalization)</li> <li><code>std</code> (tuple): Normalization stds (None = skip normalization)</li> </ul> <p>Returns: Transformed tensor of same shape</p>"},{"location":"api-reference/#adjust_brightness","title":"adjust_brightness","text":"<p>Apply brightness adjustment.</p> <pre><code>ta.adjust_brightness(image, brightness_factor)\n</code></pre> <p>Formula: <code>output = input * brightness_factor</code></p> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>brightness_factor</code> (float): Brightness multiplier</li> </ul>"},{"location":"api-reference/#adjust_contrast","title":"adjust_contrast","text":"<p>Apply contrast adjustment (torchvision-exact).</p> <pre><code>ta.adjust_contrast(image, contrast_factor)\n</code></pre> <p>Formula: <code>output = input * factor + mean(grayscale) * (1 - factor)</code></p> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>contrast_factor</code> (float): Contrast multiplier</li> </ul>"},{"location":"api-reference/#adjust_contrast_fast","title":"adjust_contrast_fast","text":"<p>Apply fast contrast adjustment.</p> <pre><code>ta.adjust_contrast_fast(image, contrast_factor)\n</code></pre> <p>Formula: <code>output = (input - 0.5) * factor + 0.5</code></p> <p>Note: Same as NVIDIA DALI. Faster and fusible, but not torchvision-exact.</p>"},{"location":"api-reference/#adjust_saturation","title":"adjust_saturation","text":"<p>Apply saturation adjustment.</p> <pre><code>ta.adjust_saturation(image, saturation_factor)\n</code></pre> <p>Formula: <code>output = input * factor + grayscale * (1 - factor)</code></p> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>saturation_factor</code> (float): Saturation multiplier (0.0 = grayscale, 1.0 = original)</li> </ul>"},{"location":"api-reference/#normalize","title":"normalize","text":"<p>Apply per-channel normalization.</p> <pre><code>ta.normalize(image, mean, std)\n</code></pre> <p>Formula: <code>output[c] = (input[c] - mean[c]) / std[c]</code></p> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>mean</code> (tuple): Per-channel means</li> <li><code>std</code> (tuple): Per-channel stds</li> </ul>"},{"location":"api-reference/#rgb_to_grayscale","title":"rgb_to_grayscale","text":"<p>Convert RGB to grayscale.</p> <pre><code>ta.rgb_to_grayscale(image, num_output_channels=1)\n</code></pre> <p>Formula: <code>gray = 0.2989*R + 0.587*G + 0.114*B</code></p> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): RGB tensor <code>(N, 3, H, W)</code></li> <li><code>num_output_channels</code> (int): 1 or 3</li> </ul>"},{"location":"api-reference/#random_grayscale","title":"random_grayscale","text":"<p>Randomly convert to grayscale.</p> <pre><code>ta.random_grayscale(image, p=0.1, num_output_channels=3)\n</code></pre> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor</li> <li><code>p</code> (float): Probability of conversion</li> <li><code>num_output_channels</code> (int): 1 or 3</li> </ul>"},{"location":"api-reference/#geometric-operations","title":"Geometric Operations","text":""},{"location":"api-reference/#crop","title":"crop","text":"<p>Crop a rectangular region from the image.</p> <pre><code>ta.crop(image, top, left, height, width)\n</code></pre> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>top</code> (int): Top coordinate of crop region</li> <li><code>left</code> (int): Left coordinate of crop region</li> <li><code>height</code> (int): Height of crop region</li> <li><code>width</code> (int): Width of crop region</li> </ul> <p>Returns: Cropped tensor of shape <code>(N, C, height, width)</code></p> <p>Example:</p> <pre><code>img = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = ta.crop(img, top=20, left=30, height=112, width=112)\n# cropped.shape = (4, 3, 112, 112)\n</code></pre>"},{"location":"api-reference/#center_crop","title":"center_crop","text":"<p>Crop the center region from the image.</p> <pre><code>ta.center_crop(image, size)\n</code></pre> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>size</code> (int or tuple): Desired output size. If int, output is square <code>(size, size)</code></li> </ul> <p>Returns: Center-cropped tensor</p> <p>Example:</p> <pre><code>img = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = ta.center_crop(img, 112)\n# cropped.shape = (4, 3, 112, 112), centered\n</code></pre>"},{"location":"api-reference/#horizontal_flip","title":"horizontal_flip","text":"<p>Flip image horizontally (left-to-right).</p> <pre><code>ta.horizontal_flip(image)\n</code></pre> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> </ul> <p>Returns: Horizontally flipped tensor of same shape</p> <p>Example:</p> <pre><code>img = torch.rand(4, 3, 224, 224, device='cuda')\nflipped = ta.horizontal_flip(img)\n</code></pre>"},{"location":"api-reference/#fused_crop_flip","title":"fused_crop_flip","text":"<p>Fused operation: Combine crop + horizontal flip in a single kernel.</p> <pre><code>ta.fused_crop_flip(image, top, left, height, width, flip_horizontal=True)\n</code></pre> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>top</code>, <code>left</code>, <code>height</code>, <code>width</code>: Crop parameters</li> <li><code>flip_horizontal</code> (bool): Whether to apply horizontal flip</li> </ul> <p>Returns: Cropped and optionally flipped tensor</p> <p>Performance: ~1.5-2x faster than sequential crop + flip</p> <p>Example:</p> <pre><code># Fused (single kernel - FAST)\nresult = ta.fused_crop_flip(img, 20, 30, 112, 112, flip_horizontal=True)\n\n# vs Sequential (2 kernels)\nresult = ta.crop(img, 20, 30, 112, 112)\nresult = ta.horizontal_flip(result)\n</code></pre>"},{"location":"api-reference/#fused_augment","title":"fused_augment","text":"<p>THE ULTIMATE: Fuse ALL augmentations (geometric + pixel) in ONE kernel! \ud83d\ude80</p> <pre><code>ta.fused_augment(\n    image,\n    top, left, height, width,\n    flip_horizontal=False,\n    brightness_factor=1.0,\n    contrast_factor=1.0,\n    saturation_factor=1.0,\n    mean=None,\n    std=None\n)\n</code></pre> <p>Operations fused: 1. Geometric: Crop + Horizontal Flip 2. Pixel: Brightness + Contrast (fast) + Saturation + Normalize</p> <p>Parameters:</p> <ul> <li><code>image</code> (torch.Tensor): Input tensor <code>(N, C, H, W)</code> on CUDA</li> <li><code>top</code>, <code>left</code>, <code>height</code>, <code>width</code>: Crop parameters</li> <li><code>flip_horizontal</code> (bool): Whether to flip horizontally</li> <li><code>brightness_factor</code> (float): Brightness multiplier (1.0 = no change)</li> <li><code>contrast_factor</code> (float): Contrast multiplier (uses fast contrast)</li> <li><code>saturation_factor</code> (float): Saturation multiplier (1.0 = no change)</li> <li><code>mean</code>, <code>std</code> (tuple): Normalization parameters (None = skip)</li> </ul> <p>Returns: Fully augmented tensor of shape <code>(N, C, height, width)</code></p> <p>Performance: Up to 12x faster on large images (8.1x average on Tesla T4, scales dramatically with image size)</p> <p>Example:</p> <pre><code># Single kernel launch for ALL operations!\nresult = ta.fused_augment(\n    img,\n    top=20, left=30, height=112, width=112,\n    flip_horizontal=True,\n    brightness_factor=1.2,\n    contrast_factor=1.1,\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# vs torchvision Compose (6 kernel launches)\nresult = tvF.crop(img, 20, 30, 112, 112)\nresult = tvF.horizontal_flip(result)\nresult = tvF.adjust_brightness(result, 1.2)\nresult = tvF.adjust_contrast(result, 1.1)\nresult = tvF.adjust_saturation(result, 0.9)\nresult = tvF.normalize(result, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n</code></pre> <p>Note: Uses fast contrast (centered scaling), not torchvision-exact. See Contrast Guide.</p>"},{"location":"api-reference/#geometric-transform-classes","title":"Geometric Transform Classes","text":""},{"location":"api-reference/#tritonrandomcrop","title":"TritonRandomCrop","text":"<p>Randomly crop a portion of the image.</p> <pre><code>ta.TritonRandomCrop(size)\n</code></pre> <p>Parameters:</p> <ul> <li><code>size</code> (int or tuple): Desired output size. If int, output is square</li> </ul> <p>Example:</p> <pre><code>transform = ta.TritonRandomCrop(112)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = transform(img)  # Random 112x112 crop\n</code></pre>"},{"location":"api-reference/#tritoncentercrop","title":"TritonCenterCrop","text":"<p>Crop the center of the image.</p> <pre><code>ta.TritonCenterCrop(size)\n</code></pre> <p>Parameters:</p> <ul> <li><code>size</code> (int or tuple): Desired output size</li> </ul> <p>Example:</p> <pre><code>transform = ta.TritonCenterCrop(112)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = transform(img)  # Center 112x112 crop\n</code></pre>"},{"location":"api-reference/#tritonrandomhorizontalflip","title":"TritonRandomHorizontalFlip","text":"<p>Randomly flip image horizontally.</p> <pre><code>ta.TritonRandomHorizontalFlip(p=0.5)\n</code></pre> <p>Parameters:</p> <ul> <li><code>p</code> (float): Probability of flipping (default: 0.5)</li> </ul> <p>Example:</p> <pre><code>transform = ta.TritonRandomHorizontalFlip(p=0.5)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nresult = transform(img)  # 50% chance of flip\n</code></pre>"},{"location":"api-reference/#tritonrandomcropflip","title":"TritonRandomCropFlip","text":"<p>Fused transform: Random crop + random flip in one kernel.</p> <pre><code>ta.TritonRandomCropFlip(size, horizontal_flip_p=0.5)\n</code></pre> <p>Parameters:</p> <ul> <li><code>size</code> (int or tuple): Desired output size</li> <li><code>horizontal_flip_p</code> (float): Probability of horizontal flip</li> </ul> <p>Performance: ~1.5-2x faster than sequential transforms</p> <p>Example:</p> <pre><code># Fused (single kernel)\ntransform = ta.TritonRandomCropFlip(112, horizontal_flip_p=0.5)\n\n# vs Sequential (2 kernels)\ntransform = transforms.Compose([\n    ta.TritonRandomCrop(112),\n    ta.TritonRandomHorizontalFlip(0.5)\n])\n</code></pre>"},{"location":"api-reference/#tritonfusedaugment","title":"TritonFusedAugment","text":"<p>THE ULTIMATE TRANSFORM: All augmentations in ONE kernel! \ud83d\ude80</p> <pre><code>ta.TritonFusedAugment(\n    crop_size,\n    horizontal_flip_p=0.0,\n    brightness=0,\n    contrast=0,\n    saturation=0,\n    random_grayscale_p=0.0,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>crop_size</code> (int or tuple): Crop size</li> <li><code>horizontal_flip_p</code> (float): Probability of horizontal flip (default: 0.0, no flip)</li> <li><code>brightness</code>, <code>contrast</code>, <code>saturation</code> (float or tuple): Color jitter ranges</li> <li><code>random_grayscale_p</code> (float): Probability of converting to grayscale (default: 0.0, no grayscale)</li> <li><code>mean</code>, <code>std</code> (tuple): Normalization parameters</li> </ul> <p>Performance: Up to 12x faster on large images (8.1x average on Tesla T4, scales dramatically with image size)</p> <p>Design Note: All augmentation parameters default to 0 (no augmentation) for consistency and predictability. Users explicitly opt-in to each augmentation they want to use.</p> <p>Example:</p> <pre><code># Replace torchvision Compose (6 kernel launches)\nold_transform = transforms.Compose([\n    transforms.RandomCrop(112),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\n# With TritonFusedAugment (1 kernel launch - significantly faster!)\nnew_transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\nimg = torch.rand(32, 3, 224, 224, device='cuda')\nresult = new_transform(img)  # Single kernel launch!\n</code></pre> <p>Note: Uses fast contrast. See Contrast Guide.</p>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/#enable_autotune-disable_autotune","title":"enable_autotune / disable_autotune","text":"<p>Control auto-tuning behavior.</p> <pre><code>ta.enable_autotune()   # Enable for optimal performance\nta.disable_autotune()  # Disable for faster startup (default)\n</code></pre>"},{"location":"api-reference/#is_autotune_enabled","title":"is_autotune_enabled","text":"<p>Check auto-tuning status.</p> <pre><code>enabled = ta.is_autotune_enabled()\n</code></pre> <p>Returns: <code>bool</code> - True if auto-tuning is enabled</p>"},{"location":"api-reference/#warmup_cache","title":"warmup_cache","text":"<p>Pre-compile kernels for specified sizes.</p> <pre><code>ta.warmup_cache(\n    batch_sizes=(32, 64),\n    image_sizes=(224, 256, 512)\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>batch_sizes</code> (tuple): Batch sizes to warm up</li> <li><code>image_sizes</code> (tuple): Image sizes (height = width) to warm up</li> </ul> <p>Note: Only useful when auto-tuning is enabled. See Auto-Tuning Guide.</p>"},{"location":"api-reference/#type-annotations","title":"Type Annotations","text":"<p>All functions support type hints:</p> <pre><code>from typing import Tuple, Optional\nimport torch\n\ndef fused_color_normalize(\n    image: torch.Tensor,\n    brightness_factor: float = 1.0,\n    contrast_factor: float = 1.0,\n    saturation_factor: float = 1.0,\n    random_grayscale_p: float = 0.0,\n    mean: Optional[Tuple[float, float, float]] = None,\n    std: Optional[Tuple[float, float, float]] = None,\n) -&gt; torch.Tensor:\n    ...\n</code></pre>"},{"location":"api-reference/#input-requirements","title":"Input Requirements","text":"<p>All operations accept:</p> <ul> <li>Device: CUDA (GPU) or CPU - CPU tensors are automatically moved to GPU</li> <li>Shape: <code>(C, H, W)</code> or <code>(N, C, H, W)</code> - 3D tensors are automatically batched</li> <li>Dtype: float32 or float16</li> <li>Range: [0, 1] for color operations (required)</li> </ul> <p>Notes: - After normalization, values can be outside [0, 1] range - 3D tensors <code>(C, H, W)</code> are automatically converted to <code>(1, C, H, W)</code> for processing - CPU tensors are automatically transferred to CUDA for GPU processing</p>"},{"location":"api-reference/#performance-tips","title":"Performance Tips","text":""},{"location":"api-reference/#use-fused-kernel-even-for-partial-operations","title":"Use Fused Kernel Even for Partial Operations","text":"<p>Key insight: Even if you only need a subset of operations, use <code>TritonFusedAugment</code> or <code>F.fused_augment</code> for best performance! Simply set unused operations to no-op values:</p> <pre><code># Example: Only need crop + normalize (no flip, no color jitter)\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    horizontal_flip_p=0.0,      # No flip\n    brightness=0.0,             # No brightness\n    contrast=0.0,               # No contrast\n    saturation=0.0,             # No saturation\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n# Still faster than calling crop() + normalize() separately!\n</code></pre> <p>The fused kernel is optimized to skip operations set to no-op values at compile time.</p>"},{"location":"api-reference/#individual-operations-performance","title":"Individual Operations Performance","text":"<p>Individual Triton operations (e.g., <code>ta.crop()</code>, <code>ta.adjust_brightness()</code>): - Small images/batches: Slightly slower than torchvision (kernel launch overhead) - Large images/batches: Faster than torchvision (better GPU utilization)</p> <p>See benchmark results for detailed performance comparisons.</p> <p>Recommendation: Use <code>TritonFusedAugment</code> for production training, regardless of how many operations you need.</p>"},{"location":"auto-tuning/","title":"Auto-Tuning Guide","text":"<p>Default Behavior</p> <p>Auto-tuning is DISABLED by default for faster startup and good-enough performance. Enable it only if you need the extra performance boost.</p>"},{"location":"auto-tuning/#what-is-auto-tuning","title":"What is Auto-Tuning?","text":"<p>Auto-tuning automatically finds the optimal kernel configuration (block size, warp count, pipeline depth) for your specific GPU and image sizes.</p> <p>When enabled, Triton-Augment auto-tunes the fused kernel (<code>fused_augment</code> / <code>TritonFusedAugment</code>) while simple operations (brightness, saturation, normalize) always use fixed defaults.</p>"},{"location":"auto-tuning/#enabling-auto-tuning","title":"Enabling Auto-Tuning","text":""},{"location":"auto-tuning/#option-1-python-api","title":"Option 1: Python API","text":"<pre><code>import triton_augment as ta\n\n# Enable auto-tuning for optimal performance\nta.enable_autotune()\n\n# Check status\nprint(ta.is_autotune_enabled())  # True\n\n# Disable if needed\nta.disable_autotune()\n</code></pre>"},{"location":"auto-tuning/#option-2-environment-variable","title":"Option 2: Environment Variable","text":"<pre><code>export TRITON_AUGMENT_ENABLE_AUTOTUNE=1\npython train.py\n</code></pre>"},{"location":"auto-tuning/#auto-tuning-details","title":"Auto-Tuning Details","text":""},{"location":"auto-tuning/#auto-tuned-kernel","title":"Auto-Tuned Kernel","text":"<p>The fused kernel (<code>fused_augment</code>) is auto-tuned across these parameters:</p> <ul> <li>BLOCK_SIZE: Number of elements processed per thread block (256, 512, 1024, 2048)</li> <li>num_warps: Thread group size for parallelism (2, 4, 8)</li> <li>num_stages: Memory pipeline depth for memory-compute overlap (2, 3, 4)</li> </ul> <p>Triton tests 12 configurations and caches the fastest one for your specific workload.</p>"},{"location":"auto-tuning/#fixed-kernels","title":"Fixed Kernels","text":"<p>Simple operations use <code>BLOCK_SIZE=1024</code> for optimal performance on most GPUs: - <code>adjust_brightness</code> - <code>adjust_saturation</code> - <code>normalize</code></p> <p>These don't need auto-tuning as they're simple memory-bound operations.</p>"},{"location":"auto-tuning/#how-it-works","title":"How It Works","text":"<ol> <li>First run: Auto-tuning tests 12 configurations and caches the best one (5-10 seconds)</li> <li>Subsequent runs: Uses cached optimal configuration (zero overhead)</li> <li>Per GPU + size: Cache is specific to your GPU model and total elements (N\u00d7C\u00d7H\u00d7W)</li> </ol>"},{"location":"auto-tuning/#cache-warm-up-when-auto-tuning-is-enabled","title":"Cache Warm-Up (When Auto-Tuning is Enabled)","text":"<p>To avoid auto-tuning delays during training, warm up the cache once:</p>"},{"location":"auto-tuning/#cli","title":"CLI","text":"<pre><code># Enable auto-tuning and warm up\nTRITON_AUGMENT_ENABLE_AUTOTUNE=1 python -m triton_augment.warmup \\\n    --batch-sizes 64,128 \\\n    --image-sizes 320,384,640\n</code></pre>"},{"location":"auto-tuning/#python-api","title":"Python API","text":"<pre><code>import triton_augment as ta\n\n# Enable and warm up\nta.enable_autotune()\nta.warmup_cache(\n    batch_sizes=(64, 128),\n    image_sizes=(320, 384, 640)\n)\n</code></pre> <p>Important Notes</p> <ul> <li>Auto-tuning must be ENABLED for warmup to test multiple configs</li> <li>Without auto-tuning, warmup just compiles the default config</li> <li>Auto-tuning is size-specific: A cache for 224\u00d7224 won't help with 320\u00d7320</li> <li>Always use your actual training dimensions</li> </ul>"},{"location":"auto-tuning/#when-auto-tuning-is-disabled-default","title":"When Auto-Tuning is Disabled (Default)","text":"<ul> <li>Uses a single well-tuned default configuration</li> <li>Zero auto-tuning overhead</li> <li>No need to warm up cache</li> <li>Good performance on most GPUs</li> <li>First use still takes ~1-2 seconds to compile (but no testing/selection)</li> </ul>"},{"location":"auto-tuning/#performance-impact","title":"Performance Impact","text":""},{"location":"auto-tuning/#expected-improvements-when-enabled","title":"Expected Improvements (When Enabled)","text":"<p>Auto-tuning typically provides: - 5-15% speedup on most operations - Best gains on:   - Larger images (512\u00d7512+)   - Larger batch sizes (64+)   - Data center GPUs (A100, H100) - Minimal gains on:   - Small images (&lt; 128\u00d7128)   - Small batches (&lt; 16)   - Consumer GPUs (RTX 30xx series)</p>"},{"location":"auto-tuning/#trade-offs","title":"Trade-offs","text":"Aspect Disabled (Default) Enabled First-run speed Fast (~1-2 sec) Slow (~5-10 sec) Steady-state performance Good (95-98%) Optimal (100%) Cache warmup needed No Yes (recommended) Best for Most users, rapid iteration Production, max performance"},{"location":"auto-tuning/#benchmarking-withwithout-auto-tuning","title":"Benchmarking With/Without Auto-Tuning","text":"<p>The recommended workflow is simple: benchmark the default config first, then enable auto-tuning and benchmark again.</p>"},{"location":"auto-tuning/#using-benchmark-scripts","title":"Using Benchmark Scripts","text":"<p>Run Default Config First!</p> <p>Always benchmark without <code>--autotune</code> first, then with <code>--autotune</code>. Once auto-tuning runs, it caches the config and you can't go back without clearing cache.</p> <pre><code># Step 1: Benchmark default config first\npython examples/benchmark.py\n\n# Step 2: Then benchmark with auto-tuning\npython examples/benchmark.py --autotune\n</code></pre> <p>Or the comprehensive benchmark: <pre><code>python examples/benchmark_triton.py --autotune\n</code></pre></p>"},{"location":"auto-tuning/#standard-benchmarking-workflow-custom-code","title":"Standard Benchmarking Workflow (Custom Code)","text":"<pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Step 1: Benchmark default config (auto-tuning is disabled by default), run it first!\nprint(\"Benchmarking default config...\")\ntime_default = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Default config: {time_default:.3f} ms\")\n\n# Step 2: Enable auto-tuning and benchmark\nprint(\"\\nEnabling auto-tuning...\")\nta.enable_autotune()\n\n# First call triggers auto-tuning (takes 5-10 seconds, only once)\nprint(\"Running auto-tuning (this will take ~5-10 seconds)...\")\n_ = transform(img)\n\n# Now benchmark with optimal config\nprint(\"Benchmarking auto-tuned config...\")\ntime_tuned = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Auto-tuned config: {time_tuned:.3f} ms\")\n\n# Compare\nspeedup = time_default / time_tuned\nprint(f\"\\nSpeedup: {speedup:.2f}x\")\n</code></pre>"},{"location":"auto-tuning/#for-reproducible-comparisons-google-colab","title":"For Reproducible Comparisons (Google Colab)","text":"<p>If you need truly isolated benchmarks on Google Colab (where cache may persist across runtime restarts), use two separate colab notebooks:</p> <p>Notebook 1 - Default Config: <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\n# Auto-tuning is disabled by default\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Default config: {time_ms:.3f} ms\")\n</code></pre></p> <p>Notebook 2 - Auto-Tuned Config: <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\n# Enable auto-tuning in fresh notebook\nta.enable_autotune()\n\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Trigger auto-tuning\n_ = transform(img)\n\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Auto-tuned config: {time_ms:.3f} ms\")\n</code></pre></p> <p>Colab Cache Behavior</p> <p>Google Colab's cache directory (<code>~/.triton/cache</code>) may persist across runtime restarts within the same session. Using separate notebooks ensures completely independent benchmarks.</p>"},{"location":"auto-tuning/#benchmarking-on-sharedcloud-services","title":"Benchmarking on Shared/Cloud Services","text":"<p>Instability on Colab, Kaggle, and Cloud GPUs</p> <p>If you're benchmarking on Google Colab, Kaggle Notebooks, or other shared cloud services, you may see unstable or inconsistent results.</p>"},{"location":"auto-tuning/#why-benchmarks-can-be-unstable","title":"Why Benchmarks Can Be Unstable","text":"<p>Shared GPU services can cause significant performance variability:</p> <ol> <li>Shared Physical GPU - Multiple users on the same GPU compete for resources</li> <li>Variable GPU Allocation - You might get different GPU models between sessions</li> <li>Thermal Throttling - GPU performance degrades when hot from other users' workloads</li> <li>Background Processes - Cloud platform monitoring and management overhead</li> <li>Network I/O - Data transfers can interfere with kernel execution timing</li> </ol>"},{"location":"auto-tuning/#symptoms-of-instability","title":"Symptoms of Instability","text":"<p>You might see: - Wildly varying benchmark times (e.g., 0.5ms one run, 200ms the next) - Incorrect speedups (e.g., 0.00x or negative speedups) - Different results between runs with identical code - Auto-tuning picking suboptimal configs due to noisy measurements</p>"},{"location":"auto-tuning/#best-practices-for-stable-benchmarks","title":"Best Practices for Stable Benchmarks","text":"<p>If you must benchmark on shared services:</p> <ol> <li> <p>Run multiple iterations and take the median:    <pre><code>from triton.testing import do_bench\n\n# do_bench already uses median of multiple runs\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\n</code></pre></p> </li> <li> <p>Warm up thoroughly before benchmarking:    <pre><code># Warm up: compile kernels and stabilize GPU state\nfor _ in range(10):\n    _ = transform(img)\ntorch.cuda.synchronize()\n\n# Now benchmark\ntime_ms = do_bench(lambda: transform(img))\n</code></pre></p> </li> <li> <p>Use a dedicated session - Close other notebooks/tabs using the GPU</p> </li> <li> <p>Restart runtime if results seem anomalous</p> </li> <li> <p>Run at off-peak times - Early morning or late night (timezone-dependent)</p> </li> <li> <p>Compare trends, not absolute numbers - Look for consistent relative speedups</p> </li> </ol>"},{"location":"auto-tuning/#for-production-benchmarks","title":"For Production Benchmarks","text":"<p>For reliable, production-grade benchmarks:</p> <ul> <li>Use dedicated GPU instances (AWS P3/P4, GCP A2, Azure NC-series)</li> <li>Lock GPU clocks to prevent throttling (requires root):   <pre><code>sudo nvidia-smi -lgc 1410,1410  # Lock to max clock\n</code></pre></li> <li>Isolate the GPU - No other processes using it</li> <li>Multiple runs - Run benchmarks 5-10 times and report mean \u00b1 std dev</li> </ul>"},{"location":"auto-tuning/#recommendation","title":"Recommendation","text":"<ul> <li>For most users: Keep auto-tuning disabled (default)</li> <li>Faster startup</li> <li>Good performance out-of-the-box</li> <li> <p>No cache management needed</p> </li> <li> <p>For production/max performance: Enable auto-tuning</p> </li> <li>Warm up cache during deployment</li> <li>Squeeze out last 5-15% performance</li> <li>Worth it for long training runs</li> </ul>"},{"location":"batch-behavior/","title":"Batch Behavior &amp; Different Parameters Per Sample","text":"<p>Different Parameters Per Sample by Default</p> <p>Triton-Augment applies different random parameters to each image in a batch by default!</p>"},{"location":"batch-behavior/#default-behavior-different-parameters-per-sample","title":"Default Behavior: Different Parameters Per Sample","text":"<pre><code>import torch\nimport triton_augment as ta\n\n# Each image gets DIFFERENT random augmentation\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2\n)\n\nresult = transform(batch)  # 32 different random augmentations! \u2705\n</code></pre> <p>How it works: - Random parameters are sampled per-image (32 different crop positions, flip decisions, color factors) - All processed in ONE kernel launch on GPU - Fast batch processing + individual randomness = best of both worlds! \ud83d\ude80</p>"},{"location":"batch-behavior/#controlling-randomness-same_on_batch-flag","title":"Controlling Randomness: <code>same_on_batch</code> Flag","text":"<p>All transform classes support the <code>same_on_batch</code> parameter:</p>"},{"location":"batch-behavior/#different-parameters-per-sample-default","title":"Different Parameters Per Sample (Default)","text":"<pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=False  # Default\n)\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\nresult = transform(batch)  # Each image: different crop, flip, brightness\n</code></pre>"},{"location":"batch-behavior/#batch-wide-parameters-same-for-all","title":"Batch-Wide Parameters (Same for All)","text":"<pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=True  # Same params for all images\n)\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\nresult = transform(batch)  # All images: same crop position, flip, brightness\n</code></pre>"},{"location":"batch-behavior/#when-to-use-each-mode","title":"When to Use Each Mode","text":""},{"location":"batch-behavior/#different-parameters-per-sample-recommended-for-training","title":"Different Parameters Per Sample (Recommended for Training)","text":"<p>\u2705 Use when: - Training neural networks (standard augmentation) - You want maximum data diversity - Each image should be augmented independently</p> <pre><code># Standard training setup\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    same_on_batch=False  # \u2705 Default, each image different\n)\n\nfor images, labels in train_loader:\n    images = images.cuda()\n    images = transform(images)  # Unique augmentation per image\n    # ... training ...\n</code></pre> <p>Performance: Still fast! One kernel launch processes entire batch with per-image params.</p>"},{"location":"batch-behavior/#batch-wide-parameters-specialized-use-cases","title":"Batch-Wide Parameters (Specialized Use Cases)","text":"<p>\u2705 Use when: - Processing video frames (consistent augmentation across frames) - Debugging (easier to see effect of specific parameters) - Specific research requirements</p> <pre><code># Video frame processing\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=True  # Same for all frames\n)\n\nvideo_frames = torch.rand(8, 3, 224, 224, device='cuda')  # 8 frames\nresult = transform(video_frames)  # Consistent augmentation across frames \u2705\n</code></pre>"},{"location":"batch-behavior/#all-transforms-support-different-parameters-per-sample","title":"All Transforms Support Different Parameters Per Sample","text":"<p>The following transforms all support <code>same_on_batch</code>:</p> <ul> <li><code>TritonFusedAugment</code> - Complete pipeline (crop, flip, color, normalize)</li> <li><code>TritonRandomCropFlip</code> - Geometric operations only</li> <li><code>TritonColorJitterNormalize</code> - ColorJitter + Normalize</li> <li><code>TritonColorJitter</code> - ColorJitter only</li> <li><code>TritonRandomCrop</code> - Random cropping</li> <li><code>TritonRandomHorizontalFlip</code> - Random flipping</li> <li><code>TritonRandomGrayscale</code> - Random grayscale conversion</li> </ul> <p>Example: <pre><code># Individual transforms also support same_on_batch\ncrop = ta.TritonRandomCrop(112, same_on_batch=False)\nflip = ta.TritonRandomHorizontalFlip(p=0.5, same_on_batch=False)\njitter = ta.TritonColorJitter(brightness=0.2, same_on_batch=False)\n</code></pre></p>"},{"location":"batch-behavior/#functional-api-fixed-parameters","title":"Functional API: Fixed Parameters","text":"<p>The functional API (<code>triton_augment.functional</code>) is for deterministic augmentations:</p> <pre><code>import triton_augment.functional as F\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Fixed parameters - same for all images\nresult = F.fused_augment(\n    batch,\n    top=20, left=30,          # Fixed crop position\n    height=112, width=112,\n    flip_horizontal=True,      # Fixed flip decision\n    brightness_factor=1.2,     # Fixed brightness\n    saturation_factor=0.9,     # Fixed saturation\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>If you need per-image fixed parameters, pass tensors:</p> <pre><code># Per-image fixed parameters (different but deterministic)\ntop_offsets = torch.tensor([10, 20, 30, ...], device='cuda')  # [32]\nbrightness = torch.tensor([1.1, 1.2, 1.3, ...], device='cuda')  # [32]\n\nresult = F.fused_augment(\n    batch,\n    top=top_offsets,           # Tensor: per-image positions\n    left=30,                   # Scalar: same for all\n    height=112, width=112,\n    flip_horizontal=True,\n    brightness_factor=brightness,  # Tensor: per-image factors\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre>"},{"location":"batch-behavior/#comparison-with-torchvision","title":"Comparison with torchvision","text":"<p>torchvision doesn't support different parameters per sample on batched tensors:</p> <pre><code>import torchvision.transforms.v2 as tv_transforms\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = tv_transforms.ColorJitter(brightness=0.2)\n\n# All 32 images get the SAME brightness factor\nresult = transform(batch)\n</code></pre> <p>To get different parameters per sample in torchvision, you must apply transforms before batching (in DataLoader), which processes images sequentially.</p> <p>Triton-Augment advantage: Different parameters per sample with GPU batch processing - best of both worlds! \ud83d\ude80</p>"},{"location":"batch-behavior/#performance-impact","title":"Performance Impact","text":"<p>Different Parameters Per Sample: - \u2705 Same kernel launch time as batch-wide - \u2705 One kernel launch for entire batch - \u2705 Minimal overhead (kernel uses <code>tl.load</code> to fetch per-image params)</p> <p>Batch-Wide Parameters: - \u2705 Slightly faster (no per-image parameter indexing) - \u26a0\ufe0f Less data diversity for training</p> <p>Verdict: Use different parameters per sample (default) for training. The performance difference is negligible (~1-2%), but data diversity is crucial!</p>"},{"location":"batch-behavior/#example-real-training-pipeline","title":"Example: Real Training Pipeline","text":"<pre><code>import torch\nimport triton_augment as ta\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Step 1: Load data on CPU with workers (fast async I/O)\ntrain_dataset = datasets.CIFAR10(\n    './data', train=True,\n    transform=transforms.ToTensor()  # Only ToTensor on CPU\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    num_workers=4,      # \u2705 Async data loading\n    pin_memory=True\n)\n\n# Step 2: GPU augmentation with different parameters per sample\naugment = ta.TritonFusedAugment(\n    crop_size=28,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    mean=(0.4914, 0.4822, 0.4465),\n    std=(0.2470, 0.2435, 0.2616),\n    same_on_batch=False  # \u2705 Each image gets unique augmentation\n)\n\n# Step 3: Training loop\nfor images, labels in train_loader:\n    images, labels = images.cuda(), labels.cuda()\n    images = augment(images)  # \ud83d\ude80 One kernel, all augmentations, per-image random!\n\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    # ... backprop ...\n</code></pre> <p>Result: - \u2705 Fast async CPU data loading (<code>num_workers &gt; 0</code>) - \u2705 Fast GPU batch processing (one kernel) - \u2705 Different parameters per sample (maximum diversity) - \u2705 Best of all worlds! \ud83d\ude80</p>"},{"location":"contrast/","title":"Contrast Implementation","text":"<p>Important</p> <p>This library implements a different contrast algorithm than torchvision for speed and fusion benefits.</p>"},{"location":"contrast/#tldr","title":"TL;DR","text":"<ul> <li><code>fused_color_normalize()</code> uses fast contrast (not torchvision-exact)</li> <li>For exact torchvision results, use individual functions</li> <li>Fast contrast is production-proven (same as NVIDIA DALI)</li> </ul>"},{"location":"contrast/#three-equivalent-ways-torchvision-exact","title":"Three Equivalent Ways (Torchvision-Exact)","text":"<p>All three approaches below produce pixel-perfect identical results:</p>"},{"location":"contrast/#1-torchvision","title":"1. Torchvision","text":"<pre><code>import torchvision.transforms.v2.functional as tvF\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nresult = tvF.adjust_brightness(img, 1.2)\nresult = tvF.adjust_contrast(result, 1.1)\nresult = tvF.adjust_saturation(result, 0.9)\nmean_t = torch.tensor([0.485, 0.456, 0.406], device='cuda').view(1, 3, 1, 1)\nstd_t = torch.tensor([0.229, 0.224, 0.225], device='cuda').view(1, 3, 1, 1)\nresult = (result - mean_t) / std_t\n</code></pre> <p>\u23f1\ufe0f Speed: Baseline</p>"},{"location":"contrast/#2-triton-individual-functions-exact","title":"2. Triton Individual Functions (Exact)","text":"<pre><code>import triton_augment.functional as F\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nresult = F.adjust_brightness(img, 1.2)\nresult = F.adjust_contrast(result, 1.1)        # Torchvision-exact\nresult = F.adjust_saturation(result, 0.9)\nresult = F.normalize(result, \n                     mean=(0.485, 0.456, 0.406),\n                     std=(0.229, 0.224, 0.225))\n</code></pre> <p>\u23f1\ufe0f Speed: Faster (optimized Triton kernels) \u26a1</p>"},{"location":"contrast/#3-triton-contrast-fused-exact-fast","title":"3. Triton Contrast + Fused (Exact + Fast)","text":"<pre><code>import triton_augment.functional as F\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\n# Apply exact contrast first\nresult = F.adjust_brightness(img, 1.2)\nresult = F.adjust_contrast(result, 1.1)        # Torchvision-exact\n\n# Then fuse remaining ops (no contrast)\nresult = F.fused_augment(\n    result,\n    brightness_factor=1.0,                     # Identity (already applied)\n    contrast_factor=1.0,                       # Identity (already applied)\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>\u23f1\ufe0f Speed: Fast (3 kernel launches) \u26a1\u26a1</p>"},{"location":"contrast/#for-maximum-speed-not-exact","title":"For Maximum Speed (Not Exact)","text":"<p>If you don't need exact torchvision reproduction:</p> <pre><code>import triton_augment.functional as F\n\n# Single fused kernel - fastest!\nresult = F.fused_augment(\n    img,\n    brightness_factor=1.2,\n    contrast_factor=1.1,                       # Fast contrast (different from torchvision)\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>\u23f1\ufe0f Speed: Fastest (single fused kernel) \ud83d\ude80</p> <p>Note</p> <p>Fast contrast uses <code>(pixel - 0.5) * factor + 0.5</code> instead of torchvision's blend-with-mean.</p>"},{"location":"contrast/#fast-contrast-comparison-to-other-libraries","title":"Fast Contrast: Comparison to Other Libraries","text":"Library Formula Type Speed NVIDIA DALI <code>(x - 0.5) * f + 0.5</code> Linear (centered) Fastest \u2705 Triton-Augment (fast) <code>(x - 0.5) * f + 0.5</code> Linear (centered) Fastest \u2705 OpenCV <code>alpha * x + beta</code> Linear Fast Torchvision <code>x * f + mean * (1-f)</code> Linear (mean) Slower Scikit-image <code>1/(1+exp(-gain*(x-cut)))</code> Sigmoid (S-curve) Slowest"},{"location":"contrast/#why-this-formula","title":"Why This Formula?","text":"<p>\u2705 Production-proven: Same as NVIDIA DALI \u2705 Fast &amp; fusible: No mean computation required  </p> <p>For exact torchvision reproduction, use <code>adjust_contrast()</code> instead of fast mode.</p>"},{"location":"contrast/#technical-details","title":"Technical Details","text":""},{"location":"contrast/#torchvision-contrast","title":"Torchvision Contrast","text":"<pre><code>grayscale_mean = mean(rgb_to_grayscale(input))\noutput = input * contrast_factor + grayscale_mean * (1 - contrast_factor)\n</code></pre> <p>Problem for fusion: Requires computing the mean of the entire image, which: - Needs an extra kernel launch - Breaks the fusion (can't fuse before knowing the mean) - Creates a data dependency</p>"},{"location":"contrast/#fast-contrast","title":"Fast Contrast","text":"<pre><code>output = (input - 0.5) * contrast_factor + 0.5\n</code></pre> <p>Benefits: - No mean computation needed - Fully fusible with other operations - Single kernel launch for entire pipeline - 0.5 is a reasonable default anchor point (middle of [0, 1] range)</p>"},{"location":"contrast/#impact-on-training","title":"Impact on Training","text":"<p>In practice, the difference is minimal: - Models learn to be robust to data augmentation variations - The specific contrast formula matters less than having contrast augmentation at all - NVIDIA DALI uses this approach in production ML systems worldwide</p>"},{"location":"float16/","title":"Float16 (Half Precision) Support","text":"<p>Triton-Augment fully supports float16, providing memory savings and potential speedup on modern GPUs.</p>"},{"location":"float16/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nimport triton_augment as ta\n\n# Create float16 images\nimages = torch.rand(32, 3, 224, 224, device='cuda', dtype=torch.float16)\n\n# Apply fused transform (works seamlessly with float16)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # Output is also float16\n</code></pre>"},{"location":"float16/#benefits","title":"Benefits","text":"<p>\ud83d\udcbe Half the memory: Float16 uses 2x less VRAM, enabling: - Larger batch sizes - Higher resolution images - More models in memory</p> <p>\u26a1 Potential speedup: On Tesla T4, we observed ~1.3-1.4x speedup for large images (1024\u00d71024+)</p> <p>\u2705 Maintained accuracy: Data augmentation is robust to lower precision</p>"},{"location":"float16/#benchmark-results-tesla-t4","title":"Benchmark Results (Tesla T4)","text":"<p>Our measurements on Ultimate Fusion (all operations in one kernel):</p> Image Size Float32 Float16 Speedup 256\u00d7256 0.41 ms 0.47 ms 0.88x (slower) 512\u00d7512 0.48 ms 0.47 ms 1.03x 640\u00d7640 0.57 ms 0.49 ms 1.15x 1024\u00d71024 0.93 ms 0.72 ms 1.29x 1280\u00d71280 1.27 ms 0.93 ms 1.36x <p>Conclusion: Float16 provides meaningful speedup for large images (600\u00d7600+), but offers minimal benefit for small images.</p> <p>\ud83d\udca1 Your mileage may vary: Run <code>examples/benchmark_triton.py</code> to measure on your GPU.</p>"},{"location":"float16/#when-to-use-float16","title":"When to Use Float16","text":""},{"location":"float16/#use-float16-when","title":"\u2705 Use Float16 When:","text":"<ul> <li>Training with mixed precision (<code>torch.cuda.amp</code>)</li> <li>Memory constrained: Need to fit larger batches or higher resolution images</li> <li>Large images: 600\u00d7600+ where float16 shows speedup (based on T4 benchmarks)</li> </ul>"},{"location":"float16/#skip-float16-when","title":"\u274c Skip Float16 When:","text":"<ul> <li>Small images: &lt; 512\u00d7512 (minimal or negative speedup on T4)</li> <li>CPU-only training: Float16 is GPU-specific</li> <li>Debugging: Float32 is easier to inspect</li> </ul>"},{"location":"float16/#precision-considerations","title":"Precision Considerations","text":"<p>Float16 results will differ slightly from float32 due to reduced precision. This is expected and acceptable for data augmentation:</p> <ul> <li>Models are robust to small input perturbations</li> <li>Augmentation inherently introduces variation</li> <li>Training with mixed precision is a standard practice</li> </ul>"},{"location":"float16/#usage-example","title":"Usage Example","text":"<p>With mixed precision training:</p> <pre><code>from torch.cuda.amp import autocast, GradScaler\nimport triton_augment as ta\n\ntransform = ta.TritonFusedAugment(...)\nscaler = GradScaler()\n\nfor images, labels in loader:\n    with autocast():  # Images automatically converted to float16\n        images = images.cuda()\n        augmented = transform(images)\n        output = model(augmented)\n        loss = criterion(output, labels)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre> <p>Manual float16 conversion:</p> <pre><code># Convert to float16 for memory savings\nimages = images.half().cuda()\naugmented = transform(images)  # Stays in float16\n</code></pre>"},{"location":"float16/#benchmarking","title":"Benchmarking","text":"<p>Compare float16 vs float32 performance:</p> <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\nbatch = 32\nimg_fp32 = torch.rand(batch, 3, 224, 224, device='cuda', dtype=torch.float32)\nimg_fp16 = img_fp32.half()\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112, brightness=0.2, saturation=0.2\n)\n\n# Benchmark\ntime_fp32 = do_bench(lambda: transform(img_fp32))\ntime_fp16 = do_bench(lambda: transform(img_fp16))\n\nprint(f\"Float32: {time_fp32:.3f} ms\")\nprint(f\"Float16: {time_fp16:.3f} ms\")\nprint(f\"Speedup: {time_fp32/time_fp16:.2f}x\")\n</code></pre>"},{"location":"float16/#why-float16-can-be-faster","title":"Why Float16 Can Be Faster","text":"<p>Float16 benefits come from: 1. Memory bandwidth: Half the data to transfer (2 bytes vs 4 bytes per value) 2. Cache efficiency: More data fits in GPU caches 3. GPU hardware: Modern GPUs have specialized float16 units</p> <p>Note: Speedup varies by GPU architecture and operation complexity. Always benchmark on your specific hardware.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>PyTorch &gt;= 2.0.0</li> <li>Triton &gt;= 2.0.0</li> <li>CUDA-capable GPU</li> </ul>"},{"location":"installation/#installation-from-pypi-recommended","title":"Installation from PyPI (Recommended)","text":"<pre><code>pip install triton-augment\n</code></pre>"},{"location":"installation/#development-installation-from-source","title":"Development Installation (From Source)","text":"<p>For contributors or those who want to modify the code:</p> <pre><code>git clone https://github.com/yuhezhang-ai/triton-augment.git\ncd triton-augment\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#input-requirements","title":"Input Requirements","text":"<p>Input Requirements</p> <ul> <li>Range: Pixel values must be in <code>[0, 1]</code> (use <code>transforms.ToTensor()</code> if loading from PIL)</li> <li>Device: GPU only (CPU tensors are automatically moved to CUDA)</li> <li>Shape: Supports both 3D <code>(C, H, W)</code> and 4D <code>(N, C, H, W)</code> tensors (automatic batching)</li> <li>Dtype: <code>float32</code> or <code>float16</code></li> </ul>"},{"location":"installation/#first-run-behavior","title":"First Run Behavior","text":"<p>On first use, Triton will compile kernels for your GPU (~1-2 seconds per image size with default config). This is normal and only happens once per GPU and image size.</p> <p>Optional: Cache Warm-Up</p> <p>To avoid compilation delays during training, you can optionally warm up the cache after installation:</p> <pre><code>python -m triton_augment.warmup\n</code></pre> <p>For more details and auto-tuning optimization, see the Auto-Tuning Guide.</p>"},{"location":"installation/#what-to-expect","title":"What to expect","text":"<ul> <li>First import: Helpful message about auto-tuning status (can be suppressed with <code>TRITON_AUGMENT_SUPPRESS_FIRST_RUN_MESSAGE=1</code>)</li> <li>First use of each image size: ~1-2 seconds (kernel compilation)</li> <li>Subsequent uses: Instant (kernels are cached)</li> </ul>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Should work without errors\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ntransform = ta.TritonColorJitterNormalize(brightness=0.2)\nresult = transform(img)\nprint(\"\u2705 Installation successful!\")\n</code></pre>"},{"location":"quickstart/","title":"Quick Start Guide","text":""},{"location":"quickstart/#basic-usage-ultimate-fusion-recommended","title":"Basic Usage: Ultimate Fusion (Recommended) \ud83d\ude80","text":"<p>The fastest way to use Triton-Augment - fuse ALL augmentations in a single kernel:</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Create a batch of images on GPU\nimages = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Replace torchvision Compose (7 kernel launches)\n# With Triton-Augment (1 kernel launch - significantly faster!)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    random_grayscale_p=0.1,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Apply transformation\naugmented = transform(images)  # Single kernel launch for ALL operations!\n</code></pre> <p>What it does: - RandomCrop (112\u00d7112) - RandomHorizontalFlip (50% probability) - ColorJitter (brightness, contrast, saturation) - Normalize</p> <p>Performance: Up to 12x faster on large images (8.1x average on Tesla T4, scales dramatically with image size)</p>"},{"location":"quickstart/#other-fusion-options","title":"Other Fusion Options","text":""},{"location":"quickstart/#pixel-only-fusion","title":"Pixel-Only Fusion","text":"<p>If you don't need geometric operations (crop/flip), use pixel fusion:</p> <pre><code># Fuse color jitter + normalize (single kernel)\ntransform = ta.TritonColorJitterNormalize(\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # Faster, single fused kernel\n</code></pre>"},{"location":"quickstart/#geometric-only-fusion","title":"Geometric-Only Fusion","text":"<p>If you only need crop + flip:</p> <pre><code># Fuse crop + flip (single kernel)\ntransform = ta.TritonRandomCropFlip(size=112, horizontal_flip_p=0.5)\n\naugmented = transform(images)  # ~1.5-2x faster\n</code></pre>"},{"location":"quickstart/#individual-operations","title":"Individual Operations","text":"<p>For maximum control, use individual operations (fixed parameters):</p> <pre><code>import triton_augment.functional as F\n\nimg = torch.rand(4, 3, 224, 224, device='cuda')\n\n# Geometric operations\ncropped = F.crop(img, top=20, left=30, height=112, width=112)\nflipped = F.horizontal_flip(img)\n\n# Color operations \nbright = F.adjust_brightness(img, 1.2)\nsaturated = F.adjust_saturation(img, 0.9)\nnormalized = F.normalize(img, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n</code></pre> <p>Or use transform classes (random augmentations):</p> <pre><code>import triton_augment as ta\n\n# Individual transforms\ncrop = ta.TritonRandomCrop(112)\nflip = ta.TritonRandomHorizontalFlip(p=0.5)\njitter = ta.TritonColorJitter(brightness=0.2)\nnormalize = ta.TritonNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n</code></pre>"},{"location":"quickstart/#training-integration","title":"Training Integration","text":"<p>Recommended Pattern: Load data on CPU (fast async I/O), augment on GPU (fast batch processing)</p> <pre><code>import torch\nimport triton_augment as ta\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Step 1: CPU data loading with workers\ntrain_dataset = datasets.CIFAR10(\n    './data', train=True,\n    transform=transforms.ToTensor()  # Only ToTensor on CPU\n)\ntrain_loader = DataLoader(\n    train_dataset, batch_size=128,\n    num_workers=4, pin_memory=True  # Fast async loading!\n)\n\n# Step 2: GPU augmentation transform\naugment = ta.TritonFusedAugment(\n    crop_size=28, horizontal_flip_p=0.5,\n    brightness=0.2, saturation=0.2,\n    mean=(0.4914, 0.4822, 0.4465),\n    std=(0.2470, 0.2435, 0.2616)\n)\n\n# Step 3: Apply in training loop\nfor images, labels in train_loader:\n    images, labels = images.cuda(), labels.cuda()\n    images = augment(images)  # All ops in 1 kernel! \ud83d\ude80\n    # ... rest of training ...\n</code></pre> <p>Why This Pattern:</p> <ul> <li>\u2705 Fast async data loading: <code>num_workers &gt; 0</code> for CPU parallelism</li> <li>\u2705 Fast GPU batch processing: All augmentations in 1 fused kernel</li> <li>\u2705 Different parameters per sample: Each image gets different random parameters (default)</li> <li>\u2705 Best of both worlds: CPU for I/O, GPU for compute</li> <li>\u2705 Kernel fusion: No intermediate memory allocations</li> <li>\u2705 Large batch advantage: Speedup increases with batch size</li> </ul> <p>Note: Set <code>same_on_batch=True</code> if you want all images to share the same random parameters.</p> <p>\ud83d\udca1 Pro Tip: Apply Triton-Augment transforms AFTER moving tensors to GPU for maximum performance!</p> <p>Full Examples: See <code>examples/train_mnist.py</code> and <code>examples/train_cifar10.py</code> for complete training scripts with neural networks.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Float16 Support - Use half-precision for 1.3-2x additional speedup</li> <li>Batch Behavior - Understand random parameter handling</li> <li>Contrast Notes - Fast contrast vs torchvision-exact</li> <li>Auto-Tuning - Optional performance optimization</li> <li>API Reference - Complete API documentation</li> </ul>"}]}