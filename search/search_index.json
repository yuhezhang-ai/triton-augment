{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#triton-augment","title":"Triton-Augment","text":"<p>GPU-Accelerated Image Augmentation with Kernel Fusion</p> <p> </p> <p>Triton-Augment is a high-performance image augmentation library that leverages OpenAI Triton to fuse common transform operations, providing significant speedups over standard PyTorch implementations.</p> <p>Key Idea: Fuse multiple GPU operations into a single kernel \u2192 eliminate intermediate memory transfers \u2192 faster augmentation.</p> <pre><code># Traditional (torchvision Compose): 7 kernel launches\ncrop \u2192 flip \u2192 brightness \u2192 contrast \u2192 saturation \u2192 grayscale \u2192 normalize\n\n# Triton-Augment Ultimate Fusion: 1 kernel launch \ud83d\ude80\n[crop + flip + brightness + contrast + saturation + grayscale + normalize]\n</code></pre>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>One Kernel, All Operations: Fuse crop, flip, color jitter, grayscale, and normalize in a single kernel - significantly faster, scales with image size! \ud83d\ude80</li> <li>Different Parameters Per Sample: Each image in batch gets different random augmentations (not just batch-wide)</li> <li>Transform &amp; Functional APIs: Random parameters (transforms) or fixed parameters (functional) - your choice</li> <li>Zero Memory Overhead: No intermediate buffers between operations</li> <li>Float16 Ready: ~1.3x speedup on large images + 50% memory savings</li> <li>Drop-in Replacement: torchvision-like API, easy migration</li> <li>Auto-Tuning: Optional performance optimization for your GPU</li> </ul>"},{"location":"#quick-start","title":"\ud83d\udce6 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install triton-augment\n</code></pre> <p>Requirements: Python 3.8+, PyTorch 2.0+, CUDA-capable GPU</p> <p>Try it now:  - Test correctness and run benchmarks without local setup</p> <p>Note: Colab is a shared service - performance may vary due to GPU allocation and resource contention. For stable benchmarking, use a dedicated GPU.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Recommended: Ultimate Fusion \ud83d\ude80</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Create batch of images on GPU\nimages = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Replace torchvision Compose (7 kernel launches)\n# With Triton-Augment (1 kernel launch - significantly faster!)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    random_grayscale_p=0.1,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # \ud83d\ude80 Single kernel for entire pipeline!\n</code></pre> <p>Need only some operations? Use <code>TritonFusedAugment</code> with default/no-op values, or use specialized APIs (they all use the same fused kernel internally):</p> <pre><code># Option 1: Ultimate API with partial operations (set unused to 0/default)\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    horizontal_flip_p=0.0,  # No flip\n    brightness=0.0,         # No brightness\n    saturation=0.2,         # Only saturation\n    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n)\n\n# Option 2: Specialized APIs (convenience wrappers, same kernel internally)\ncolor_only = ta.TritonColorJitterNormalize(brightness=0.2, saturation=0.2, ...)\ngeo_only = ta.TritonRandomCropFlip(size=112, horizontal_flip_p=0.5)\n</code></pre> <p>\u2192 More Examples</p>"},{"location":"#input-requirements","title":"\u26a0\ufe0f Input Requirements","text":"<ul> <li>Range: Images must be in <code>[0, 1]</code> range (e.g., use <code>torchvision.transforms.ToTensor()</code>)</li> <li>Device: GPU (CUDA) - CPU tensors automatically moved to GPU</li> <li>Shape: <code>(C, H, W)</code> or <code>(N, C, H, W)</code> - 3D tensors automatically batched</li> <li>Dtype: <code>float32</code> or <code>float16</code></li> </ul>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Full documentation: Navigation menu on the left (or see GitHub repo <code>docs/</code> folder)</p> Guide Description Quick Start Get started in 5 minutes with examples Installation Setup and requirements API Reference Complete API documentation for all functions and classes Float16 Support Use half-precision for ~1.3x speedup (large images) and 50% memory savings Contrast Notes Fused kernel uses fast contrast (different from torchvision). See how to get exact torchvision results Auto-Tuning Optional performance optimization for your GPU and data size (disabled by default). Includes cache warm-up guide Batch Behavior Different parameters per sample (default) vs batch-wide parameters. Understanding <code>same_on_batch</code> flag"},{"location":"#performance","title":"\u26a1 Performance","text":"<p>\ud83d\udcca Run benchmarks yourself on Google Colab - Verify correctness and performance on free GPU Note: Colab performance may vary due to shared resources</p>"},{"location":"#benchmark-results","title":"Benchmark Results","text":"<p>Real training scenario with random augmentations on Tesla T4 (Google Colab Free Tier):</p> Image Size Batch Crop Size Torchvision Triton Fused Speedup 256\u00d7256 32 224\u00d7224 2.48 ms 0.56 ms 4.5x 256\u00d7256 64 224\u00d7224 4.51 ms 0.69 ms 6.5x 600\u00d7600 32 512\u00d7512 11.82 ms 1.26 ms 9.4x 1280\u00d71280 32 1024\u00d71024 48.91 ms 4.07 ms 12.0x <p>Average Speedup: 8.1x \ud83d\ude80</p> <p>Operations: RandomCrop + RandomHorizontalFlip + ColorJitter + RandomGrayscale + Normalize</p> <p>Performance scales with image size \u2014 larger images benefit more from kernel fusion:</p> <p> </p> <p>Speedup advantage increases dramatically for larger images (600\u00d7600+). Triton maintains near-constant runtime while Torchvision scales linearly.</p> <p>\ud83d\udcca Additional Benchmarks (NVIDIA A100 on Google Colab):</p> Image Size Batch Crop Size Torchvision Triton Fused Speedup 256\u00d7256 32 224\u00d7224 0.61 ms 0.44 ms 1.4x 256\u00d7256 64 224\u00d7224 0.93 ms 0.43 ms 2.1x 600\u00d7600 32 512\u00d7512 2.19 ms 0.50 ms 4.4x 1280\u00d71280 32 1024\u00d71024 8.23 ms 0.94 ms 8.7x <p>Average: 4.1x (A100's high memory bandwidth makes torchvision already fast, so relative improvement is smaller)</p> <p>\ud83d\udca1 Why better speedup on T4? Kernel fusion reduces memory bandwidth bottlenecks, which matters more on bandwidth-limited GPUs like T4 (320 GB/s) vs A100 (1,555 GB/s). This means greater benefits on consumer and mid-range hardware.</p>"},{"location":"#run-your-own-benchmarks","title":"Run Your Own Benchmarks","text":"<p>Quick Benchmark (Ultimate Fusion only): <pre><code># Simple, clean table output - easy to run!\npython examples/benchmark.py\n</code></pre></p> <p>Note: Benchmarks use <code>torchvision.transforms.v2</code> (not the legacy v1 API) for comparison.</p> <p>Detailed Benchmark (All operations): <pre><code># Comprehensive analysis with visualizations\npython examples/benchmark_triton.py\n</code></pre></p> <p>\ud83d\udca1 Auto-Tuning: The results above use default configurations. Auto-tuning can provide additional speedup on dedicated GPUs (local workstations, cloud instances). On shared cloud services (Colab, Kaggle), auto-tuning benefits may be limited due to variable GPU utilization. See Auto-Tuning Guide for details.</p>"},{"location":"#when-to-use-triton-augment","title":"\ud83c\udfaf When to Use Triton-Augment?","text":"<p>\ud83d\udca1 Use Triton-Augment + Torchvision together:</p> <ul> <li>Torchvision: Data loading, resize, ToTensor, rotation, affine, etc.</li> <li>Triton-Augment: Replace supported operations (currently: crop, flip, color jitter, grayscale, normalize; more coming) with fused GPU kernels</li> </ul> <p>Best speedup when:</p> <ul> <li>Large images (600\u00d7600+) or large batches</li> <li>Data augmentations are your bottleneck</li> </ul> <p>Stick with Torchvision only if:</p> <ul> <li>Small images (&lt; 256\u00d7256) on high-end GPUs (A100+)</li> <li>CPU-only training</li> </ul> <p>\ud83d\udca1 TL;DR: Use both! Triton-Augment replaces Torchvision's fusible ops for 8-12x speedup.</p>"},{"location":"#training-integration","title":"\ud83c\udf93 Training Integration","text":"<p>Want to use Triton-Augment in your training pipeline? See the Quick Start Guide for:</p> <ul> <li>Complete training examples (MNIST, CIFAR-10)</li> <li>DataLoader integration patterns</li> <li>Best practices for CPU data loading + GPU augmentation</li> <li>Why this architecture is fast</li> </ul> <p>Quick snippet:</p> <pre><code># Step 1: Load data on CPU with workers\ntrain_loader = DataLoader(..., num_workers=4)\n\n# Step 2: Create GPU augmentation (once)\naugment = ta.TritonFusedAugment(crop_size=28, ...)\n\n# Step 3: Apply in training loop on GPU batches\nfor images, labels in train_loader:\n    images = images.cuda()\n    images = augment(images)  # \ud83d\ude80 1 kernel for all ops!\n    outputs = model(images)\n</code></pre> <p>\u2192 Full Training Guide</p>"},{"location":"#roadmap","title":"\ud83d\udccb Roadmap","text":"<ul> <li>[x] Phase 1: Fused color operations (brightness, contrast, saturation, normalize)</li> <li>[x] Phase 1.5: Grayscale, float16 support, auto-tuning</li> <li>[x] Phase 2: Basic Geometric operations (crop, flip) + Ultimate fusion \ud83d\ude80</li> <li>[ ] Phase 3: Extended operations (resize, rotation, blur, erasing, mixup)</li> </ul> <p>\u2192 Detailed Roadmap</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions welcome! Please see CONTRIBUTING.md for guidelines.</p> <pre><code># Development setup\npip install -e \".[dev]\"\n\n# Useful commands\nmake help        # Show all available commands\nmake test        # Run tests\n</code></pre> <p>\u2192 Complete Contributing Guide</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>Apache License 2.0 - see LICENSE file.</p>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>OpenAI Triton - GPU programming framework</li> <li>PyTorch - Deep learning foundation</li> <li>torchvision - API inspiration</li> </ul>"},{"location":"#author","title":"\ud83d\udc64 Author","text":"<p>Yuhe Zhang</p> <ul> <li>\ud83d\udcbc LinkedIn: Yuhe Zhang</li> <li>\ud83d\udce7 Email: yuhezhang.zju@gmail.com</li> </ul> <p>Research interests: Applied ML, Computer Vision, Efficient Deep Learning, GPU Acceleration</p>"},{"location":"#project","title":"\ud83d\udce7 Project","text":"<ul> <li>Issues and feature requests: GitHub Issues</li> <li>PyPI Package: pypi.org/project/triton-augment</li> </ul> <p>\u2b50 If you find this library useful, please consider starring the repo! \u2b50</p>"},{"location":"DOCUMENTATION/","title":"Documentation Guide","text":"<p>This document explains how to build, preview, and deploy the Triton-Augment documentation.</p>"},{"location":"DOCUMENTATION/#structure","title":"Structure","text":"<pre><code>README.md                 # Home page (shared with GitHub)\ndocs/\n\u251c\u2500\u2500 installation.md       # Installation guide\n\u251c\u2500\u2500 quickstart.md        # Quick start tutorial\n\u251c\u2500\u2500 float16.md           # Float16 support\n\u251c\u2500\u2500 batch-behavior.md    # Batch behavior guide\n\u251c\u2500\u2500 contrast.md          # Contrast implementation details\n\u251c\u2500\u2500 auto-tuning.md       # Auto-tuning guide\n\u251c\u2500\u2500 api-reference.md     # API reference\n\u251c\u2500\u2500 requirements.txt     # Docs dependencies\n\u2514\u2500\u2500 stylesheets/\n    \u2514\u2500\u2500 extra.css        # Custom CSS\n</code></pre> <p>Note: The home page uses the root <code>README.md</code> to avoid duplication between GitHub and documentation site.</p>"},{"location":"DOCUMENTATION/#building-the-documentation","title":"Building the Documentation","text":""},{"location":"DOCUMENTATION/#install-dependencies","title":"Install Dependencies","text":"<pre><code># From project root\npip install -r docs/requirements.txt\n</code></pre>"},{"location":"DOCUMENTATION/#preview-locally","title":"Preview Locally","text":"<pre><code># Start local server (with auto-reload)\nmkdocs serve\n\n# Open in browser\n# http://127.0.0.1:8000\n</code></pre> <p>The server will auto-reload when you edit documentation files.</p>"},{"location":"DOCUMENTATION/#build-static-site","title":"Build Static Site","text":"<pre><code># Build HTML files\nmkdocs build\n\n# Output is in site/ directory\n# Open site/index.html in browser to preview\n</code></pre>"},{"location":"DOCUMENTATION/#deploying-to-github-pages","title":"Deploying to GitHub Pages","text":""},{"location":"DOCUMENTATION/#option-1-automatic-recommended","title":"Option 1: Automatic (Recommended)","text":"<pre><code># Build and deploy in one command\nmkdocs gh-deploy\n\n# This will:\n# 1. Build the site\n# 2. Push to gh-pages branch\n# 3. GitHub will serve it automatically\n</code></pre> <p>Your docs will be available at: <code>https://&lt;username&gt;.github.io/&lt;repo&gt;/</code></p>"},{"location":"DOCUMENTATION/#option-2-manual","title":"Option 2: Manual","text":"<pre><code># Build locally\nmkdocs build\n\n# Copy site/ contents to your web server\n# Or commit site/ to gh-pages branch manually\n</code></pre>"},{"location":"DOCUMENTATION/#documentation-workflow","title":"Documentation Workflow","text":""},{"location":"DOCUMENTATION/#adding-a-new-page","title":"Adding a New Page","text":"<ol> <li>Create new <code>.md</code> file in <code>docs/</code></li> <li>Add entry to <code>mkdocs.yml</code> navigation</li> <li>Preview with <code>mkdocs serve</code></li> <li>Commit and deploy</li> </ol> <p>Example:</p> <pre><code># mkdocs.yml\nnav:\n  - Home: index.md\n  - Getting Started:\n      - Installation: installation.md\n      - Quick Start: quickstart.md\n      - Your New Page: new-page.md  # Add here\n</code></pre>"},{"location":"DOCUMENTATION/#updating-api-reference","title":"Updating API Reference","text":"<p>The API reference (<code>docs/api-reference.md</code>) is manually written. To add new functions:</p> <ol> <li>Add function documentation to <code>api-reference.md</code></li> <li>Follow the existing format (code examples + parameter descriptions)</li> <li>Add cross-references to relevant guides</li> </ol> <p>For auto-generated API docs (future): - Use <code>mkdocstrings</code> plugin (already configured) - Add docstring references: <code>triton_augment.functional</code></p>"},{"location":"DOCUMENTATION/#writing-style","title":"Writing Style","text":"<ul> <li>Use code examples: Show, don't just tell</li> <li>Add admonitions: <code>!!! note</code>, <code>!!! warning</code>, <code>!!! tip</code></li> <li>Include cross-references: Link related pages</li> <li>Keep it concise: Short paragraphs, clear headings</li> <li>Use tables: For comparisons and parameter lists</li> </ul>"},{"location":"DOCUMENTATION/#markdown-extensions","title":"Markdown Extensions","text":"<p>Available extensions (configured in <code>mkdocs.yml</code>):</p> <pre><code># Admonitions (call-out boxes)\n!!! note \"Optional Title\"\n    This is a note\n\n!!! warning\n    This is a warning\n\n# Code blocks with syntax highlighting\n```python\nimport triton_augment as ta\n</code></pre>"},{"location":"DOCUMENTATION/#tabbed-content","title":"Tabbed content","text":"Tab 1Tab 2 <p>Content 1</p> <p>Content 2</p>"},{"location":"DOCUMENTATION/#tables","title":"Tables","text":"Header 1 Header 2 Cell 1 Cell 2 <pre><code>## Theme Customization\n\n### Colors\n\nEdit `mkdocs.yml`:\n\n```yaml\ntheme:\n  palette:\n    primary: indigo  # Change to: red, pink, purple, etc.\n    accent: indigo\n</code></pre>"},{"location":"DOCUMENTATION/#logo-and-favicon","title":"Logo and Favicon","text":"<p>Add files: - <code>docs/images/logo.png</code> - <code>docs/images/favicon.ico</code></p> <p>Update <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  logo: images/logo.png\n  favicon: images/favicon.ico\n</code></pre>"},{"location":"DOCUMENTATION/#custom-css","title":"Custom CSS","text":"<p>Edit <code>docs/stylesheets/extra.css</code> for custom styles.</p>"},{"location":"DOCUMENTATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DOCUMENTATION/#build-errors","title":"Build Errors","text":"<pre><code># Clear cache\nrm -rf site/\n\n# Rebuild\nmkdocs build\n</code></pre>"},{"location":"DOCUMENTATION/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code># Reinstall\npip install -r docs/requirements.txt --upgrade\n</code></pre>"},{"location":"DOCUMENTATION/#gh-deploy-issues","title":"gh-deploy Issues","text":"<pre><code># Check git status\ngit status\n\n# Ensure you're on main branch\ngit checkout main\n\n# Try force deploy\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"DOCUMENTATION/#best-practices","title":"Best Practices","text":"<ol> <li>Test locally before deploying (<code>mkdocs serve</code>)</li> <li>Keep README short - link to full docs</li> <li>Update docs when adding features</li> <li>Use examples - code speaks louder than words</li> <li>Cross-reference - link related pages</li> <li>Version docs - note when features were added</li> </ol>"},{"location":"DOCUMENTATION/#continuous-integration-optional","title":"Continuous Integration (Optional)","text":"<p>Add GitHub Actions to auto-deploy docs:</p> <pre><code># .github/workflows/docs.yml\nname: Deploy Documentation\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.x\n      - run: pip install -r docs/requirements.txt\n      - run: mkdocs gh-deploy --force\n</code></pre>"},{"location":"DOCUMENTATION/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>mkdocstrings</li> <li>Markdown Guide</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete auto-generated API reference for all Triton-Augment operations.</p>"},{"location":"api-reference/#transform-classes","title":"Transform Classes","text":"<p>Transform classes provide stateful, random augmentations similar to torchvision. Recommended for training pipelines.</p>"},{"location":"api-reference/#triton_augment.TritonFusedAugment","title":"triton_augment.TritonFusedAugment","text":"<p>               Bases: <code>Module</code></p> <p>Fused augmentation: All operations in ONE kernel.</p> <p>This transform combines ALL augmentations in a single GPU kernel launch: - Geometric Tier: RandomCrop + RandomHorizontalFlip - Pixel Tier: ColorJitter (brightness, contrast, saturation) + Normalize</p> <p>Performance: up to 12x faster than torchvision.transforms.Compose!</p> <p>This is the PEAK PERFORMANCE path - single kernel launch for entire pipeline. No intermediate memory allocations or kernel launch overhead.</p> <p>Parameters:</p> Name Type Description Default <code>crop_size</code> <code>int | tuple[int, int]</code> <p>Desired output size (int or tuple). If int, output is square (crop_size, crop_size).</p> required <code>horizontal_flip_p</code> <code>float</code> <p>Probability of horizontal flip (default: 0.0, no flip)</p> <code>0.0</code> <code>brightness</code> <code>float | tuple[float, float]</code> <p>How much to jitter brightness. If float, chosen uniformly from [max(0, 1-brightness), 1+brightness].        If tuple, chosen uniformly from [brightness[0], brightness[1]].</p> <code>0</code> <code>contrast</code> <code>float | tuple[float, float]</code> <p>How much to jitter contrast (same format as brightness)</p> <code>0</code> <code>saturation</code> <code>float | tuple[float, float]</code> <p>How much to jitter saturation (same format as brightness)</p> <code>0</code> <code>random_grayscale_p</code> <code>float</code> <p>Probability of converting to grayscale (default: 0.0, no grayscale)</p> <code>0.0</code> <code>mean</code> <code>tuple[float, float, float]</code> <p>Sequence of means for R, G, B channels</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>tuple[float, float, float]</code> <p>Sequence of stds for R, G, B channels</p> <code>(0.229, 0.224, 0.225)</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in the batch share the same parameters.                  If False (default), each image in the batch gets different random parameters.</p> <code>False</code> Example <pre><code># Replace torchvision Compose with single transform\n# OLD (6 kernel launches):\ntransform = transforms.Compose([\n    transforms.RandomCrop(112),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\n# NEW (1 kernel launch - significantly faster!):\nimport triton_augment as ta\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nresult = transform(img)  # Single kernel launch!\n</code></pre> Note <ul> <li>Uses FAST contrast (centered scaling), not torchvision's blend-with-mean</li> <li>By default, each image gets different random parameters (set same_on_batch=False for same params)</li> <li>Input must be (N, 3, H, W) float tensor on CUDA in [0, 1] range</li> </ul>"},{"location":"api-reference/#triton_augment.TritonFusedAugment-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonFusedAugment.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply all augmentations in a single fused kernel.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (N, C, H, W) or (C, H, W)    Can be on CPU or CUDA (will be moved to CUDA automatically)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Augmented tensor of same shape and device as input</p>"},{"location":"api-reference/#triton_augment.TritonColorJitterNormalize","title":"triton_augment.TritonColorJitterNormalize","text":"<p>               Bases: <code>Module</code></p> <p>Combined color jitter, random grayscale, and normalization in a single fused operation.</p> <p>This class combines TritonColorJitter, TritonRandomGrayscale, and TritonNormalize  into a single operation that uses a fused kernel for maximum performance. This is the recommended way to apply color augmentations and normalization.</p> <p>Parameters:</p> Name Type Description Default <code>brightness</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter brightness (same as TritonColorJitter)</p> <code>None</code> <code>contrast</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter contrast (same as TritonColorJitter)</p> <code>None</code> <code>saturation</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter saturation (same as TritonColorJitter)</p> <code>None</code> <code>random_grayscale_p</code> <code>float</code> <p>Probability of converting to grayscale (default: 0.0)</p> <code>0.0</code> <code>mean</code> <code>Tuple[float, float, float]</code> <p>Sequence of means for normalization (R, G, B)</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>Tuple[float, float, float]</code> <p>Sequence of standard deviations for normalization (R, G, B)</p> <code>(0.229, 0.224, 0.225)</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch share the same random parameters                  If False (default), each image in batch gets different random parameters</p> <code>False</code> Example <pre><code># Full augmentation pipeline in one transform (per-image randomness)\ntransform = TritonColorJitterNormalize(\n    brightness=0.2,  # Range: [0.8, 1.2]\n    contrast=0.2,    # Range: [0.8, 1.2]\n    saturation=0.2,  # Range: [0.8, 1.2]\n    random_grayscale_p=0.1,  # 10% chance of grayscale (per-image)\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    same_on_batch=False\n)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\naugmented = transform(img)  # Each image gets different augmentation\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonColorJitterNormalize-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonColorJitterNormalize.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply random color jitter and normalization in a single fused operation.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input tensor of shape (N, C, H, W) or (C, H, W)  Can be on CPU or CUDA (will be moved to CUDA automatically)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Augmented and normalized tensor of same shape and device as input</p>"},{"location":"api-reference/#triton_augment.TritonRandomCropFlip","title":"triton_augment.TritonRandomCropFlip","text":"<p>               Bases: <code>Module</code></p> <p>Fused random crop + random horizontal flip.</p> <p>This class combines random crop and random horizontal flip in a SINGLE kernel launch, eliminating intermediate memory transfers.</p> <p>Performance: ~1.5-2x faster than applying TritonRandomCrop + TritonRandomHorizontalFlip sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Union[int, Sequence[int]]</code> <p>Desired output size (height, width) or int for square crop</p> required <code>horizontal_flip_p</code> <code>float</code> <p>Probability of horizontal flip (default: 0.5)</p> <code>0.5</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch share the same random parameters                  If False (default), each image in batch gets different random parameters</p> <code>False</code> Example <pre><code># Fused version (FAST - single kernel, per-image randomness)\ntransform_fused = TritonRandomCropFlip(112, horizontal_flip_p=0.5, same_on_batch=False)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nresult = transform_fused(img)  # Each image gets different crop &amp; flip\n\n# Equivalent sequential version (SLOWER - 2 kernels)\ntransform_seq = nn.Sequential(\n    TritonRandomCrop(112, same_on_batch=False),\n    TritonRandomHorizontalFlip(p=0.5, same_on_batch=False)\n)\nresult_seq = transform_seq(img)\n</code></pre> Note <p>The fused version uses compile-time branching (tl.constexpr), so there's zero overhead when flip is not triggered.</p>"},{"location":"api-reference/#triton_augment.TritonRandomCropFlip-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomCropFlip.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (N, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Randomly cropped (and optionally flipped) tensor of shape (N, C, size[0], size[1])</p>"},{"location":"api-reference/#triton_augment.TritonColorJitter","title":"triton_augment.TritonColorJitter","text":"<p>               Bases: <code>Module</code></p> <p>Randomly change the brightness, contrast, and saturation of an image.</p> <p>This is a GPU-accelerated version of torchvision.transforms.v2.ColorJitter that uses a fused kernel for maximum performance.</p> <p>IMPORTANT: Contrast uses FAST mode (centered scaling: <code>(pixel - 0.5) * factor + 0.5</code>), NOT torchvision's blend-with-mean approach. This is much faster and provides similar visual results.</p> <p>If you need exact torchvision behavior, use the individual functional APIs: - <code>F.adjust_brightness()</code> (exact) - <code>F.adjust_contrast()</code> (torchvision-exact, slower) - <code>F.adjust_saturation()</code> (exact)</p> <p>Parameters:</p> Name Type Description Default <code>brightness</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter brightness.        brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]        or the given [min, max]. Should be non negative numbers.</p> <code>None</code> <code>contrast</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter contrast (uses FAST mode).      contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]      or the given [min, max]. Should be non-negative numbers.</p> <code>None</code> <code>saturation</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter saturation.        saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]        or the given [min, max]. Should be non negative numbers.</p> <code>None</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch share the same random parameters                  If False (default), each image in batch gets different random parameters</p> <code>False</code> Example <pre><code># Basic usage with per-image randomness\ntransform = TritonColorJitter(\n    brightness=0.2,  # Range: [0.8, 1.2]\n    contrast=0.2,    # Range: [0.8, 1.2] (FAST contrast)\n    saturation=0.2,  # Range: [0.8, 1.2]\n    same_on_batch=False\n)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\naugmented = transform(img)  # Each image gets different augmentation\n\n# Custom ranges\ntransform = TritonColorJitter(\n    brightness=(0.5, 1.5),  # Custom range\n    contrast=(0.7, 1.3),     # Custom range (FAST mode)\n    saturation=(0.0, 2.0)    # Custom range\n)\n</code></pre> Performance <ul> <li>Uses fused kernel for all operations in a single pass</li> <li>Faster than sequential operations</li> <li>For even more speed, combine with normalization using TritonColorJitterNormalize</li> </ul>"},{"location":"api-reference/#triton_augment.TritonColorJitter-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonColorJitter.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply random color jitter to the input image tensor.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Augmented image tensor of the same shape and dtype</p>"},{"location":"api-reference/#triton_augment.TritonNormalize","title":"triton_augment.TritonNormalize","text":"<p>               Bases: <code>Module</code></p> <p>Normalize a tensor image with mean and standard deviation.</p> <p>This is a GPU-accelerated version of torchvision.transforms.Normalize that uses a Triton kernel for improved performance.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Tuple[float, float, float]</code> <p>Sequence of means for each channel (R, G, B)</p> required <code>std</code> <code>Tuple[float, float, float]</code> <p>Sequence of standard deviations for each channel (R, G, B)</p> required Example <pre><code># ImageNet normalization\nnormalize = TritonNormalize(\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nnormalized = normalize(img)\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonNormalize-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonNormalize.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Normalize the input image tensor.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input tensor of shape (N, C, H, W) or (C, H, W)  Can be on CPU or CUDA (will be moved to CUDA automatically)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of same shape and device as input</p>"},{"location":"api-reference/#triton_augment.TritonRandomGrayscale","title":"triton_augment.TritonRandomGrayscale","text":"<p>               Bases: <code>Module</code></p> <p>Randomly convert image to grayscale with probability p.</p> <p>Matches torchvision.transforms.v2.RandomGrayscale behavior with optional per-image randomness.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of converting to grayscale (default: 0.1)</p> <code>0.1</code> <code>num_output_channels</code> <code>int</code> <p>Number of output channels (1 or 3, default: 3)                 Usually 3 to maintain compatibility with RGB pipelines</p> <code>3</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch make the same grayscale decision                  If False (default), each image in batch independently decides grayscale conversion</p> <code>False</code> Example <pre><code># Per-image randomness (each image independently converted)\ntransform = TritonRandomGrayscale(p=0.5, num_output_channels=3, same_on_batch=False)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nresult = transform(img)  # Each image has 50% chance of being grayscale\n\n# Batch-wide (all images converted or none)\ntransform = TritonRandomGrayscale(p=0.5, num_output_channels=3, same_on_batch=True)\nresult = transform(img)  # Either all 4 images are grayscale or none are\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonRandomGrayscale-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomGrayscale.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (N, 3, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Image tensor, either original or grayscale based on probability</p>"},{"location":"api-reference/#triton_augment.TritonGrayscale","title":"triton_augment.TritonGrayscale","text":"<p>               Bases: <code>Module</code></p> <p>Convert image to grayscale.</p> <p>Matches torchvision.transforms.v2.Grayscale behavior. Uses weights: 0.2989R + 0.587G + 0.114*B</p> <p>Parameters:</p> Name Type Description Default <code>num_output_channels</code> <code>int</code> <p>Number of output channels (1 or 3).                 If 1, output is single-channel grayscale.                 If 3, grayscale is replicated to 3 channels.</p> <code>1</code> Example <pre><code>transform = TritonGrayscale(num_output_channels=3)\nimg = torch.rand(1, 3, 224, 224, device='cuda')\ngray = transform(img)  # Shape: (1, 3, 224, 224), all channels identical\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonGrayscale-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonGrayscale.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (N, 3, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Grayscale tensor of shape (N, num_output_channels, H, W)</p>"},{"location":"api-reference/#triton_augment.TritonRandomCrop","title":"triton_augment.TritonRandomCrop","text":"<p>               Bases: <code>Module</code></p> <p>Crop a random portion of the image.</p> <p>Matches torchvision.transforms.v2.RandomCrop behavior (simplified MVP version).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Union[int, Sequence[int]]</code> <p>Desired output size (height, width) or int for square crop</p> required <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch crop at the same position.                  If False (default), each image in batch gets different random crop position.</p> <code>False</code> Example <p><pre><code>transform = TritonRandomCrop(112)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = transform(img)\ncropped.shape\n</code></pre> torch.Size([4, 3, 112, 112])</p> Note <p>For MVP, padding is not supported. Image must be larger than crop size. Future versions will support padding, pad_if_needed, fill, padding_mode.</p>"},{"location":"api-reference/#triton_augment.TritonRandomCrop-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomCrop.get_params","title":"get_params  <code>staticmethod</code>","text":"<pre><code>get_params(image: Tensor, output_size: Tuple[int, int]) -&gt; Tuple[int, int, int, int]\n</code></pre> <p>Get parameters for random crop.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor (N, C, H, W)</p> required <code>output_size</code> <code>Tuple[int, int]</code> <p>Desired output size (height, width)</p> required <p>Returns:</p> Type Description <code>Tuple[int, int, int, int]</code> <p>Tuple (top, left, height, width) for cropping</p>"},{"location":"api-reference/#triton_augment.TritonRandomCrop.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (N, C, H, W) or (C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Randomly cropped tensor of shape (N, C, size[0], size[1])</p>"},{"location":"api-reference/#triton_augment.TritonCenterCrop","title":"triton_augment.TritonCenterCrop","text":"<p>               Bases: <code>Module</code></p> <p>Crop the center of the image.</p> <p>Matches torchvision.transforms.v2.CenterCrop behavior.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Union[int, Sequence[int]]</code> <p>Desired output size (height, width) or int for square crop</p> required Example <p><pre><code>transform = TritonCenterCrop(112)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = transform(img)\ncropped.shape\n</code></pre> torch.Size([4, 3, 112, 112])</p>"},{"location":"api-reference/#triton_augment.TritonCenterCrop-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonCenterCrop.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (N, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Center-cropped tensor of shape (N, C, size[0], size[1])</p>"},{"location":"api-reference/#triton_augment.TritonRandomHorizontalFlip","title":"triton_augment.TritonRandomHorizontalFlip","text":"<p>               Bases: <code>Module</code></p> <p>Horizontally flip the image randomly with probability p.</p> <p>Matches torchvision.transforms.v2.RandomHorizontalFlip behavior.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of flipping (default: 0.5)</p> <code>0.5</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch share the same decision.                  If False (default), each image in batch gets different random decision.</p> <code>False</code> Example <pre><code>transform = TritonRandomHorizontalFlip(p=0.5)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nflipped = transform(img)  # Each image has 50% chance of being flipped\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonRandomHorizontalFlip-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomHorizontalFlip.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (N, C, H, W) or (C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Image tensor, either original or horizontally flipped</p>"},{"location":"api-reference/#functional-api","title":"Functional API","text":"<p>Low-level functional interface for fine-grained control with fixed parameters. Use when you need deterministic operations.</p>"},{"location":"api-reference/#triton_augment.functional.fused_augment","title":"triton_augment.functional.fused_augment","text":"<pre><code>fused_augment(image: Tensor, top: int | Tensor, left: int | Tensor, height: int, width: int, flip_horizontal: bool | Tensor = False, brightness_factor: float | Tensor = 1.0, contrast_factor: float | Tensor = 1.0, saturation_factor: float | Tensor = 1.0, grayscale: bool | Tensor = False, mean: tuple[float, float, float] | None = None, std: tuple[float, float, float] | None = None) -&gt; torch.Tensor\n</code></pre> <p>Fused augmentation: ALL operations in ONE kernel.</p> <p>Combines geometric (crop + flip) and pixel (color + normalize) operations in a single GPU kernel, providing maximum performance.</p> <p>Performance: Up to 12x faster on large images (8.1x average on Tesla T4, scales dramatically with image size)</p> <p>Operations applied in sequence: 1. Geometric: Crop + Optional Horizontal Flip (index transformations, per-image) 2. Pixel: Brightness + Contrast (fast) + Saturation + Random Grayscale + Normalize (value operations, per-image)</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>top</code> <code>int | Tensor</code> <p>Crop top offset (int or int32 tensor of shape (N,) for per-image)</p> required <code>left</code> <code>int | Tensor</code> <p>Crop left offset (int or int32 tensor of shape (N,) for per-image)</p> required <code>height</code> <code>int</code> <p>Crop height</p> required <code>width</code> <code>int</code> <p>Crop width</p> required <code>flip_horizontal</code> <code>bool | Tensor</code> <p>Whether to flip horizontally (bool or uint8 tensor of shape (N,) for per-image, default: False)</p> <code>False</code> <code>brightness_factor</code> <code>float | Tensor</code> <p>Brightness multiplier (float or tensor of shape (N,) for per-image, 1.0 = no change)</p> <code>1.0</code> <code>contrast_factor</code> <code>float | Tensor</code> <p>Contrast multiplier (float or tensor of shape (N,) for per-image, 1.0 = no change) [FAST mode]</p> <code>1.0</code> <code>saturation_factor</code> <code>float | Tensor</code> <p>Saturation multiplier (float or tensor of shape (N,) for per-image, 1.0 = no change)</p> <code>1.0</code> <code>grayscale</code> <code>bool | Tensor</code> <p>Whether to convert to grayscale (bool or uint8 tensor of shape (N,) for per-image, default: False)</p> <code>False</code> <code>mean</code> <code>tuple[float, float, float] | None</code> <p>Normalization mean parameters (None = skip normalization)</p> <code>None</code> <code>std</code> <code>tuple[float, float, float] | None</code> <p>Normalization std parameters (None = skip normalization)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Transformed tensor of shape (N, C, height, width)</p> Example <pre><code>img = torch.rand(4, 3, 224, 224, device='cuda')\n# Single kernel launch for ALL operations!\nresult = fused_augment(\n    img,\n    top=20, left=30, height=112, width=112,\n    flip_horizontal=True,\n    brightness_factor=1.2,\n    contrast_factor=1.1,\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Equivalent sequential operations (MUCH slower - 6 kernel launches):\nresult_seq = crop(img, 20, 30, 112, 112)\nresult_seq = horizontal_flip(result_seq)\nresult_seq = adjust_brightness(result_seq, 1.2)\nresult_seq = adjust_contrast_fast(result_seq, 1.1)\nresult_seq = adjust_saturation(result_seq, 0.9)\nresult_seq = normalize(result_seq, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n</code></pre> Note <ul> <li>Uses FAST contrast (centered scaling), not torchvision's blend-with-mean</li> <li>For torchvision-exact contrast, use sequential operations</li> <li>Zero intermediate memory allocations!</li> </ul>"},{"location":"api-reference/#triton_augment.functional.adjust_brightness","title":"triton_augment.functional.adjust_brightness","text":"<pre><code>adjust_brightness(image: Tensor, brightness_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust brightness of an image (MULTIPLICATIVE operation).</p> <p>Matches torchvision.transforms.v2.functional.adjust_brightness exactly. Reference: torchvision/transforms/v2/functional/_color.py line 114-125</p> <p>Formula: output = input * brightness_factor</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>brightness_factor</code> <code>float</code> <p>How much to adjust the brightness. Must be non-negative.               0 gives a black image, 1 gives the original image,               2 increases brightness by 2x.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Brightness-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If brightness_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nbright_img = adjust_brightness(img, brightness_factor=1.2)  # 20% brighter\ndark_img = adjust_brightness(img, brightness_factor=0.8)   # 20% darker\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.adjust_contrast","title":"triton_augment.functional.adjust_contrast","text":"<pre><code>adjust_contrast(image: Tensor, contrast_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust contrast of an image.</p> <p>Matches torchvision.transforms.v2.functional.adjust_contrast exactly. Reference: torchvision/transforms/v2/functional/_color.py line 190-206</p> output = blend(image, grayscale_mean, contrast_factor) <p>= image * contrast_factor + grayscale_mean * (1 - contrast_factor)</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>contrast_factor</code> <code>float</code> <p>How much to adjust the contrast. Must be non-negative.             0 gives a gray image, 1 gives the original image,             values &gt; 1 increase contrast.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Contrast-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If contrast_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nhigh_contrast = adjust_contrast(img, contrast_factor=1.5)\nlow_contrast = adjust_contrast(img, contrast_factor=0.5)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.adjust_contrast_fast","title":"triton_augment.functional.adjust_contrast_fast","text":"<pre><code>adjust_contrast_fast(image: Tensor, contrast_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust contrast of an image using FAST centered scaling.</p> <p>This is faster than adjust_contrast() because it doesn't require computing the grayscale mean. Uses formula: output = (input - 0.5) * contrast_factor + 0.5</p> <p>NOTE: This is NOT equivalent to torchvision's adjust_contrast, but provides similar perceptual results and is fully fusible with other operations.</p> <p>Use this when: - You want maximum performance with fusion - Exact torchvision reproduction is not critical</p> <p>Use adjust_contrast() when: - You need exact torchvision compatibility - Reproducibility with torchvision is required</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>contrast_factor</code> <code>float</code> <p>How much to adjust the contrast. Must be non-negative.             0.5 decreases contrast, 1.0 gives original, &gt;1.0 increases.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Contrast-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If contrast_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nhigh_contrast = adjust_contrast_fast(img, contrast_factor=1.5)\n# Use in fused operation for maximum speed\nresult = fused_color_normalize(img, contrast_factor=1.5, ...)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.adjust_saturation","title":"triton_augment.functional.adjust_saturation","text":"<pre><code>adjust_saturation(image: Tensor, saturation_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust color saturation of an image.</p> <p>Matches torchvision.transforms.v2.functional.adjust_saturation exactly.</p> Formula <pre><code>output = blend(image, grayscale, saturation_factor)\n       = image * saturation_factor + grayscale * (1 - saturation_factor)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>saturation_factor</code> <code>float</code> <p>How much to adjust the saturation. Must be non-negative.               0 will give a grayscale image,               1 will give the original image,               values &gt; 1 increase saturation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Saturation-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If saturation_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\ngrayscale = adjust_saturation(img, saturation_factor=0.0)\nsaturated = adjust_saturation(img, saturation_factor=2.0)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.normalize","title":"triton_augment.functional.normalize","text":"<pre><code>normalize(image: Tensor, mean: tuple[float, float, float], std: tuple[float, float, float]) -&gt; torch.Tensor\n</code></pre> <p>Normalize a tensor image with mean and standard deviation.</p> This function normalizes each channel <p>output[c] = (input[c] - mean[c]) / std[c]</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>mean</code> <code>tuple[float, float, float]</code> <p>Tuple of mean values for each channel (R, G, B)</p> required <code>std</code> <code>tuple[float, float, float]</code> <p>Tuple of standard deviation values for each channel (R, G, B)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of the same shape and dtype</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nnormalized = normalize(img,\n                      mean=(0.485, 0.456, 0.406),\n                      std=(0.229, 0.224, 0.225))\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.rgb_to_grayscale","title":"triton_augment.functional.rgb_to_grayscale","text":"<pre><code>rgb_to_grayscale(image: Tensor, num_output_channels: int = 1, grayscale_mask: Tensor | None = None) -&gt; torch.Tensor\n</code></pre> <p>Convert RGB image to grayscale with optional per-image masking.</p> <p>Matches torchvision.transforms.v2.functional.rgb_to_grayscale exactly. Uses weights: 0.2989R + 0.587G + 0.114*B</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, 3, H, W) on CUDA</p> required <code>num_output_channels</code> <code>int</code> <p>Number of output channels (1 or 3)                 If 3, grayscale is replicated across channels</p> <code>1</code> <code>grayscale_mask</code> <code>Tensor | None</code> <p>Optional per-image mask [N] (uint8: 0=keep original, 1=convert to gray)            If None, converts all images</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Grayscale tensor of shape (N, num_output_channels, H, W)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_output_channels not in {1, 3} or if input not RGB</p> Example <pre><code>img = torch.rand(4, 3, 224, 224, device='cuda')\n# Convert all images\ngray = rgb_to_grayscale(img, num_output_channels=3)\n# Convert only some images (per-image mask)\nmask = torch.tensor([1, 0, 1, 0], dtype=torch.bool, device='cuda')\ngray = rgb_to_grayscale(img, num_output_channels=3, grayscale_mask=mask)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.crop","title":"triton_augment.functional.crop","text":"<pre><code>crop(image: Tensor, top: int | Tensor, left: int | Tensor, height: int, width: int) -&gt; torch.Tensor\n</code></pre> <p>Crop a rectangular region from the input image, with optional per-image crop positions.</p> <p>Matches torchvision.transforms.v2.functional.crop exactly when using scalar top/left. Reference: torchvision/transforms/v2/functional/_geometry.py line 1787</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>top</code> <code>int | Tensor</code> <p>Top pixel coordinate for cropping (int or int32 tensor of shape (N,) for per-image)</p> required <code>left</code> <code>int | Tensor</code> <p>Left pixel coordinate for cropping (int or int32 tensor of shape (N,) for per-image)</p> required <code>height</code> <code>int</code> <p>Height of the cropped image</p> required <code>width</code> <code>int</code> <p>Width of the cropped image</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Cropped tensor of shape (N, C, height, width)</p> Example <pre><code>img = torch.rand(2, 3, 224, 224, device='cuda')\n# Crop all images at same position\ncropped = crop(img, top=56, left=56, height=112, width=112)\n# Crop each image at different position\ntops = torch.tensor([56, 100], device='cuda', dtype=torch.int32)\nlefts = torch.tensor([56, 80], device='cuda', dtype=torch.int32)\ncropped = crop(img, top=tops, left=lefts, height=112, width=112)\n</code></pre> Note <p>For MVP, this requires valid crop coordinates (no padding). Future versions will support padding for out-of-bounds crops.</p>"},{"location":"api-reference/#triton_augment.functional.center_crop","title":"triton_augment.functional.center_crop","text":"<pre><code>center_crop(image: Tensor, output_size: tuple[int, int] | int) -&gt; torch.Tensor\n</code></pre> <p>Crop the center of the image to the given size.</p> <p>Matches torchvision.transforms.v2.functional.center_crop exactly. Reference: torchvision/transforms/v2/functional/_geometry.py line 2545</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>output_size</code> <code>tuple[int, int] | int</code> <p>Desired output size (height, width) or int for square crop</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Center-cropped tensor of shape (N, C, output_size[0], output_size[1])</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If output_size is larger than image size</p> Example <pre><code>img = torch.rand(2, 3, 224, 224, device='cuda')\n# Center crop to 112x112\ncropped = center_crop(img, (112, 112))\n# or for square crop\ncropped = center_crop(img, 112)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.horizontal_flip","title":"triton_augment.functional.horizontal_flip","text":"<pre><code>horizontal_flip(image: Tensor, flip_mask: Tensor | None = None) -&gt; torch.Tensor\n</code></pre> <p>Flip the image horizontally (left to right), with optional per-image control.</p> <p>Matches torchvision.transforms.v2.functional.horizontal_flip exactly when flip_mask=None. Reference: torchvision/transforms/v2/functional/_geometry.py line 56</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>flip_mask</code> <code>Tensor | None</code> <p>Optional uint8 tensor of shape (N,) indicating which images to flip (0=no flip, 1=flip).       If None, flips all images (default behavior).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Horizontally flipped tensor of the same shape</p> Example <pre><code>img = torch.rand(2, 3, 224, 224, device='cuda')\n# Flip all images\nflipped = horizontal_flip(img)\n# Flip only first image\nflip_mask = torch.tensor([1, 0], device='cuda', dtype=torch.bool)\nflipped = horizontal_flip(img, flip_mask)\n</code></pre> Note <p>This uses a custom Triton kernel. For standalone flip operations, PyTorch's tensor.flip(-1) is highly optimized and may be comparable. The main benefit is when fusing with crop (see fused_crop_flip).</p>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/#triton_augment.enable_autotune","title":"triton_augment.enable_autotune","text":"<pre><code>enable_autotune()\n</code></pre> <p>Enable kernel auto-tuning for optimal performance.</p> <p>When enabled, Triton will test multiple kernel configurations and cache the best one for your GPU and image sizes.</p> Example <pre><code>import triton_augment as ta\nta.enable_autotune()\n# Now kernels will auto-tune on first use\n</code></pre>"},{"location":"api-reference/#triton_augment.disable_autotune","title":"triton_augment.disable_autotune","text":"<pre><code>disable_autotune()\n</code></pre> <p>Disable kernel auto-tuning and use fixed defaults.</p> <p>When disabled, kernels use fixed configurations that work well across most GPUs and image sizes without tuning overhead.</p> Example <pre><code>import triton_augment as ta\nta.disable_autotune()\n# Now kernels will use fixed defaults (faster, good performance)\n</code></pre>"},{"location":"api-reference/#triton_augment.is_autotune_enabled","title":"triton_augment.is_autotune_enabled","text":"<pre><code>is_autotune_enabled() -&gt; bool\n</code></pre> <p>Check if auto-tuning is currently enabled.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if auto-tuning is enabled, False otherwise</p>"},{"location":"api-reference/#triton_augment.warmup_cache","title":"triton_augment.warmup_cache","text":"<pre><code>warmup_cache(batch_sizes: Tuple[int, ...] = (32, 64), image_sizes: Tuple[int, ...] = (224, 256, 512), verbose: bool = True)\n</code></pre> <p>Pre-populate the auto-tuning cache for common image sizes.</p> <p>This function runs the fused kernel with various common configurations to trigger auto-tuning and cache the optimal settings. This eliminates the 5-10 second delay on first use.</p> <p>Parameters:</p> Name Type Description Default <code>batch_sizes</code> <code>Tuple[int, ...]</code> <p>Tuple of batch sizes to warm up (default: (32, 64))</p> <code>(32, 64)</code> <code>image_sizes</code> <code>Tuple[int, ...]</code> <p>Tuple of square image sizes to warm up (default: (224, 256, 512))</p> <code>(224, 256, 512)</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress messages (default: True)</p> <code>True</code> Example <pre><code>import triton_augment as ta\n# Warm up cache for common training scenarios\nta.warmup_cache()\n# Custom sizes for your specific use case\nta.warmup_cache(batch_sizes=(16, 128), image_sizes=(128, 384))\n</code></pre>"},{"location":"api-reference/#input-requirements","title":"Input Requirements","text":"<p>All operations accept:</p> <ul> <li>Device: CUDA (GPU) or CPU - CPU tensors are automatically moved to GPU</li> <li>Shape: <code>(C, H, W)</code> or <code>(N, C, H, W)</code> - 3D tensors are automatically batched</li> <li>Dtype: float32 or float16</li> <li>Range: [0, 1] for color operations (required)</li> </ul> <p>Notes: - After normalization, values can be outside [0, 1] range - 3D tensors <code>(C, H, W)</code> are automatically converted to <code>(1, C, H, W)</code> for processing - CPU tensors are automatically transferred to CUDA for GPU processing</p>"},{"location":"api-reference/#performance-tips","title":"Performance Tips","text":""},{"location":"api-reference/#1-use-fused-kernel-even-for-partial-operations","title":"1. Use Fused Kernel Even for Partial Operations","text":"<p>Key insight: Even if you only need a subset of operations, use <code>TritonFusedAugment</code> or <code>F.fused_augment</code> for best performance! Simply set unused operations to no-op values:</p> <pre><code># Example: Only need crop + normalize (no flip, no color jitter)\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    horizontal_flip_p=0.0,      # No flip\n    brightness=0.0,             # No brightness\n    contrast=0.0,               # No contrast\n    saturation=0.0,             # No saturation\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n# Still faster than calling crop() + normalize() separately!\n</code></pre> <p>The fused kernel is optimized to skip operations set to no-op values at compile time.</p>"},{"location":"api-reference/#2-individual-operations-performance","title":"2. Individual Operations Performance","text":"<p>Individual Triton operations (e.g., <code>ta.crop()</code>, <code>ta.adjust_brightness()</code>): - Small images/batches: Slightly slower than torchvision (kernel launch overhead) - Large images/batches: Faster than torchvision (better GPU utilization)</p> <p>Recommendation: Use <code>TritonFusedAugment</code> for production training, regardless of how many operations you need.</p>"},{"location":"api-reference/#3-auto-tuning","title":"3. Auto-Tuning","text":"<p>Enable auto-tuning for optimal performance on your specific GPU and data sizes:</p> <pre><code>import triton_augment as ta\n\nta.enable_autotune()  # Enable once at start of training\n\n# Optional: Pre-compile kernels for your data sizes\nta.warmup_cache(batch_sizes=(32, 64), image_sizes=(224, 512))\n</code></pre> <p>See Auto-Tuning Guide for detailed configuration.</p>"},{"location":"api-reference/#additional-resources","title":"Additional Resources","text":"<ul> <li>Quick Start Guide: Training integration examples</li> <li>Float16 Support: Half-precision performance and memory savings</li> <li>Contrast Notes: Differences between fast and torchvision-exact contrast</li> <li>Batch Behavior: Understanding <code>same_on_batch</code> parameter</li> <li>Benchmark Results: Detailed performance comparisons</li> </ul>"},{"location":"auto-tuning/","title":"Auto-Tuning Guide","text":"<p>Default Behavior</p> <p>Auto-tuning is DISABLED by default for faster startup and good-enough performance. Enable it only if you need the extra performance boost.</p>"},{"location":"auto-tuning/#what-is-auto-tuning","title":"What is Auto-Tuning?","text":"<p>Auto-tuning automatically finds the optimal kernel configuration (block size, warp count, pipeline depth) for your specific GPU and image sizes.</p> <p>When enabled, Triton-Augment auto-tunes the fused kernel (<code>fused_augment</code> / <code>TritonFusedAugment</code>) while simple operations (brightness, saturation, normalize) always use fixed defaults.</p>"},{"location":"auto-tuning/#enabling-auto-tuning","title":"Enabling Auto-Tuning","text":""},{"location":"auto-tuning/#option-1-python-api","title":"Option 1: Python API","text":"<pre><code>import triton_augment as ta\n\n# Enable auto-tuning for optimal performance\nta.enable_autotune()\n\n# Check status\nprint(ta.is_autotune_enabled())  # True\n\n# Disable if needed\nta.disable_autotune()\n</code></pre>"},{"location":"auto-tuning/#option-2-environment-variable","title":"Option 2: Environment Variable","text":"<pre><code>export TRITON_AUGMENT_ENABLE_AUTOTUNE=1\npython train.py\n</code></pre>"},{"location":"auto-tuning/#auto-tuning-details","title":"Auto-Tuning Details","text":""},{"location":"auto-tuning/#auto-tuned-kernel","title":"Auto-Tuned Kernel","text":"<p>The fused kernel (<code>fused_augment</code>) is auto-tuned across these parameters:</p> <ul> <li>BLOCK_SIZE: Number of elements processed per thread block (256, 512, 1024, 2048)</li> <li>num_warps: Thread group size for parallelism (2, 4, 8)</li> <li>num_stages: Memory pipeline depth for memory-compute overlap (2, 3, 4)</li> </ul> <p>Triton tests 12 configurations and caches the fastest one for your specific workload.</p>"},{"location":"auto-tuning/#fixed-kernels","title":"Fixed Kernels","text":"<p>Simple operations use <code>BLOCK_SIZE=1024</code> for optimal performance on most GPUs: - <code>adjust_brightness</code> - <code>adjust_saturation</code> - <code>normalize</code></p> <p>These don't need auto-tuning as they're simple memory-bound operations.</p>"},{"location":"auto-tuning/#how-it-works","title":"How It Works","text":"<ol> <li>First run: Auto-tuning tests 12 configurations and caches the best one (5-10 seconds)</li> <li>Subsequent runs: Uses cached optimal configuration (zero overhead)</li> <li>Per GPU + size: Cache is specific to your GPU model and total elements (N\u00d7C\u00d7H\u00d7W)</li> </ol>"},{"location":"auto-tuning/#cache-warm-up-when-auto-tuning-is-enabled","title":"Cache Warm-Up (When Auto-Tuning is Enabled)","text":"<p>To avoid auto-tuning delays during training, warm up the cache once:</p>"},{"location":"auto-tuning/#cli","title":"CLI","text":"<pre><code># Enable auto-tuning and warm up\nTRITON_AUGMENT_ENABLE_AUTOTUNE=1 python -m triton_augment.warmup \\\n    --batch-sizes 64,128 \\\n    --image-sizes 320,384,640\n</code></pre>"},{"location":"auto-tuning/#python-api","title":"Python API","text":"<pre><code>import triton_augment as ta\n\n# Enable and warm up\nta.enable_autotune()\nta.warmup_cache(\n    batch_sizes=(64, 128),\n    image_sizes=(320, 384, 640)\n)\n</code></pre> <p>Important Notes</p> <ul> <li>Auto-tuning must be ENABLED for warmup to test multiple configs</li> <li>Without auto-tuning, warmup just compiles the default config</li> <li>Auto-tuning is size-specific: A cache for 224\u00d7224 won't help with 320\u00d7320</li> <li>Always use your actual training dimensions</li> </ul>"},{"location":"auto-tuning/#when-auto-tuning-is-disabled-default","title":"When Auto-Tuning is Disabled (Default)","text":"<ul> <li>Uses a single well-tuned default configuration</li> <li>Zero auto-tuning overhead</li> <li>No need to warm up cache</li> <li>Good performance on most GPUs</li> <li>First use still takes ~1-2 seconds to compile (but no testing/selection)</li> </ul>"},{"location":"auto-tuning/#performance-impact","title":"Performance Impact","text":""},{"location":"auto-tuning/#expected-improvements-when-enabled","title":"Expected Improvements (When Enabled)","text":"<p>Auto-tuning typically provides: - 5-15% speedup on most operations - Best gains on:   - Larger images (512\u00d7512+)   - Larger batch sizes (64+)   - Data center GPUs (A100, H100) - Minimal gains on:   - Small images (&lt; 128\u00d7128)   - Small batches (&lt; 16)   - Consumer GPUs (RTX 30xx series)</p>"},{"location":"auto-tuning/#trade-offs","title":"Trade-offs","text":"Aspect Disabled (Default) Enabled First-run speed Fast (~1-2 sec) Slow (~5-10 sec) Steady-state performance Good (95-98%) Optimal (100%) Cache warmup needed No Yes (recommended) Best for Most users, rapid iteration Production, max performance"},{"location":"auto-tuning/#benchmarking-withwithout-auto-tuning","title":"Benchmarking With/Without Auto-Tuning","text":"<p>The recommended workflow is simple: benchmark the default config first, then enable auto-tuning and benchmark again.</p>"},{"location":"auto-tuning/#using-benchmark-scripts","title":"Using Benchmark Scripts","text":"<p>Run Default Config First!</p> <p>Always benchmark without <code>--autotune</code> first, then with <code>--autotune</code>. Once auto-tuning runs, it caches the config and you can't go back without clearing cache.</p> <pre><code># Step 1: Benchmark default config first\npython examples/benchmark.py\n\n# Step 2: Then benchmark with auto-tuning\npython examples/benchmark.py --autotune\n</code></pre> <p>Or the comprehensive benchmark: <pre><code>python examples/benchmark_triton.py --autotune\n</code></pre></p>"},{"location":"auto-tuning/#standard-benchmarking-workflow-custom-code","title":"Standard Benchmarking Workflow (Custom Code)","text":"<pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Step 1: Benchmark default config (auto-tuning is disabled by default), run it first!\nprint(\"Benchmarking default config...\")\ntime_default = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Default config: {time_default:.3f} ms\")\n\n# Step 2: Enable auto-tuning and benchmark\nprint(\"\\nEnabling auto-tuning...\")\nta.enable_autotune()\n\n# First call triggers auto-tuning (takes 5-10 seconds, only once)\nprint(\"Running auto-tuning (this will take ~5-10 seconds)...\")\n_ = transform(img)\n\n# Now benchmark with optimal config\nprint(\"Benchmarking auto-tuned config...\")\ntime_tuned = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Auto-tuned config: {time_tuned:.3f} ms\")\n\n# Compare\nspeedup = time_default / time_tuned\nprint(f\"\\nSpeedup: {speedup:.2f}x\")\n</code></pre>"},{"location":"auto-tuning/#for-reproducible-comparisons-google-colab","title":"For Reproducible Comparisons (Google Colab)","text":"<p>If you need truly isolated benchmarks on Google Colab (where cache may persist across runtime restarts), use two separate colab notebooks:</p> <p>Notebook 1 - Default Config: <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\n# Auto-tuning is disabled by default\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Default config: {time_ms:.3f} ms\")\n</code></pre></p> <p>Notebook 2 - Auto-Tuned Config: <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\n# Enable auto-tuning in fresh notebook\nta.enable_autotune()\n\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Trigger auto-tuning\n_ = transform(img)\n\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Auto-tuned config: {time_ms:.3f} ms\")\n</code></pre></p> <p>Colab Cache Behavior</p> <p>Google Colab's cache directory (<code>~/.triton/cache</code>) may persist across runtime restarts within the same session. Using separate notebooks ensures completely independent benchmarks.</p>"},{"location":"auto-tuning/#benchmarking-on-sharedcloud-services","title":"Benchmarking on Shared/Cloud Services","text":"<p>Instability on Colab, Kaggle, and Cloud GPUs</p> <p>If you're benchmarking on Google Colab, Kaggle Notebooks, or other shared cloud services, you may see unstable or inconsistent results.</p>"},{"location":"auto-tuning/#why-benchmarks-can-be-unstable","title":"Why Benchmarks Can Be Unstable","text":"<p>Shared GPU services can cause significant performance variability:</p> <ol> <li>Shared Physical GPU - Multiple users on the same GPU compete for resources</li> <li>Variable GPU Allocation - You might get different GPU models between sessions</li> <li>Thermal Throttling - GPU performance degrades when hot from other users' workloads</li> <li>Background Processes - Cloud platform monitoring and management overhead</li> <li>Network I/O - Data transfers can interfere with kernel execution timing</li> </ol>"},{"location":"auto-tuning/#symptoms-of-instability","title":"Symptoms of Instability","text":"<p>You might see: - Wildly varying benchmark times (e.g., 0.5ms one run, 200ms the next) - Incorrect speedups (e.g., 0.00x or negative speedups) - Different results between runs with identical code - Auto-tuning picking suboptimal configs due to noisy measurements</p>"},{"location":"auto-tuning/#best-practices-for-stable-benchmarks","title":"Best Practices for Stable Benchmarks","text":"<p>If you must benchmark on shared services:</p> <ol> <li> <p>Run multiple iterations and take the median:    <pre><code>from triton.testing import do_bench\n\n# do_bench already uses median of multiple runs\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\n</code></pre></p> </li> <li> <p>Warm up thoroughly before benchmarking:    <pre><code># Warm up: compile kernels and stabilize GPU state\nfor _ in range(10):\n    _ = transform(img)\ntorch.cuda.synchronize()\n\n# Now benchmark\ntime_ms = do_bench(lambda: transform(img))\n</code></pre></p> </li> <li> <p>Use a dedicated session - Close other notebooks/tabs using the GPU</p> </li> <li> <p>Restart runtime if results seem anomalous</p> </li> <li> <p>Run at off-peak times - Early morning or late night (timezone-dependent)</p> </li> <li> <p>Compare trends, not absolute numbers - Look for consistent relative speedups</p> </li> </ol>"},{"location":"auto-tuning/#for-production-benchmarks","title":"For Production Benchmarks","text":"<p>For reliable, production-grade benchmarks:</p> <ul> <li>Use dedicated GPU instances (AWS P3/P4, GCP A2, Azure NC-series)</li> <li>Lock GPU clocks to prevent throttling (requires root):   <pre><code>sudo nvidia-smi -lgc 1410,1410  # Lock to max clock\n</code></pre></li> <li>Isolate the GPU - No other processes using it</li> <li>Multiple runs - Run benchmarks 5-10 times and report mean \u00b1 std dev</li> </ul>"},{"location":"auto-tuning/#recommendation","title":"Recommendation","text":"<ul> <li>For most users: Keep auto-tuning disabled (default)</li> <li>Faster startup</li> <li>Good performance out-of-the-box</li> <li> <p>No cache management needed</p> </li> <li> <p>For production/max performance: Enable auto-tuning</p> </li> <li>Warm up cache during deployment</li> <li>Squeeze out last 5-15% performance</li> <li>Worth it for long training runs</li> </ul>"},{"location":"batch-behavior/","title":"Batch Behavior &amp; Different Parameters Per Sample","text":"<p>Different Parameters Per Sample by Default</p> <p>Triton-Augment applies different random parameters to each image in a batch by default!</p>"},{"location":"batch-behavior/#default-behavior-different-parameters-per-sample","title":"Default Behavior: Different Parameters Per Sample","text":"<pre><code>import torch\nimport triton_augment as ta\n\n# Each image gets DIFFERENT random augmentation\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2\n)\n\nresult = transform(batch)  # 32 different random augmentations! \u2705\n</code></pre> <p>How it works: - Random parameters are sampled per-image (32 different crop positions, flip decisions, color factors) - All processed in ONE kernel launch on GPU - Fast batch processing + individual randomness = best of both worlds! \ud83d\ude80</p>"},{"location":"batch-behavior/#controlling-randomness-same_on_batch-flag","title":"Controlling Randomness: <code>same_on_batch</code> Flag","text":"<p>All transform classes support the <code>same_on_batch</code> parameter:</p>"},{"location":"batch-behavior/#different-parameters-per-sample-default","title":"Different Parameters Per Sample (Default)","text":"<pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=False  # Default\n)\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\nresult = transform(batch)  # Each image: different crop, flip, brightness\n</code></pre>"},{"location":"batch-behavior/#batch-wide-parameters-same-for-all","title":"Batch-Wide Parameters (Same for All)","text":"<pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=True  # Same params for all images\n)\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\nresult = transform(batch)  # All images: same crop position, flip, brightness\n</code></pre>"},{"location":"batch-behavior/#when-to-use-each-mode","title":"When to Use Each Mode","text":""},{"location":"batch-behavior/#different-parameters-per-sample-recommended-for-training","title":"Different Parameters Per Sample (Recommended for Training)","text":"<p>\u2705 Use when: - Training neural networks (standard augmentation) - You want maximum data diversity - Each image should be augmented independently</p> <pre><code># Standard training setup\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    same_on_batch=False  # \u2705 Default, each image different\n)\n\nfor images, labels in train_loader:\n    images = images.cuda()\n    images = transform(images)  # Unique augmentation per image\n    # ... training ...\n</code></pre> <p>Performance: Still fast! One kernel launch processes entire batch with per-image params.</p>"},{"location":"batch-behavior/#batch-wide-parameters-specialized-use-cases","title":"Batch-Wide Parameters (Specialized Use Cases)","text":"<p>\u2705 Use when: - Processing video frames (consistent augmentation across frames) - Debugging (easier to see effect of specific parameters) - Specific research requirements</p> <pre><code># Video frame processing\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=True  # Same for all frames\n)\n\nvideo_frames = torch.rand(8, 3, 224, 224, device='cuda')  # 8 frames\nresult = transform(video_frames)  # Consistent augmentation across frames \u2705\n</code></pre>"},{"location":"batch-behavior/#all-transforms-support-different-parameters-per-sample","title":"All Transforms Support Different Parameters Per Sample","text":"<p>The following transforms all support <code>same_on_batch</code>:</p> <ul> <li><code>TritonFusedAugment</code> - Complete pipeline (crop, flip, color, normalize)</li> <li><code>TritonRandomCropFlip</code> - Geometric operations only</li> <li><code>TritonColorJitterNormalize</code> - ColorJitter + Normalize</li> <li><code>TritonColorJitter</code> - ColorJitter only</li> <li><code>TritonRandomCrop</code> - Random cropping</li> <li><code>TritonRandomHorizontalFlip</code> - Random flipping</li> <li><code>TritonRandomGrayscale</code> - Random grayscale conversion</li> </ul> <p>Example: <pre><code># Individual transforms also support same_on_batch\ncrop = ta.TritonRandomCrop(112, same_on_batch=False)\nflip = ta.TritonRandomHorizontalFlip(p=0.5, same_on_batch=False)\njitter = ta.TritonColorJitter(brightness=0.2, same_on_batch=False)\n</code></pre></p>"},{"location":"batch-behavior/#functional-api-fixed-parameters","title":"Functional API: Fixed Parameters","text":"<p>The functional API (<code>triton_augment.functional</code>) is for deterministic augmentations:</p> <pre><code>import triton_augment.functional as F\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Fixed parameters - same for all images\nresult = F.fused_augment(\n    batch,\n    top=20, left=30,          # Fixed crop position\n    height=112, width=112,\n    flip_horizontal=True,      # Fixed flip decision\n    brightness_factor=1.2,     # Fixed brightness\n    saturation_factor=0.9,     # Fixed saturation\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>If you need per-image fixed parameters, pass tensors:</p> <pre><code># Per-image fixed parameters (different but deterministic)\ntop_offsets = torch.tensor([10, 20, 30, ...], device='cuda')  # [32]\nbrightness = torch.tensor([1.1, 1.2, 1.3, ...], device='cuda')  # [32]\n\nresult = F.fused_augment(\n    batch,\n    top=top_offsets,           # Tensor: per-image positions\n    left=30,                   # Scalar: same for all\n    height=112, width=112,\n    flip_horizontal=True,\n    brightness_factor=brightness,  # Tensor: per-image factors\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre>"},{"location":"batch-behavior/#comparison-with-torchvision","title":"Comparison with torchvision","text":"<p>torchvision doesn't support different parameters per sample on batched tensors:</p> <pre><code>import torchvision.transforms.v2 as tv_transforms\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = tv_transforms.ColorJitter(brightness=0.2)\n\n# All 32 images get the SAME brightness factor\nresult = transform(batch)\n</code></pre> <p>To get different parameters per sample in torchvision, you must apply transforms before batching (in DataLoader), which processes images sequentially.</p> <p>Triton-Augment advantage: Different parameters per sample with GPU batch processing - best of both worlds! \ud83d\ude80</p>"},{"location":"batch-behavior/#performance-impact","title":"Performance Impact","text":"<p>Different Parameters Per Sample: - \u2705 Same kernel launch time as batch-wide - \u2705 One kernel launch for entire batch - \u2705 Minimal overhead (kernel uses <code>tl.load</code> to fetch per-image params)</p> <p>Batch-Wide Parameters: - \u2705 Slightly faster (no per-image parameter indexing) - \u26a0\ufe0f Less data diversity for training</p> <p>Verdict: Use different parameters per sample (default) for training. The performance difference is negligible (~1-2%), but data diversity is crucial!</p>"},{"location":"batch-behavior/#example-real-training-pipeline","title":"Example: Real Training Pipeline","text":"<pre><code>import torch\nimport triton_augment as ta\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Step 1: Load data on CPU with workers (fast async I/O)\ntrain_dataset = datasets.CIFAR10(\n    './data', train=True,\n    transform=transforms.ToTensor()  # Only ToTensor on CPU\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    num_workers=4,      # \u2705 Async data loading\n    pin_memory=True\n)\n\n# Step 2: GPU augmentation with different parameters per sample\naugment = ta.TritonFusedAugment(\n    crop_size=28,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    mean=(0.4914, 0.4822, 0.4465),\n    std=(0.2470, 0.2435, 0.2616),\n    same_on_batch=False  # \u2705 Each image gets unique augmentation\n)\n\n# Step 3: Training loop\nfor images, labels in train_loader:\n    images, labels = images.cuda(), labels.cuda()\n    images = augment(images)  # \ud83d\ude80 One kernel, all augmentations, per-image random!\n\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    # ... backprop ...\n</code></pre> <p>Result: - \u2705 Fast async CPU data loading (<code>num_workers &gt; 0</code>) - \u2705 Fast GPU batch processing (one kernel) - \u2705 Different parameters per sample (maximum diversity) - \u2705 Best of all worlds! \ud83d\ude80</p>"},{"location":"contrast/","title":"Contrast Implementation","text":"<p>Important</p> <p>This library implements a different contrast algorithm than torchvision for speed and fusion benefits.</p>"},{"location":"contrast/#tldr","title":"TL;DR","text":"<ul> <li><code>fused_color_normalize()</code> uses fast contrast (not torchvision-exact)</li> <li>For exact torchvision results, use individual functions</li> <li>Fast contrast is production-proven (same as NVIDIA DALI)</li> </ul>"},{"location":"contrast/#three-equivalent-ways-torchvision-exact","title":"Three Equivalent Ways (Torchvision-Exact)","text":"<p>All three approaches below produce pixel-perfect identical results:</p>"},{"location":"contrast/#1-torchvision","title":"1. Torchvision","text":"<pre><code>import torchvision.transforms.v2.functional as tvF\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nresult = tvF.adjust_brightness(img, 1.2)\nresult = tvF.adjust_contrast(result, 1.1)\nresult = tvF.adjust_saturation(result, 0.9)\nmean_t = torch.tensor([0.485, 0.456, 0.406], device='cuda').view(1, 3, 1, 1)\nstd_t = torch.tensor([0.229, 0.224, 0.225], device='cuda').view(1, 3, 1, 1)\nresult = (result - mean_t) / std_t\n</code></pre> <p>\u23f1\ufe0f Speed: Baseline</p>"},{"location":"contrast/#2-triton-individual-functions-exact","title":"2. Triton Individual Functions (Exact)","text":"<pre><code>import triton_augment.functional as F\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nresult = F.adjust_brightness(img, 1.2)\nresult = F.adjust_contrast(result, 1.1)        # Torchvision-exact\nresult = F.adjust_saturation(result, 0.9)\nresult = F.normalize(result, \n                     mean=(0.485, 0.456, 0.406),\n                     std=(0.229, 0.224, 0.225))\n</code></pre> <p>\u23f1\ufe0f Speed: Faster (optimized Triton kernels) \u26a1</p>"},{"location":"contrast/#3-triton-contrast-fused-exact-fast","title":"3. Triton Contrast + Fused (Exact + Fast)","text":"<pre><code>import triton_augment.functional as F\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\n# Apply exact contrast first\nresult = F.adjust_brightness(img, 1.2)\nresult = F.adjust_contrast(result, 1.1)        # Torchvision-exact\n\n# Then fuse remaining ops (no contrast)\nresult = F.fused_augment(\n    result,\n    brightness_factor=1.0,                     # Identity (already applied)\n    contrast_factor=1.0,                       # Identity (already applied)\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>\u23f1\ufe0f Speed: Fast (3 kernel launches) \u26a1\u26a1</p>"},{"location":"contrast/#for-maximum-speed-not-exact","title":"For Maximum Speed (Not Exact)","text":"<p>If you don't need exact torchvision reproduction:</p> <pre><code>import triton_augment.functional as F\n\n# Single fused kernel - fastest!\nresult = F.fused_augment(\n    img,\n    brightness_factor=1.2,\n    contrast_factor=1.1,                       # Fast contrast (different from torchvision)\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>\u23f1\ufe0f Speed: Fastest (single fused kernel) \ud83d\ude80</p> <p>Note</p> <p>Fast contrast uses <code>(pixel - 0.5) * factor + 0.5</code> instead of torchvision's blend-with-mean.</p>"},{"location":"contrast/#fast-contrast-comparison-to-other-libraries","title":"Fast Contrast: Comparison to Other Libraries","text":"Library Formula Type Speed NVIDIA DALI <code>(x - 0.5) * f + 0.5</code> Linear (centered) Fastest \u2705 Triton-Augment (fast) <code>(x - 0.5) * f + 0.5</code> Linear (centered) Fastest \u2705 OpenCV <code>alpha * x + beta</code> Linear Fast Torchvision <code>x * f + mean * (1-f)</code> Linear (mean) Slower Scikit-image <code>1/(1+exp(-gain*(x-cut)))</code> Sigmoid (S-curve) Slowest"},{"location":"contrast/#why-this-formula","title":"Why This Formula?","text":"<p>\u2705 Production-proven: Same as NVIDIA DALI \u2705 Fast &amp; fusible: No mean computation required  </p> <p>For exact torchvision reproduction, use <code>adjust_contrast()</code> instead of fast mode.</p>"},{"location":"contrast/#technical-details","title":"Technical Details","text":""},{"location":"contrast/#torchvision-contrast","title":"Torchvision Contrast","text":"<pre><code>grayscale_mean = mean(rgb_to_grayscale(input))\noutput = input * contrast_factor + grayscale_mean * (1 - contrast_factor)\n</code></pre> <p>Problem for fusion: Requires computing the mean of the entire image, which: - Needs an extra kernel launch - Breaks the fusion (can't fuse before knowing the mean) - Creates a data dependency</p>"},{"location":"contrast/#fast-contrast","title":"Fast Contrast","text":"<pre><code>output = (input - 0.5) * contrast_factor + 0.5\n</code></pre> <p>Benefits: - No mean computation needed - Fully fusible with other operations - Single kernel launch for entire pipeline - 0.5 is a reasonable default anchor point (middle of [0, 1] range)</p>"},{"location":"contrast/#impact-on-training","title":"Impact on Training","text":"<p>In practice, the difference is minimal: - Models learn to be robust to data augmentation variations - The specific contrast formula matters less than having contrast augmentation at all - NVIDIA DALI uses this approach in production ML systems worldwide</p>"},{"location":"float16/","title":"Float16 (Half Precision) Support","text":"<p>Triton-Augment fully supports float16, providing memory savings and potential speedup on modern GPUs.</p>"},{"location":"float16/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nimport triton_augment as ta\n\n# Create float16 images\nimages = torch.rand(32, 3, 224, 224, device='cuda', dtype=torch.float16)\n\n# Apply fused transform (works seamlessly with float16)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # Output is also float16\n</code></pre>"},{"location":"float16/#benefits","title":"Benefits","text":"<p>\ud83d\udcbe Half the memory: Float16 uses 2x less VRAM, enabling: - Larger batch sizes - Higher resolution images - More models in memory</p> <p>\u26a1 Potential speedup: On Tesla T4, we observed ~1.3-1.4x speedup for large images (1024\u00d71024+)</p> <p>\u2705 Maintained accuracy: Data augmentation is robust to lower precision</p>"},{"location":"float16/#benchmark-results-tesla-t4","title":"Benchmark Results (Tesla T4)","text":"<p>Our measurements on Ultimate Fusion (all operations in one kernel):</p> Image Size Float32 Float16 Speedup 256\u00d7256 0.41 ms 0.47 ms 0.88x (slower) 512\u00d7512 0.48 ms 0.47 ms 1.03x 640\u00d7640 0.57 ms 0.49 ms 1.15x 1024\u00d71024 0.93 ms 0.72 ms 1.29x 1280\u00d71280 1.27 ms 0.93 ms 1.36x <p>Conclusion: Float16 provides meaningful speedup for large images (600\u00d7600+), but offers minimal benefit for small images.</p> <p>\ud83d\udca1 Your mileage may vary: Run <code>examples/benchmark_triton.py</code> to measure on your GPU.</p>"},{"location":"float16/#when-to-use-float16","title":"When to Use Float16","text":""},{"location":"float16/#use-float16-when","title":"\u2705 Use Float16 When:","text":"<ul> <li>Training with mixed precision (<code>torch.cuda.amp</code>)</li> <li>Memory constrained: Need to fit larger batches or higher resolution images</li> <li>Large images: 600\u00d7600+ where float16 shows speedup (based on T4 benchmarks)</li> </ul>"},{"location":"float16/#skip-float16-when","title":"\u274c Skip Float16 When:","text":"<ul> <li>Small images: &lt; 512\u00d7512 (minimal or negative speedup on T4)</li> <li>CPU-only training: Float16 is GPU-specific</li> <li>Debugging: Float32 is easier to inspect</li> </ul>"},{"location":"float16/#precision-considerations","title":"Precision Considerations","text":"<p>Float16 results will differ slightly from float32 due to reduced precision. This is expected and acceptable for data augmentation:</p> <ul> <li>Models are robust to small input perturbations</li> <li>Augmentation inherently introduces variation</li> <li>Training with mixed precision is a standard practice</li> </ul>"},{"location":"float16/#usage-example","title":"Usage Example","text":"<p>With mixed precision training:</p> <pre><code>from torch.cuda.amp import autocast, GradScaler\nimport triton_augment as ta\n\ntransform = ta.TritonFusedAugment(...)\nscaler = GradScaler()\n\nfor images, labels in loader:\n    with autocast():  # Images automatically converted to float16\n        images = images.cuda()\n        augmented = transform(images)\n        output = model(augmented)\n        loss = criterion(output, labels)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre> <p>Manual float16 conversion:</p> <pre><code># Convert to float16 for memory savings\nimages = images.half().cuda()\naugmented = transform(images)  # Stays in float16\n</code></pre>"},{"location":"float16/#benchmarking","title":"Benchmarking","text":"<p>Compare float16 vs float32 performance:</p> <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\nbatch = 32\nimg_fp32 = torch.rand(batch, 3, 224, 224, device='cuda', dtype=torch.float32)\nimg_fp16 = img_fp32.half()\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112, brightness=0.2, saturation=0.2\n)\n\n# Benchmark\ntime_fp32 = do_bench(lambda: transform(img_fp32))\ntime_fp16 = do_bench(lambda: transform(img_fp16))\n\nprint(f\"Float32: {time_fp32:.3f} ms\")\nprint(f\"Float16: {time_fp16:.3f} ms\")\nprint(f\"Speedup: {time_fp32/time_fp16:.2f}x\")\n</code></pre>"},{"location":"float16/#why-float16-can-be-faster","title":"Why Float16 Can Be Faster","text":"<p>Float16 benefits come from: 1. Memory bandwidth: Half the data to transfer (2 bytes vs 4 bytes per value) 2. Cache efficiency: More data fits in GPU caches 3. GPU hardware: Modern GPUs have specialized float16 units</p> <p>Note: Speedup varies by GPU architecture and operation complexity. Always benchmark on your specific hardware.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>PyTorch &gt;= 2.0.0</li> <li>Triton &gt;= 2.0.0</li> <li>CUDA-capable GPU</li> </ul>"},{"location":"installation/#installation-from-pypi-recommended","title":"Installation from PyPI (Recommended)","text":"<pre><code>pip install triton-augment\n</code></pre>"},{"location":"installation/#development-installation-from-source","title":"Development Installation (From Source)","text":"<p>For contributors or those who want to modify the code:</p> <pre><code>git clone https://github.com/yuhezhang-ai/triton-augment.git\ncd triton-augment\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#input-requirements","title":"Input Requirements","text":"<p>Input Requirements</p> <ul> <li>Range: Pixel values must be in <code>[0, 1]</code> (use <code>transforms.ToTensor()</code> if loading from PIL)</li> <li>Device: GPU only (CPU tensors are automatically moved to CUDA)</li> <li>Shape: Supports both 3D <code>(C, H, W)</code> and 4D <code>(N, C, H, W)</code> tensors (automatic batching)</li> <li>Dtype: <code>float32</code> or <code>float16</code></li> </ul>"},{"location":"installation/#first-run-behavior","title":"First Run Behavior","text":"<p>On first use, Triton will compile kernels for your GPU (~1-2 seconds per image size with default config). This is normal and only happens once per GPU and image size.</p> <p>Optional: Cache Warm-Up</p> <p>To avoid compilation delays during training, you can optionally warm up the cache after installation:</p> <pre><code>python -m triton_augment.warmup\n</code></pre> <p>For more details and auto-tuning optimization, see the Auto-Tuning Guide.</p>"},{"location":"installation/#what-to-expect","title":"What to expect","text":"<ul> <li>First import: Helpful message about auto-tuning status (can be suppressed with <code>TRITON_AUGMENT_SUPPRESS_FIRST_RUN_MESSAGE=1</code>)</li> <li>First use of each image size: ~1-2 seconds (kernel compilation)</li> <li>Subsequent uses: Instant (kernels are cached)</li> </ul>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Should work without errors\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ntransform = ta.TritonColorJitterNormalize(brightness=0.2)\nresult = transform(img)\nprint(\"\u2705 Installation successful!\")\n</code></pre>"},{"location":"quickstart/","title":"Quick Start Guide","text":""},{"location":"quickstart/#basic-usage-ultimate-fusion-recommended","title":"Basic Usage: Ultimate Fusion (Recommended) \ud83d\ude80","text":"<p>The fastest way to use Triton-Augment - fuse ALL augmentations in a single kernel:</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Create a batch of images on GPU\nimages = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Replace torchvision Compose (7 kernel launches)\n# With Triton-Augment (1 kernel launch - significantly faster!)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    random_grayscale_p=0.1,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Apply transformation\naugmented = transform(images)  # Single kernel launch for ALL operations!\n</code></pre> <p>What it does: - RandomCrop (112\u00d7112) - RandomHorizontalFlip (50% probability) - ColorJitter (brightness, contrast, saturation) - Normalize</p> <p>Performance: Up to 12x faster on large images (8.1x average on Tesla T4, scales dramatically with image size)</p>"},{"location":"quickstart/#other-fusion-options","title":"Other Fusion Options","text":""},{"location":"quickstart/#pixel-only-fusion","title":"Pixel-Only Fusion","text":"<p>If you don't need geometric operations (crop/flip), use pixel fusion:</p> <pre><code># Fuse color jitter + normalize (single kernel)\ntransform = ta.TritonColorJitterNormalize(\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # Faster, single fused kernel\n</code></pre>"},{"location":"quickstart/#geometric-only-fusion","title":"Geometric-Only Fusion","text":"<p>If you only need crop + flip:</p> <pre><code># Fuse crop + flip (single kernel)\ntransform = ta.TritonRandomCropFlip(size=112, horizontal_flip_p=0.5)\n\naugmented = transform(images)  # ~1.5-2x faster\n</code></pre>"},{"location":"quickstart/#individual-operations","title":"Individual Operations","text":"<p>For maximum control, use individual operations (fixed parameters):</p> <pre><code>import triton_augment.functional as F\n\nimg = torch.rand(4, 3, 224, 224, device='cuda')\n\n# Geometric operations\ncropped = F.crop(img, top=20, left=30, height=112, width=112)\nflipped = F.horizontal_flip(img)\n\n# Color operations \nbright = F.adjust_brightness(img, 1.2)\nsaturated = F.adjust_saturation(img, 0.9)\nnormalized = F.normalize(img, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n</code></pre> <p>Or use transform classes (random augmentations):</p> <pre><code>import triton_augment as ta\n\n# Individual transforms\ncrop = ta.TritonRandomCrop(112)\nflip = ta.TritonRandomHorizontalFlip(p=0.5)\njitter = ta.TritonColorJitter(brightness=0.2)\nnormalize = ta.TritonNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n</code></pre>"},{"location":"quickstart/#training-integration","title":"Training Integration","text":"<p>Recommended Pattern: Load data on CPU (fast async I/O), augment on GPU (fast batch processing)</p> <pre><code>import torch\nimport triton_augment as ta\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Step 1: CPU data loading with workers\ntrain_dataset = datasets.CIFAR10(\n    './data', train=True,\n    transform=transforms.ToTensor()  # Only ToTensor on CPU\n)\ntrain_loader = DataLoader(\n    train_dataset, batch_size=128,\n    num_workers=4, pin_memory=True  # Fast async loading!\n)\n\n# Step 2: GPU augmentation transform\naugment = ta.TritonFusedAugment(\n    crop_size=28, horizontal_flip_p=0.5,\n    brightness=0.2, saturation=0.2,\n    mean=(0.4914, 0.4822, 0.4465),\n    std=(0.2470, 0.2435, 0.2616)\n)\n\n# Step 3: Apply in training loop\nfor images, labels in train_loader:\n    images, labels = images.cuda(), labels.cuda()\n    images = augment(images)  # All ops in 1 kernel! \ud83d\ude80\n    # ... rest of training ...\n</code></pre> <p>Why This Pattern:</p> <ul> <li>\u2705 Fast async data loading: <code>num_workers &gt; 0</code> for CPU parallelism</li> <li>\u2705 Fast GPU batch processing: All augmentations in 1 fused kernel</li> <li>\u2705 Different parameters per sample: Each image gets different random parameters (default)</li> <li>\u2705 Best of both worlds: CPU for I/O, GPU for compute</li> <li>\u2705 Kernel fusion: No intermediate memory allocations</li> <li>\u2705 Large batch advantage: Speedup increases with batch size</li> </ul> <p>Note: Set <code>same_on_batch=True</code> if you want all images to share the same random parameters.</p> <p>\ud83d\udca1 Pro Tip: Apply Triton-Augment transforms AFTER moving tensors to GPU for maximum performance!</p> <p>Full Examples: See <code>examples/train_mnist.py</code> and <code>examples/train_cifar10.py</code> for complete training scripts with neural networks.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Float16 Support - Use half-precision for 1.3-2x additional speedup</li> <li>Batch Behavior - Understand random parameter handling</li> <li>Contrast Notes - Fast contrast vs torchvision-exact</li> <li>Auto-Tuning - Optional performance optimization</li> <li>API Reference - Complete API documentation</li> </ul>"}]}