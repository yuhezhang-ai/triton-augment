{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#triton-augment","title":"Triton-Augment","text":"<p>GPU-Accelerated Image Augmentation with Kernel Fusion</p> <p> </p> <p>Triton-Augment is a high-performance image augmentation library that leverages OpenAI Triton to fuse common transform operations, providing significant speedups over standard PyTorch implementations.</p>"},{"location":"#5-73x-faster-than-torchvisionkornia-on-image-and-video-augmentation","title":"\u26a1 5 - 73x Faster than Torchvision/Kornia on Image and Video Augmentation","text":"<p>Replace your augmentation pipeline with a single fused kernel and get:</p> <ul> <li> <p>Image Speedup: 8.1x average speedup on Tesla T4 and up to 12x faster on large images (1280\u00d71280) - compared to torchvision.transforms.v2.</p> </li> <li> <p>Video Speedup: 5D video tensor support with <code>same_on_batch=False, same_on_frame=True</code> control. Speedup: 8.6x vs Torchvision, 73.7x vs Kornia \ud83d\ude80</p> </li> </ul> <p>\ud83d\udcca See full benchmarks \u2192</p> <p>Key Idea: Fuse multiple GPU operations into a single kernel \u2192 eliminate intermediate memory transfers \u2192 faster augmentation.</p> <pre><code># Traditional (torchvision Compose): 7 kernel launches\ncrop \u2192 flip \u2192 brightness \u2192 contrast \u2192 saturation \u2192 grayscale \u2192 normalize\n\n# Triton-Augment Ultimate Fusion: 1 kernel launch \ud83d\ude80\n[crop + flip + brightness + contrast + saturation + grayscale + normalize]\n</code></pre>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>One Kernel, All Operations: Fuse crop, flip, color jitter, grayscale, and normalize in a single kernel - significantly faster, scales with image size! \ud83d\ude80</li> <li>5D Video Tensor Support: Native support for <code>[N, T, C, H, W]</code> video tensors with <code>same_on_frame</code> control for consistent augmentation across frames</li> <li>Different Parameters Per Sample: Each image in batch gets different random augmentations (not just batch-wide)</li> <li>Zero Memory Overhead: No intermediate buffers between operations</li> <li>Drop-in Replacement: torchvision-like transforms &amp; functional APIs, easy migration</li> <li>Auto-Tuning: Optional performance optimization for your GPU</li> <li>Float16 Ready: ~1.3x speedup on large images + 50% memory savings</li> </ul>"},{"location":"#quick-start","title":"\ud83d\udce6 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install triton-augment\n</code></pre> <p>Requirements: Python 3.8+, PyTorch 2.0+, CUDA-capable GPU</p> <p>Try it now:  - Test correctness and run benchmarks without local setup</p> <p>Note: Colab is a shared service - performance may vary due to GPU allocation and resource contention. For stable benchmarking, use a dedicated GPU.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Recommended: Ultimate Fusion \ud83d\ude80</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Create batch of images on GPU\nimages = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Replace torchvision Compose (7 kernel launches)\n# With Triton-Augment (1 kernel launch - significantly faster!)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    grayscale_p=0.1,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # \ud83d\ude80 Single kernel for entire pipeline!\n</code></pre> <p>Video (5D) Support: Native support for video tensors <code>[N, T, C, H, W]</code>:</p> <pre><code># Video batch: 8 videos \u00d7 16 frames \u00d7 3 channels \u00d7 224\u00d7224\nvideos = torch.rand(8, 16, 3, 224, 224, device='cuda')\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2, contrast=0.2, saturation=0.2,\n    same_on_frame=True,  # Same augmentation for all frames (default)\n    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(videos)  # Shape: [8, 16, 3, 112, 112]\n</code></pre> <p>Need only some operations? Set unused parameters to their default values:</p> <pre><code># Example: Only saturation adjustment + horizontal flip\ntransform = ta.TritonFusedAugment(\n    crop_size=224,          # No crop (same size as input)\n    saturation=0.2,         # Only saturation jitter\n    horizontal_flip_p=0.5,  # Only random flip\n)\n</code></pre> <p>Specialized APIs: For convenience, also available: <code>TritonColorJitterNormalize</code>, <code>TritonRandomCropFlip</code>, etc.</p>"},{"location":"#combine-with-torchvision-transforms","title":"\ud83d\udd17 Combine with Torchvision Transforms","text":"<p>For operations not yet supported by Triton-Augment (like rotation, perspective transforms, etc.), combine with torchvision transforms:</p> <pre><code>import torchvision.transforms.v2 as transforms\n\n# Triton-Augment + Torchvision (per-image randomness + unsupported ops)\ntransform = transforms.Compose([\n    transforms.RandomRotation(degrees=15),  # Torchvision (no per-image randomness)\n    ta.TritonColorJitterNormalize(         # Triton-Augment (per-image randomness)\n        brightness=0.2, contrast=0.2, saturation=0.2,\n        mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n    )\n])\n</code></pre> <p>Note: Torchvision transforms.v2 apply the same random parameters to all images in a batch, while Triton-Augment provides true per-image randomness. Kornia also supports per-image randomness, but is slower in our benchmarks.</p> <p>\u2192 More Examples</p>"},{"location":"#input-requirements","title":"\u26a0\ufe0f Input Requirements","text":"<ul> <li>Range: Images must be in <code>[0, 1]</code> range (e.g., use <code>torchvision.transforms.ToTensor()</code>)</li> <li>Device: GPU (CUDA) - CPU tensors automatically moved to GPU</li> <li>Shape: <code>(C, H, W)</code>, <code>(N, C, H, W)</code>, or <code>(N, T, C, H, W)</code> - 5D for video</li> <li>Dtype: <code>float32</code> or <code>float16</code></li> </ul>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Full documentation: Navigation menu on the left (or see GitHub repo <code>docs/</code> folder)</p> Guide Description Quick Start Get started in 5 minutes with examples Installation Setup and requirements API Reference Complete API documentation for all functions and classes Contrast Notes Fused kernel uses fast contrast (different from torchvision). See how to get exact torchvision results Auto-Tuning Optional performance optimization for your GPU and data size (disabled by default). Includes cache warm-up guide Batch Behavior Different parameters per sample (default) vs batch-wide parameters. Understanding <code>same_on_batch</code> flag Float16 Support Use half-precision for ~1.3x speedup (large images) and 50% memory savings Comparison with Other Libraries How Triton-Augment compares to DALI, Kornia, and when to use each"},{"location":"#performance","title":"\u26a1 Performance","text":"<p>\ud83d\udcca Run benchmarks yourself on Google Colab - Verify correctness and performance on free GPU Note: Colab performance may vary due to shared resources</p>"},{"location":"#image-augmentation-benchmark-results","title":"Image Augmentation Benchmark Results","text":"<p>Real training scenario with random augmentations on Tesla T4 (Google Colab Free Tier):</p> Image Size Batch Crop Size Torchvision Triton Fused Speedup 256\u00d7256 32 224\u00d7224 2.48 ms 0.56 ms 4.5x 256\u00d7256 64 224\u00d7224 4.51 ms 0.69 ms 6.5x 600\u00d7600 32 512\u00d7512 11.82 ms 1.26 ms 9.4x 1280\u00d71280 32 1024\u00d71024 48.91 ms 4.07 ms 12.0x <p>Average Speedup: 8.1x \ud83d\ude80</p> <p>Operations: RandomCrop + RandomHorizontalFlip + ColorJitter + RandomGrayscale + Normalize</p> <p>Note: Benchmarks use <code>torchvision.transforms.v2</code> (not the legacy v1 API) for comparison.</p> <p>Performance scales with image size \u2014 larger images benefit more from kernel fusion:</p> <p> </p> <p>\ud83d\udcca Additional Benchmarks (NVIDIA A100 on Google Colab):</p> Image Size Batch Crop Size Torchvision Triton Fused Speedup 256\u00d7256 32 224\u00d7224 0.61 ms 0.44 ms 1.4x 256\u00d7256 64 224\u00d7224 0.93 ms 0.43 ms 2.1x 600\u00d7600 32 512\u00d7512 2.19 ms 0.50 ms 4.4x 1280\u00d71280 32 1024\u00d71024 8.23 ms 0.94 ms 8.7x <p>Average: 4.1x</p> <p>Why better speedup on T4? Kernel fusion reduces memory bandwidth bottlenecks, which matters more on bandwidth-limited GPUs like T4 (320 GB/s) vs A100 (1,555 GB/s). This means greater benefits on consumer and mid-range hardware.</p>"},{"location":"#video-5d-tensor-benchmarks","title":"Video (5D Tensor) Benchmarks","text":"<p>Video augmentation on Tesla T4 (Google Colab Free Tier) - Input shape <code>[N, T, C, H, W]</code>:</p> Batch Frames Image Size Crop Size Torchvision Kornia VideoSeq Triton Fused Speedup vs TV Speedup vs Kornia 8 16 256\u00d7256 224\u00d7224 8.86 ms 78.20 ms 1.21 ms 7.3x 64.6x 4 32 256\u00d7256 224\u00d7224 8.84 ms 78.39 ms 1.08 ms 8.2x 72.6x 16 8 256\u00d7256 224\u00d7224 9.06 ms 78.69 ms 1.07 ms 8.5x 73.5x 8 16 512\u00d7512 448\u00d7448 33.75 ms 272.59 ms 3.24 ms 10.4x 84.1x <p>Average Speedup vs Torchvision: 8.6x Average Speedup vs Kornia: 73.7x \ud83d\ude80</p>"},{"location":"#run-your-own-benchmarks","title":"Run Your Own Benchmarks","text":"<p>Quick Benchmark (Ultimate Fusion only): <pre><code># Simple, clean table output - easy to run!\npython examples/benchmark.py\npython examples/benchmark_video.py\n</code></pre></p> <p>Detailed Benchmark (All operations): <pre><code># Comprehensive analysis with visualizations\npython examples/benchmark_triton.py\n</code></pre></p>"},{"location":"#auto-tuning","title":"\ud83d\udca1 Auto-Tuning","text":"<p>All benchmark results shown above use default kernel configurations. Auto-tuning can provide additional speedup on dedicated GPUs.</p> <p>What is Auto-Tuning?</p> <p>Triton kernels have tunable parameters (block sizes, warps per thread, etc.) that affect performance. Auto-tuning automatically searches for the optimal configuration for your specific GPU and data sizes.</p> <p>When to use:</p> <ul> <li> <p>\u2705 Dedicated GPUs (local workstations, cloud instances): 10-30% additional speedup</p> </li> <li> <p>\u26a0\ufe0f Shared services (Colab, Kaggle): Limited benefits, but can help stabilize performance</p> </li> </ul> <p>Quick start: <pre><code>import triton_augment as ta\n\nta.set_autotune(True)  # Enable auto-tuning (one-time cost, results cached)\ntransform = ta.TritonFusedAugment(...)\naugmented = transform(images)  # First run: tests configs; subsequent: uses cache\n</code></pre></p> <p>\u26a0\ufe0f Performance Variability: Our highly optimized kernels are more sensitive to resource contention. If you experience sudden latency spikes on shared services, this is expected due to competing workloads. Auto-tuning can help find more stable configurations.</p> <p>\ud83d\udcd6 Full guide: Auto-Tuning Guide - Detailed instructions, cache management, and warm-up strategies</p>"},{"location":"#when-to-use-triton-augment","title":"\ud83c\udfaf When to Use Triton-Augment?","text":"<p>Use Triton-Augment + Torchvision together:</p> <ul> <li>Torchvision: Data loading, resize, ToTensor, rotation, affine, etc.</li> <li>Triton-Augment: Replace supported operations (currently: crop, flip, color jitter, grayscale, normalize; more coming) with fused GPU kernels</li> </ul> <p>Best speedup when:</p> <ul> <li>Large images (500x500+) or large batches</li> <li>Data augmentations are your bottleneck</li> </ul> <p>Stick with Torchvision only if:</p> <ul> <li>CPU-only training</li> <li>Experiencing extreme latency variability on shared services (e.g., consistent 10x+ spikes) - our optimized kernels are more sensitive to resource contention. Try auto-tuning first; if instability persists, Torchvision may be more stable</li> </ul> <p>\ud83d\udca1 TL;DR: Use both! Triton-Augment replaces Torchvision's fusible ops for 8-12x speedup.</p>"},{"location":"#training-integration","title":"\ud83c\udf93 Training Integration","text":"<p>Want to use Triton-Augment in your training pipeline? See the Quick Start Guide for:</p> <ul> <li>Complete training examples (MNIST, CIFAR-10)</li> <li>DataLoader integration patterns</li> <li>Best practices for CPU data loading + GPU augmentation</li> <li>Why this architecture is fast</li> </ul> <p>Quick snippet:</p> <pre><code># Step 1: Load data on CPU with workers\ntrain_loader = DataLoader(..., num_workers=4)\n\n# Step 2: Create GPU augmentation (once)\naugment = ta.TritonFusedAugment(crop_size=28, ...)\n\n# Step 3: Apply in training loop on GPU batches\nfor images, labels in train_loader:\n    images = images.cuda()\n    images = augment(images)  # \ud83d\ude80 1 kernel for all ops!\n    outputs = model(images)\n</code></pre> <p>\u2192 Full Training Guide</p>"},{"location":"#roadmap","title":"\ud83d\udccb Roadmap","text":"<ul> <li>[x] Phase 1: Fused color operations (brightness, contrast, saturation, normalize)</li> <li>[x] Phase 1.5: Grayscale, float16 support, auto-tuning</li> <li>[x] Phase 2: Basic Geometric operations (crop, flip) + Ultimate fusion \ud83d\ude80</li> <li>[x] Phase 2.5: 5D video tensor support <code>[N, T, C, H, W]</code> with <code>same_on_frame</code> parameter</li> <li>[ ] Phase 3: Extended operations (resize, rotation, blur, erasing, mixup, etc.)</li> <li>[ ] Future: Differentiable augmentation (autograd support, available in Kornia) - evaluate demand vs performance tradeoff</li> </ul> <p>\u2192 Detailed Roadmap</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions welcome! Please see CONTRIBUTING.md for guidelines.</p> <pre><code># Development setup\npip install -e \".[dev]\"\n\n# Useful commands\nmake help        # Show all available commands\nmake test        # Run tests\n</code></pre> <p>\u2192 Complete Contributing Guide</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>Apache License 2.0 - see LICENSE file.</p>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>OpenAI Triton - GPU programming framework</li> <li>PyTorch - Deep learning foundation</li> <li>torchvision - API inspiration</li> </ul>"},{"location":"#author","title":"\ud83d\udc64 Author","text":"<p>Yuhe Zhang</p> <ul> <li>\ud83d\udcbc LinkedIn: Yuhe Zhang</li> <li>\ud83d\udce7 Email: yuhezhang.zju @ gmail.com</li> </ul> <p>Research interests: Applied ML, Computer Vision, Efficient Deep Learning, GPU Acceleration</p>"},{"location":"#project","title":"\ud83d\udce7 Project","text":"<ul> <li>Issues and feature requests: GitHub Issues</li> <li>PyPI Package: pypi.org/project/triton-augment</li> </ul> <p>\u2b50 If you find this library useful, please consider starring the repo! \u2b50</p>"},{"location":"DOCUMENTATION/","title":"Documentation Guide","text":"<p>This document explains how to build, preview, and deploy the Triton-Augment documentation.</p>"},{"location":"DOCUMENTATION/#structure","title":"Structure","text":"<pre><code>README.md                 # Home page (shared with GitHub)\ndocs/\n\u251c\u2500\u2500 installation.md       # Installation guide\n\u251c\u2500\u2500 quickstart.md        # Quick start tutorial\n\u251c\u2500\u2500 float16.md           # Float16 support\n\u251c\u2500\u2500 batch-behavior.md    # Batch behavior guide\n\u251c\u2500\u2500 contrast.md          # Contrast implementation details\n\u251c\u2500\u2500 auto-tuning.md       # Auto-tuning guide\n\u251c\u2500\u2500 api-reference.md     # API reference\n\u251c\u2500\u2500 requirements.txt     # Docs dependencies\n\u2514\u2500\u2500 stylesheets/\n    \u2514\u2500\u2500 extra.css        # Custom CSS\n</code></pre> <p>Note: The home page uses the root <code>README.md</code> to avoid duplication between GitHub and documentation site.</p>"},{"location":"DOCUMENTATION/#building-the-documentation","title":"Building the Documentation","text":""},{"location":"DOCUMENTATION/#install-dependencies","title":"Install Dependencies","text":"<pre><code># From project root\npip install -r docs/requirements.txt\n</code></pre>"},{"location":"DOCUMENTATION/#preview-locally","title":"Preview Locally","text":"<pre><code># Start local server (with auto-reload)\nmkdocs serve\n\n# Open in browser\n# http://127.0.0.1:8000\n</code></pre> <p>The server will auto-reload when you edit documentation files.</p>"},{"location":"DOCUMENTATION/#build-static-site","title":"Build Static Site","text":"<pre><code># Build HTML files\nmkdocs build\n\n# Output is in site/ directory\n# Open site/index.html in browser to preview\n</code></pre>"},{"location":"DOCUMENTATION/#deploying-to-github-pages","title":"Deploying to GitHub Pages","text":""},{"location":"DOCUMENTATION/#option-1-automatic-recommended","title":"Option 1: Automatic (Recommended)","text":"<pre><code># Build and deploy in one command\nmkdocs gh-deploy\n\n# This will:\n# 1. Build the site\n# 2. Push to gh-pages branch\n# 3. GitHub will serve it automatically\n</code></pre> <p>Your docs will be available at: <code>https://&lt;username&gt;.github.io/&lt;repo&gt;/</code></p>"},{"location":"DOCUMENTATION/#option-2-manual","title":"Option 2: Manual","text":"<pre><code># Build locally\nmkdocs build\n\n# Copy site/ contents to your web server\n# Or commit site/ to gh-pages branch manually\n</code></pre>"},{"location":"DOCUMENTATION/#documentation-workflow","title":"Documentation Workflow","text":""},{"location":"DOCUMENTATION/#adding-a-new-page","title":"Adding a New Page","text":"<ol> <li>Create new <code>.md</code> file in <code>docs/</code></li> <li>Add entry to <code>mkdocs.yml</code> navigation</li> <li>Preview with <code>mkdocs serve</code></li> <li>Commit and deploy</li> </ol> <p>Example:</p> <pre><code># mkdocs.yml\nnav:\n  - Home: index.md\n  - Getting Started:\n      - Installation: installation.md\n      - Quick Start: quickstart.md\n      - Your New Page: new-page.md  # Add here\n</code></pre>"},{"location":"DOCUMENTATION/#updating-api-reference","title":"Updating API Reference","text":"<p>The API reference (<code>docs/api-reference.md</code>) is manually written. To add new functions:</p> <ol> <li>Add function documentation to <code>api-reference.md</code></li> <li>Follow the existing format (code examples + parameter descriptions)</li> <li>Add cross-references to relevant guides</li> </ol> <p>For auto-generated API docs (future): - Use <code>mkdocstrings</code> plugin (already configured) - Add docstring references: <code>triton_augment.functional</code></p>"},{"location":"DOCUMENTATION/#writing-style","title":"Writing Style","text":"<ul> <li>Use code examples: Show, don't just tell</li> <li>Add admonitions: <code>!!! note</code>, <code>!!! warning</code>, <code>!!! tip</code></li> <li>Include cross-references: Link related pages</li> <li>Keep it concise: Short paragraphs, clear headings</li> <li>Use tables: For comparisons and parameter lists</li> </ul>"},{"location":"DOCUMENTATION/#markdown-extensions","title":"Markdown Extensions","text":"<p>Available extensions (configured in <code>mkdocs.yml</code>):</p> <pre><code># Admonitions (call-out boxes)\n!!! note \"Optional Title\"\n    This is a note\n\n!!! warning\n    This is a warning\n\n# Code blocks with syntax highlighting\n```python\nimport triton_augment as ta\n</code></pre>"},{"location":"DOCUMENTATION/#tabbed-content","title":"Tabbed content","text":"Tab 1Tab 2 <p>Content 1</p> <p>Content 2</p>"},{"location":"DOCUMENTATION/#tables","title":"Tables","text":"Header 1 Header 2 Cell 1 Cell 2 <pre><code>## Theme Customization\n\n### Colors\n\nEdit `mkdocs.yml`:\n\n```yaml\ntheme:\n  palette:\n    primary: indigo  # Change to: red, pink, purple, etc.\n    accent: indigo\n</code></pre>"},{"location":"DOCUMENTATION/#logo-and-favicon","title":"Logo and Favicon","text":"<p>Add files: - <code>docs/images/logo.png</code> - <code>docs/images/favicon.ico</code></p> <p>Update <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  logo: images/logo.png\n  favicon: images/favicon.ico\n</code></pre>"},{"location":"DOCUMENTATION/#custom-css","title":"Custom CSS","text":"<p>Edit <code>docs/stylesheets/extra.css</code> for custom styles.</p>"},{"location":"DOCUMENTATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DOCUMENTATION/#build-errors","title":"Build Errors","text":"<pre><code># Clear cache\nrm -rf site/\n\n# Rebuild\nmkdocs build\n</code></pre>"},{"location":"DOCUMENTATION/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code># Reinstall\npip install -r docs/requirements.txt --upgrade\n</code></pre>"},{"location":"DOCUMENTATION/#gh-deploy-issues","title":"gh-deploy Issues","text":"<pre><code># Check git status\ngit status\n\n# Ensure you're on main branch\ngit checkout main\n\n# Try force deploy\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"DOCUMENTATION/#best-practices","title":"Best Practices","text":"<ol> <li>Test locally before deploying (<code>mkdocs serve</code>)</li> <li>Keep README short - link to full docs</li> <li>Update docs when adding features</li> <li>Use examples - code speaks louder than words</li> <li>Cross-reference - link related pages</li> <li>Version docs - note when features were added</li> </ol>"},{"location":"DOCUMENTATION/#continuous-integration-optional","title":"Continuous Integration (Optional)","text":"<p>Add GitHub Actions to auto-deploy docs:</p> <pre><code># .github/workflows/docs.yml\nname: Deploy Documentation\n\non:\n  push:\n    branches: [main]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.x\n      - run: pip install -r docs/requirements.txt\n      - run: mkdocs gh-deploy --force\n</code></pre>"},{"location":"DOCUMENTATION/#resources","title":"Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>mkdocstrings</li> <li>Markdown Guide</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete auto-generated API reference for all Triton-Augment operations.</p>"},{"location":"api-reference/#transform-classes","title":"Transform Classes","text":"<p>Transform classes provide stateful, random augmentations similar to torchvision. Recommended for training pipelines.</p>"},{"location":"api-reference/#triton_augment.TritonFusedAugment","title":"triton_augment.TritonFusedAugment","text":"<p>               Bases: <code>Module</code></p> <p>Fused augmentation: All operations in ONE kernel.</p> <p>This transform combines ALL augmentations in a single GPU kernel launch: - Geometric Tier: RandomCrop + RandomHorizontalFlip - Pixel Tier: ColorJitter (brightness, contrast, saturation) + Normalize</p> <p>Performance: up to 12x faster than torchvision.transforms.Compose!</p> <p>This is the PEAK PERFORMANCE path - single kernel launch for entire pipeline. No intermediate memory allocations or kernel launch overhead.</p> <p>Parameters:</p> Name Type Description Default <code>crop_size</code> <code>int | tuple[int, int]</code> <p>Desired output size (int or tuple). If int, output is square (crop_size, crop_size).</p> required <code>horizontal_flip_p</code> <code>float</code> <p>Probability of horizontal flip (default: 0.0, no flip)</p> <code>0.0</code> <code>brightness</code> <code>float | tuple[float, float]</code> <p>How much to jitter brightness. If float, chosen uniformly from [max(0, 1-brightness), 1+brightness].        If tuple, chosen uniformly from [brightness[0], brightness[1]].</p> <code>0</code> <code>contrast</code> <code>float | tuple[float, float]</code> <p>How much to jitter contrast (same format as brightness)</p> <code>0</code> <code>saturation</code> <code>float | tuple[float, float]</code> <p>How much to jitter saturation (same format as brightness)</p> <code>0</code> <code>grayscale_p</code> <code>float</code> <p>Probability of converting to grayscale (default: 0.0, no grayscale)</p> <code>0.0</code> <code>mean</code> <code>Optional[tuple[float, float, float]]</code> <p>Sequence of means for R, G, B channels. If None, normalization is skipped.</p> <code>None</code> <code>std</code> <code>Optional[tuple[float, float, float]]</code> <p>Sequence of stds for R, G, B channels. If None, normalization is skipped.</p> <code>None</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch (N dimension) share the same random parameters.           If False (default), each image gets different random parameters.</p> <code>False</code> <code>same_on_frame</code> <code>bool</code> <p>If True, all frames in a video (T dimension) share the same random parameters.           If False, each frame gets different random parameters.           Only applies to 5D input [N, T, C, H, W]. Default: True (consistent augmentation across frames).</p> <code>True</code> Example <pre><code># Replace torchvision Compose with single transform\n# OLD (6 kernel launches):\ntransform = transforms.Compose([\n    transforms.RandomCrop(112),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\n# NEW (1 kernel launch - significantly faster!):\nimport triton_augment as ta\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),  # ImageNet normalization (optional)\n    std=(0.229, 0.224, 0.225)    # ImageNet normalization (optional)\n)\n\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nresult = transform(img)  # Single kernel launch!\n\n# Without normalization (mean=None, std=None by default)\ntransform_no_norm = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2, contrast=0.2, saturation=0.2\n)\n</code></pre> Note <ul> <li>Uses FAST contrast (centered scaling), not torchvision's blend-with-mean</li> <li>By default, each image gets different random parameters (set same_on_batch=False for same params)</li> <li>Input must be (C, H, W), (N, C, H, W), or (N, T, C, H, W) float tensor on CUDA in [0, 1] range</li> </ul>"},{"location":"api-reference/#triton_augment.TritonFusedAugment-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonFusedAugment.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply all augmentations in a single fused kernel.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)    Can be on CPU or CUDA (will be moved to CUDA automatically)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Augmented tensor of same shape and device as input</p>"},{"location":"api-reference/#triton_augment.TritonColorJitterNormalize","title":"triton_augment.TritonColorJitterNormalize","text":"<p>               Bases: <code>Module</code></p> <p>Combined color jitter, random grayscale, and normalization in a single fused operation.</p> <p>This class combines TritonColorJitter, TritonRandomGrayscale, and TritonNormalize  into a single operation that uses a fused kernel for maximum performance. This is the recommended way to apply color augmentations and normalization.</p> <p>Parameters:</p> Name Type Description Default <code>brightness</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter brightness (same as TritonColorJitter)</p> <code>None</code> <code>contrast</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter contrast (same as TritonColorJitter)</p> <code>None</code> <code>saturation</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter saturation (same as TritonColorJitter)</p> <code>None</code> <code>grayscale_p</code> <code>float</code> <p>Probability of converting to grayscale (default: 0.0)</p> <code>0.0</code> <code>mean</code> <code>Optional[Tuple[float, float, float]]</code> <p>Sequence of means for normalization (R, G, B). If None, normalization is skipped.</p> <code>None</code> <code>std</code> <code>Optional[Tuple[float, float, float]]</code> <p>Sequence of standard deviations for normalization (R, G, B). If None, normalization is skipped.</p> <code>None</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch share the same random parameters                  If False (default), each image in batch gets different random parameters</p> <code>False</code> Example <pre><code># Full augmentation pipeline in one transform (per-image randomness)\ntransform = TritonColorJitterNormalize(\n    brightness=0.2,  # Range: [0.8, 1.2]\n    contrast=0.2,    # Range: [0.8, 1.2]\n    saturation=0.2,  # Range: [0.8, 1.2]\n    grayscale_p=0.1,  # 10% chance of grayscale (per-image)\n    mean=(0.485, 0.456, 0.406),  # ImageNet normalization (optional)\n    std=(0.229, 0.224, 0.225),    # ImageNet normalization (optional)\n    same_on_batch=False\n)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\naugmented = transform(img)  # Each image gets different augmentation\n\n# Without normalization (mean=None, std=None by default)\ntransform_no_norm = TritonColorJitterNormalize(\n    brightness=0.2, contrast=0.2, saturation=0.2\n)\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonColorJitterNormalize-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonColorJitterNormalize.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply random color jitter and normalization in a single fused operation.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)  Can be on CPU or CUDA (will be moved to CUDA automatically)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Augmented and normalized tensor of same shape and device as input</p>"},{"location":"api-reference/#triton_augment.TritonRandomCropFlip","title":"triton_augment.TritonRandomCropFlip","text":"<p>               Bases: <code>Module</code></p> <p>Fused random crop + random horizontal flip.</p> <p>This class combines random crop and random horizontal flip in a SINGLE kernel launch, eliminating intermediate memory transfers.</p> <p>Performance: ~1.5-2x faster than applying TritonRandomCrop + TritonRandomHorizontalFlip sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Union[int, Sequence[int]]</code> <p>Desired output size (height, width) or int for square crop</p> required <code>horizontal_flip_p</code> <code>float</code> <p>Probability of horizontal flip (default: 0.5)</p> <code>0.5</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch (N dimension) share the same random parameters.           If False (default), each image gets different random parameters.</p> <code>False</code> <code>same_on_frame</code> <code>bool</code> <p>If True, all frames in a video (T dimension) share the same random parameters.           If False, each frame gets different random parameters.           Only applies to 5D input [N, T, C, H, W]. Default: True.</p> <code>True</code> Example <pre><code># Fused version (FAST - single kernel, per-image randomness)\ntransform_fused = TritonRandomCropFlip(112, horizontal_flip_p=0.5, same_on_batch=False)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nresult = transform_fused(img)  # Each image gets different crop &amp; flip\n\n# Equivalent sequential version (SLOWER - 2 kernels)\ntransform_seq = nn.Sequential(\n    TritonRandomCrop(112, same_on_batch=False),\n    TritonRandomHorizontalFlip(p=0.5, same_on_batch=False)\n)\nresult_seq = transform_seq(img)\n</code></pre> Note <p>The fused version uses compile-time branching (tl.constexpr), so there's zero overhead when flip is not triggered.</p>"},{"location":"api-reference/#triton_augment.TritonRandomCropFlip-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomCropFlip.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Randomly cropped (and optionally flipped) tensor of shape matching input</p>"},{"location":"api-reference/#triton_augment.TritonColorJitter","title":"triton_augment.TritonColorJitter","text":"<p>               Bases: <code>Module</code></p> <p>Randomly change the brightness, contrast, and saturation of an image.</p> <p>This is a GPU-accelerated version of torchvision.transforms.v2.ColorJitter that uses a fused kernel for maximum performance.</p> <p>IMPORTANT: Contrast uses FAST mode (centered scaling: <code>(pixel - 0.5) * factor + 0.5</code>), NOT torchvision's blend-with-mean approach. This is much faster and provides similar visual results.</p> <p>If you need exact torchvision behavior, use the individual functional APIs: - <code>F.adjust_brightness()</code> (exact) - <code>F.adjust_contrast()</code> (torchvision-exact, slower) - <code>F.adjust_saturation()</code> (exact)</p> <p>Parameters:</p> Name Type Description Default <code>brightness</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter brightness.        brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]        or the given [min, max]. Should be non negative numbers.</p> <code>None</code> <code>contrast</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter contrast (uses FAST mode).      contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]      or the given [min, max]. Should be non-negative numbers.</p> <code>None</code> <code>saturation</code> <code>Optional[Union[float, Sequence[float]]]</code> <p>How much to jitter saturation.        saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]        or the given [min, max]. Should be non negative numbers.</p> <code>None</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch (N dimension) share the same random parameters.           If False (default), each image gets different random parameters.</p> <code>False</code> <code>same_on_frame</code> <code>bool</code> <p>If True, all frames in a video (T dimension) share the same random parameters.           If False, each frame gets different random parameters.           Only applies to 5D input [N, T, C, H, W]. Default: True (consistent augmentation across frames).</p> <code>True</code> Example <pre><code># Basic usage with per-image randomness\ntransform = TritonColorJitter(\n    brightness=0.2,  # Range: [0.8, 1.2]\n    contrast=0.2,    # Range: [0.8, 1.2] (FAST contrast)\n    saturation=0.2,  # Range: [0.8, 1.2]\n    same_on_batch=False\n)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\naugmented = transform(img)  # Each image gets different augmentation\n\n# Custom ranges\ntransform = TritonColorJitter(\n    brightness=(0.5, 1.5),  # Custom range\n    contrast=(0.7, 1.3),     # Custom range (FAST mode)\n    saturation=(0.0, 2.0)    # Custom range\n)\n</code></pre> Performance <ul> <li>Uses fused kernel for all operations in a single pass</li> <li>Faster than sequential operations</li> <li>For even more speed, combine with normalization using TritonColorJitterNormalize</li> </ul>"},{"location":"api-reference/#triton_augment.TritonColorJitter-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonColorJitter.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Apply random color jitter to the input image tensor.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input image tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Augmented image tensor of the same shape and dtype</p>"},{"location":"api-reference/#triton_augment.TritonNormalize","title":"triton_augment.TritonNormalize","text":"<p>               Bases: <code>Module</code></p> <p>Normalize a tensor image with mean and standard deviation.</p> <p>This is a GPU-accelerated version of torchvision.transforms.Normalize that uses a Triton kernel for improved performance.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Tuple[float, float, float]</code> <p>Sequence of means for each channel (R, G, B)</p> required <code>std</code> <code>Tuple[float, float, float]</code> <p>Sequence of standard deviations for each channel (R, G, B)</p> required Example <pre><code># ImageNet normalization\nnormalize = TritonNormalize(\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nnormalized = normalize(img)\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonNormalize-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonNormalize.forward","title":"forward","text":"<pre><code>forward(img: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Normalize the input image tensor.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)  Can be on CPU or CUDA (will be moved to CUDA automatically)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of same shape and device as input</p>"},{"location":"api-reference/#triton_augment.TritonRandomGrayscale","title":"triton_augment.TritonRandomGrayscale","text":"<p>               Bases: <code>Module</code></p> <p>Randomly convert image to grayscale with probability p.</p> <p>Matches torchvision.transforms.v2.RandomGrayscale behavior with optional per-image randomness.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of converting to grayscale (default: 0.1)</p> <code>0.1</code> <code>num_output_channels</code> <code>int</code> <p>Number of output channels (1 or 3, default: 3)                 Usually 3 to maintain compatibility with RGB pipelines</p> <code>3</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch (N dimension) make the same grayscale decision.           If False (default), each image independently decides grayscale conversion.</p> <code>False</code> <code>same_on_frame</code> <code>bool</code> <p>If True, all frames in a video (T dimension) make the same grayscale decision.           If False, each frame independently decides.           Only applies to 5D input [N, T, C, H, W]. Default: True.</p> <code>True</code> Example <pre><code># Per-image randomness (each image independently converted)\ntransform = TritonRandomGrayscale(p=0.5, num_output_channels=3, same_on_batch=False)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nresult = transform(img)  # Each image has 50% chance of being grayscale\n\n# Batch-wide (all images converted or none)\ntransform = TritonRandomGrayscale(p=0.5, num_output_channels=3, same_on_batch=True)\nresult = transform(img)  # Either all 4 images are grayscale or none are\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonRandomGrayscale-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomGrayscale.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, 3, H, W), or (N, T, 3, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Image tensor, either original or grayscale based on probability</p>"},{"location":"api-reference/#triton_augment.TritonGrayscale","title":"triton_augment.TritonGrayscale","text":"<p>               Bases: <code>Module</code></p> <p>Convert image to grayscale.</p> <p>Matches torchvision.transforms.v2.Grayscale behavior. Uses weights: 0.2989R + 0.587G + 0.114*B</p> <p>Parameters:</p> Name Type Description Default <code>num_output_channels</code> <code>int</code> <p>Number of output channels (1 or 3).                 If 1, output is single-channel grayscale.                 If 3, grayscale is replicated to 3 channels.</p> <code>1</code> Example <pre><code>transform = TritonGrayscale(num_output_channels=3)\nimg = torch.rand(1, 3, 224, 224, device='cuda')\ngray = transform(img)  # Shape: (1, 3, 224, 224), all channels identical\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonGrayscale-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonGrayscale.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, 3, H, W), or (N, T, 3, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Grayscale tensor of shape matching input structure</p>"},{"location":"api-reference/#triton_augment.TritonRandomCrop","title":"triton_augment.TritonRandomCrop","text":"<p>               Bases: <code>Module</code></p> <p>Crop a random portion of the image.</p> <p>Matches torchvision.transforms.v2.RandomCrop behavior (simplified MVP version).</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Union[int, Sequence[int]]</code> <p>Desired output size (height, width) or int for square crop</p> required <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch (N dimension) crop at the same position.           If False (default), each image gets different random crop position.</p> <code>False</code> <code>same_on_frame</code> <code>bool</code> <p>If True, all frames in a video (T dimension) crop at the same position.           If False, each frame gets different random crop.           Only applies to 5D input [N, T, C, H, W]. Default: True.</p> <code>True</code> Example <p><pre><code>transform = TritonRandomCrop(112)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = transform(img)\ncropped.shape\n</code></pre> torch.Size([4, 3, 112, 112])</p> Note <p>For MVP, padding is not supported. Image must be larger than crop size. Future versions will support padding, pad_if_needed, fill, padding_mode.</p>"},{"location":"api-reference/#triton_augment.TritonRandomCrop-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomCrop.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Randomly cropped tensor of shape matching input</p>"},{"location":"api-reference/#triton_augment.TritonCenterCrop","title":"triton_augment.TritonCenterCrop","text":"<p>               Bases: <code>Module</code></p> <p>Crop the center of the image.</p> <p>Matches torchvision.transforms.v2.CenterCrop behavior.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Union[int, Sequence[int]]</code> <p>Desired output size (height, width) or int for square crop</p> required Example <p><pre><code>transform = TritonCenterCrop(112)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ncropped = transform(img)\ncropped.shape\n</code></pre> torch.Size([4, 3, 112, 112])</p>"},{"location":"api-reference/#triton_augment.TritonCenterCrop-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonCenterCrop.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Center-cropped tensor of shape matching input</p>"},{"location":"api-reference/#triton_augment.TritonRandomHorizontalFlip","title":"triton_augment.TritonRandomHorizontalFlip","text":"<p>               Bases: <code>Module</code></p> <p>Horizontally flip the image randomly with probability p.</p> <p>Matches torchvision.transforms.v2.RandomHorizontalFlip behavior.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of flipping (default: 0.5)</p> <code>0.5</code> <code>same_on_batch</code> <code>bool</code> <p>If True, all images in batch (N dimension) share the same flip decision.           If False (default), each image gets different random decision.</p> <code>False</code> <code>same_on_frame</code> <code>bool</code> <p>If True, all frames in a video (T dimension) share the same flip decision.           If False, each frame gets different random decision.           Only applies to 5D input [N, T, C, H, W]. Default: True.</p> <code>True</code> Example <pre><code>transform = TritonRandomHorizontalFlip(p=0.5)\nimg = torch.rand(4, 3, 224, 224, device='cuda')\nflipped = transform(img)  # Each image has 50% chance of being flipped\n</code></pre>"},{"location":"api-reference/#triton_augment.TritonRandomHorizontalFlip-functions","title":"Functions","text":""},{"location":"api-reference/#triton_augment.TritonRandomHorizontalFlip.forward","title":"forward","text":"<pre><code>forward(image: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input tensor of shape (C, H, W), (N, C, H, W), or (N, T, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Image tensor, either original or horizontally flipped</p>"},{"location":"api-reference/#functional-api","title":"Functional API","text":"<p>Low-level functional interface for fine-grained control with fixed parameters. Use when you need deterministic operations.</p>"},{"location":"api-reference/#triton_augment.functional.fused_augment","title":"triton_augment.functional.fused_augment","text":"<pre><code>fused_augment(image: Tensor, top: int | Tensor, left: int | Tensor, height: int, width: int, flip_horizontal: bool | Tensor = False, brightness_factor: float | Tensor = 1.0, contrast_factor: float | Tensor = 1.0, saturation_factor: float | Tensor = 1.0, grayscale: bool | Tensor = False, mean: tuple[float, float, float] | None = None, std: tuple[float, float, float] | None = None) -&gt; torch.Tensor\n</code></pre> <p>Fused augmentation: ALL operations in ONE kernel.</p> <p>Combines geometric (crop + flip) and pixel (color + normalize) operations in a single GPU kernel, providing maximum performance.</p> <p>Performance: Up to 12x faster on large images (8.1x average on Tesla T4, scales dramatically with image size)</p> <p>Operations applied in sequence: 1. Geometric: Crop + Optional Horizontal Flip (index transformations, per-image) 2. Pixel: Brightness + Contrast (fast) + Saturation + Random Grayscale + Normalize (value operations, per-image)</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>top</code> <code>int | Tensor</code> <p>Crop top offset (int or int32 tensor of shape (N,) for per-image)</p> required <code>left</code> <code>int | Tensor</code> <p>Crop left offset (int or int32 tensor of shape (N,) for per-image)</p> required <code>height</code> <code>int</code> <p>Crop height</p> required <code>width</code> <code>int</code> <p>Crop width</p> required <code>flip_horizontal</code> <code>bool | Tensor</code> <p>Whether to flip horizontally (bool or uint8 tensor of shape (N,) for per-image, default: False)</p> <code>False</code> <code>brightness_factor</code> <code>float | Tensor</code> <p>Brightness multiplier (float or tensor of shape (N,) for per-image, 1.0 = no change)</p> <code>1.0</code> <code>contrast_factor</code> <code>float | Tensor</code> <p>Contrast multiplier (float or tensor of shape (N,) for per-image, 1.0 = no change) [FAST mode]</p> <code>1.0</code> <code>saturation_factor</code> <code>float | Tensor</code> <p>Saturation multiplier (float or tensor of shape (N,) for per-image, 1.0 = no change)</p> <code>1.0</code> <code>grayscale</code> <code>bool | Tensor</code> <p>Whether to convert to grayscale (bool or uint8 tensor of shape (N,) for per-image, default: False)</p> <code>False</code> <code>mean</code> <code>tuple[float, float, float] | None</code> <p>Normalization mean parameters (None = skip normalization)</p> <code>None</code> <code>std</code> <code>tuple[float, float, float] | None</code> <p>Normalization std parameters (None = skip normalization)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Transformed tensor of shape (N, C, height, width)</p> Example <pre><code>img = torch.rand(4, 3, 224, 224, device='cuda')\n# Single kernel launch for ALL operations!\nresult = fused_augment(\n    img,\n    top=20, left=30, height=112, width=112,\n    flip_horizontal=True,\n    brightness_factor=1.2,\n    contrast_factor=1.1,\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Equivalent sequential operations (MUCH slower - 6 kernel launches):\nresult_seq = crop(img, 20, 30, 112, 112)\nresult_seq = horizontal_flip(result_seq)\nresult_seq = adjust_brightness(result_seq, 1.2)\nresult_seq = adjust_contrast_fast(result_seq, 1.1)\nresult_seq = adjust_saturation(result_seq, 0.9)\nresult_seq = normalize(result_seq, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n</code></pre> Note <ul> <li>Uses FAST contrast (centered scaling), not torchvision's blend-with-mean</li> <li>For torchvision-exact contrast, use sequential operations</li> <li>Zero intermediate memory allocations!</li> </ul>"},{"location":"api-reference/#triton_augment.functional.adjust_brightness","title":"triton_augment.functional.adjust_brightness","text":"<pre><code>adjust_brightness(image: Tensor, brightness_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust brightness of an image (MULTIPLICATIVE operation).</p> <p>Matches torchvision.transforms.v2.functional.adjust_brightness exactly. Reference: torchvision/transforms/v2/functional/_color.py line 114-125</p> <p>Formula: output = input * brightness_factor</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>brightness_factor</code> <code>float</code> <p>How much to adjust the brightness. Must be non-negative.               0 gives a black image, 1 gives the original image,               2 increases brightness by 2x.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Brightness-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If brightness_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nbright_img = adjust_brightness(img, brightness_factor=1.2)  # 20% brighter\ndark_img = adjust_brightness(img, brightness_factor=0.8)   # 20% darker\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.adjust_contrast","title":"triton_augment.functional.adjust_contrast","text":"<pre><code>adjust_contrast(image: Tensor, contrast_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust contrast of an image.</p> <p>Matches torchvision.transforms.v2.functional.adjust_contrast exactly. Reference: torchvision/transforms/v2/functional/_color.py line 190-206</p> output = blend(image, grayscale_mean, contrast_factor) <p>= image * contrast_factor + grayscale_mean * (1 - contrast_factor)</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>contrast_factor</code> <code>float</code> <p>How much to adjust the contrast. Must be non-negative.             0 gives a gray image, 1 gives the original image,             values &gt; 1 increase contrast.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Contrast-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If contrast_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nhigh_contrast = adjust_contrast(img, contrast_factor=1.5)\nlow_contrast = adjust_contrast(img, contrast_factor=0.5)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.adjust_contrast_fast","title":"triton_augment.functional.adjust_contrast_fast","text":"<pre><code>adjust_contrast_fast(image: Tensor, contrast_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust contrast of an image using FAST centered scaling.</p> <p>This is faster than adjust_contrast() because it doesn't require computing the grayscale mean. Uses formula: output = (input - 0.5) * contrast_factor + 0.5</p> <p>NOTE: This is NOT equivalent to torchvision's adjust_contrast, but provides similar perceptual results and is fully fusible with other operations.</p> <p>Use this when: - You want maximum performance with fusion - Exact torchvision reproduction is not critical</p> <p>Use adjust_contrast() when: - You need exact torchvision compatibility - Reproducibility with torchvision is required</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>contrast_factor</code> <code>float</code> <p>How much to adjust the contrast. Must be non-negative.             0.5 decreases contrast, 1.0 gives original, &gt;1.0 increases.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Contrast-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If contrast_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nhigh_contrast = adjust_contrast_fast(img, contrast_factor=1.5)\n# Use in fused operation for maximum speed\nresult = fused_color_normalize(img, contrast_factor=1.5, ...)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.adjust_saturation","title":"triton_augment.functional.adjust_saturation","text":"<pre><code>adjust_saturation(image: Tensor, saturation_factor: float) -&gt; torch.Tensor\n</code></pre> <p>Adjust color saturation of an image.</p> <p>Matches torchvision.transforms.v2.functional.adjust_saturation exactly.</p> Formula <pre><code>output = blend(image, grayscale, saturation_factor)\n       = image * saturation_factor + grayscale * (1 - saturation_factor)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>saturation_factor</code> <code>float</code> <p>How much to adjust the saturation. Must be non-negative.               0 will give a grayscale image,               1 will give the original image,               values &gt; 1 increase saturation.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Saturation-adjusted tensor of the same shape and dtype</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If saturation_factor is negative</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\ngrayscale = adjust_saturation(img, saturation_factor=0.0)\nsaturated = adjust_saturation(img, saturation_factor=2.0)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.normalize","title":"triton_augment.functional.normalize","text":"<pre><code>normalize(image: Tensor, mean: tuple[float, float, float], std: tuple[float, float, float]) -&gt; torch.Tensor\n</code></pre> <p>Normalize a tensor image with mean and standard deviation.</p> This function normalizes each channel <p>output[c] = (input[c] - mean[c]) / std[c]</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>mean</code> <code>tuple[float, float, float]</code> <p>Tuple of mean values for each channel (R, G, B)</p> required <code>std</code> <code>tuple[float, float, float]</code> <p>Tuple of standard deviation values for each channel (R, G, B)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor of the same shape and dtype</p> Example <pre><code>img = torch.rand(1, 3, 224, 224, device='cuda')\nnormalized = normalize(img,\n                      mean=(0.485, 0.456, 0.406),\n                      std=(0.229, 0.224, 0.225))\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.rgb_to_grayscale","title":"triton_augment.functional.rgb_to_grayscale","text":"<pre><code>rgb_to_grayscale(image: Tensor, num_output_channels: int = 1, grayscale_mask: Tensor | None = None) -&gt; torch.Tensor\n</code></pre> <p>Convert RGB image to grayscale with optional per-image masking.</p> <p>Matches torchvision.transforms.v2.functional.rgb_to_grayscale exactly. Uses weights: 0.2989R + 0.587G + 0.114*B</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, 3, H, W) on CUDA</p> required <code>num_output_channels</code> <code>int</code> <p>Number of output channels (1 or 3)                 If 3, grayscale is replicated across channels</p> <code>1</code> <code>grayscale_mask</code> <code>Tensor | None</code> <p>Optional per-image mask [N] (uint8: 0=keep original, 1=convert to gray)            If None, converts all images</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Grayscale tensor of shape (N, num_output_channels, H, W)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_output_channels not in {1, 3} or if input not RGB</p> Example <pre><code>img = torch.rand(4, 3, 224, 224, device='cuda')\n# Convert all images\ngray = rgb_to_grayscale(img, num_output_channels=3)\n# Convert only some images (per-image mask)\nmask = torch.tensor([1, 0, 1, 0], dtype=torch.bool, device='cuda')\ngray = rgb_to_grayscale(img, num_output_channels=3, grayscale_mask=mask)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.crop","title":"triton_augment.functional.crop","text":"<pre><code>crop(image: Tensor, top: int | Tensor, left: int | Tensor, height: int, width: int) -&gt; torch.Tensor\n</code></pre> <p>Crop a rectangular region from the input image, with optional per-image crop positions.</p> <p>Matches torchvision.transforms.v2.functional.crop exactly when using scalar top/left. Reference: torchvision/transforms/v2/functional/_geometry.py line 1787</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>top</code> <code>int | Tensor</code> <p>Top pixel coordinate for cropping (int or int32 tensor of shape (N,) for per-image)</p> required <code>left</code> <code>int | Tensor</code> <p>Left pixel coordinate for cropping (int or int32 tensor of shape (N,) for per-image)</p> required <code>height</code> <code>int</code> <p>Height of the cropped image</p> required <code>width</code> <code>int</code> <p>Width of the cropped image</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Cropped tensor of shape (N, C, height, width)</p> Example <pre><code>img = torch.rand(2, 3, 224, 224, device='cuda')\n# Crop all images at same position\ncropped = crop(img, top=56, left=56, height=112, width=112)\n# Crop each image at different position\ntops = torch.tensor([56, 100], device='cuda', dtype=torch.int32)\nlefts = torch.tensor([56, 80], device='cuda', dtype=torch.int32)\ncropped = crop(img, top=tops, left=lefts, height=112, width=112)\n</code></pre> Note <p>For MVP, this requires valid crop coordinates (no padding). Future versions will support padding for out-of-bounds crops.</p>"},{"location":"api-reference/#triton_augment.functional.center_crop","title":"triton_augment.functional.center_crop","text":"<pre><code>center_crop(image: Tensor, output_size: tuple[int, int] | int) -&gt; torch.Tensor\n</code></pre> <p>Crop the center of the image to the given size.</p> <p>Matches torchvision.transforms.v2.functional.center_crop exactly. Reference: torchvision/transforms/v2/functional/_geometry.py line 2545</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>output_size</code> <code>tuple[int, int] | int</code> <p>Desired output size (height, width) or int for square crop</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Center-cropped tensor of shape (N, C, output_size[0], output_size[1])</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If output_size is larger than image size</p> Example <pre><code>img = torch.rand(2, 3, 224, 224, device='cuda')\n# Center crop to 112x112\ncropped = center_crop(img, (112, 112))\n# or for square crop\ncropped = center_crop(img, 112)\n</code></pre>"},{"location":"api-reference/#triton_augment.functional.horizontal_flip","title":"triton_augment.functional.horizontal_flip","text":"<pre><code>horizontal_flip(image: Tensor, flip_mask: Tensor | None = None) -&gt; torch.Tensor\n</code></pre> <p>Flip the image horizontally (left to right), with optional per-image control.</p> <p>Matches torchvision.transforms.v2.functional.horizontal_flip exactly when flip_mask=None. Reference: torchvision/transforms/v2/functional/_geometry.py line 56</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image tensor of shape (N, C, H, W) on CUDA</p> required <code>flip_mask</code> <code>Tensor | None</code> <p>Optional uint8 tensor of shape (N,) indicating which images to flip (0=no flip, 1=flip).       If None, flips all images (default behavior).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Horizontally flipped tensor of the same shape</p> Example <pre><code>img = torch.rand(2, 3, 224, 224, device='cuda')\n# Flip all images\nflipped = horizontal_flip(img)\n# Flip only first image\nflip_mask = torch.tensor([1, 0], device='cuda', dtype=torch.bool)\nflipped = horizontal_flip(img, flip_mask)\n</code></pre> Note <p>This uses a custom Triton kernel. For standalone flip operations, PyTorch's tensor.flip(-1) is highly optimized and may be comparable. The main benefit is when fusing with crop (see fused_crop_flip).</p>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/#triton_augment.enable_autotune","title":"triton_augment.enable_autotune","text":"<pre><code>enable_autotune()\n</code></pre> <p>Enable kernel auto-tuning for optimal performance.</p> <p>When enabled, Triton will test multiple kernel configurations and cache the best one for your GPU and image sizes.</p> Example <pre><code>import triton_augment as ta\nta.enable_autotune()\n# Now kernels will auto-tune on first use\n</code></pre>"},{"location":"api-reference/#triton_augment.disable_autotune","title":"triton_augment.disable_autotune","text":"<pre><code>disable_autotune()\n</code></pre> <p>Disable kernel auto-tuning and use fixed defaults.</p> <p>When disabled, kernels use fixed configurations that work well across most GPUs and image sizes without tuning overhead.</p> Example <pre><code>import triton_augment as ta\nta.disable_autotune()\n# Now kernels will use fixed defaults (faster, good performance)\n</code></pre>"},{"location":"api-reference/#triton_augment.is_autotune_enabled","title":"triton_augment.is_autotune_enabled","text":"<pre><code>is_autotune_enabled() -&gt; bool\n</code></pre> <p>Check if auto-tuning is currently enabled.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if auto-tuning is enabled, False otherwise</p>"},{"location":"api-reference/#triton_augment.warmup_cache","title":"triton_augment.warmup_cache","text":"<pre><code>warmup_cache(batch_sizes: Tuple[int, ...] = (32, 64), image_sizes: Tuple[int, ...] = (224, 256, 512), verbose: bool = True)\n</code></pre> <p>Pre-populate the auto-tuning cache for common image sizes.</p> <p>This function runs the fused kernel with various common configurations to trigger auto-tuning and cache the optimal settings. This eliminates the 5-10 second delay on first use.</p> <p>Parameters:</p> Name Type Description Default <code>batch_sizes</code> <code>Tuple[int, ...]</code> <p>Tuple of batch sizes to warm up (default: (32, 64))</p> <code>(32, 64)</code> <code>image_sizes</code> <code>Tuple[int, ...]</code> <p>Tuple of square image sizes to warm up (default: (224, 256, 512))</p> <code>(224, 256, 512)</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress messages (default: True)</p> <code>True</code> Example <pre><code>import triton_augment as ta\n# Warm up cache for common training scenarios\nta.warmup_cache()\n# Custom sizes for your specific use case\nta.warmup_cache(batch_sizes=(16, 128), image_sizes=(128, 384))\n</code></pre>"},{"location":"api-reference/#input-requirements","title":"Input Requirements","text":""},{"location":"api-reference/#transform-classes_1","title":"Transform Classes","text":"<p>Transform classes (e.g., <code>TritonFusedAugment</code>, <code>TritonColorJitter</code>, etc.) accept:</p> <ul> <li>Device: CUDA (GPU) or CPU - CPU tensors are automatically moved to GPU</li> <li>Shape: <code>(C, H, W)</code>, <code>(N, C, H, W)</code>, or <code>(N, T, C, H, W)</code> - 3D, 4D, or 5D (video)</li> <li>Dtype: float32 or float16</li> <li>Range: [0, 1] for color operations (required)</li> </ul> <p>Notes:</p> <ul> <li>3D tensors <code>(C, H, W)</code> are automatically converted to <code>(1, C, H, W)</code> internally for processing</li> <li> <p>5D tensors <code>(N, T, C, H, W)</code> are supported for video augmentation (batch, frames, channels, height, width)</p> </li> <li> <p>For 5D inputs, use <code>same_on_frame=True</code> (default) for consistent augmentation across frames, or <code>same_on_frame=False</code> for independent per-frame augmentation</p> </li> <li> <p>After normalization, values can be outside [0, 1] range</p> </li> <li> <p>CPU tensors are automatically transferred to CUDA for GPU processing</p> </li> </ul>"},{"location":"api-reference/#functional-api_1","title":"Functional API","text":"<p>Functional functions (e.g., <code>fused_augment()</code>, <code>crop()</code>, <code>normalize()</code>, etc.) expect:</p> <ul> <li>Device: CUDA (GPU) - must be on CUDA device</li> <li>Shape: <code>(N, C, H, W)</code> - 4D tensors only</li> <li>Dtype: float32 or float16</li> <li>Range: [0, 1] for color operations (required)</li> </ul> <p>Note: Transform classes handle 3D/5D normalization internally. If using the functional API directly, ensure inputs are already in 4D format <code>(N, C, H, W)</code>.</p>"},{"location":"api-reference/#performance-tips","title":"Performance Tips","text":""},{"location":"api-reference/#1-use-fused-kernel-even-for-partial-operations","title":"1. Use Fused Kernel Even for Partial Operations","text":"<p>Key insight: Even if you only need a subset of operations, use <code>TritonFusedAugment</code> or <code>F.fused_augment</code> for best performance! Simply set unused operations to no-op values:</p> <pre><code># Example: Only need crop + normalize (no flip, no color jitter)\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    horizontal_flip_p=0.0,      # No flip\n    brightness=0.0,             # No brightness\n    contrast=0.0,               # No contrast\n    saturation=0.0,             # No saturation\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n# Still faster than calling crop() + normalize() separately!\n</code></pre> <p>The fused kernel is optimized to skip operations set to no-op values at compile time.</p>"},{"location":"api-reference/#2-auto-tuning","title":"2. Auto-Tuning","text":"<p>Enable auto-tuning for optimal performance on your specific GPU and data sizes:</p> <pre><code>import triton_augment as ta\n\nta.enable_autotune()  # Enable once at start of training\n\n# Optional: Pre-compile kernels for your data sizes\nta.warmup_cache(batch_sizes=(32, 64), image_sizes=(224, 512))\n</code></pre> <p>See Auto-Tuning Guide for detailed configuration.</p>"},{"location":"api-reference/#additional-resources","title":"Additional Resources","text":"<ul> <li>Quick Start Guide: Training integration examples</li> <li>Float16 Support: Half-precision performance and memory savings</li> <li>Contrast Notes: Differences between fast and torchvision-exact contrast</li> <li>Batch Behavior: Understanding <code>same_on_batch</code> parameter</li> <li>Benchmark Results: Detailed performance comparisons</li> </ul>"},{"location":"auto-tuning/","title":"Auto-Tuning Guide","text":"<p>Default Behavior</p> <p>Auto-tuning is DISABLED by default for faster startup and good-enough performance. Enable it only if you need the extra performance boost.</p>"},{"location":"auto-tuning/#what-is-auto-tuning","title":"What is Auto-Tuning?","text":"<p>Auto-tuning automatically finds the optimal kernel configuration (block size, warp count, pipeline depth) for your specific GPU and image sizes.</p> <p>When enabled, Triton-Augment auto-tunes the fused kernel (<code>fused_augment</code> / <code>TritonFusedAugment</code>) while simple operations (brightness, saturation, normalize) always use fixed defaults.</p>"},{"location":"auto-tuning/#enabling-auto-tuning","title":"Enabling Auto-Tuning","text":""},{"location":"auto-tuning/#option-1-python-api","title":"Option 1: Python API","text":"<pre><code>import triton_augment as ta\n\n# Enable auto-tuning for optimal performance\nta.enable_autotune()\n\n# Check status\nprint(ta.is_autotune_enabled())  # True\n\n# Disable if needed\nta.disable_autotune()\n</code></pre>"},{"location":"auto-tuning/#option-2-environment-variable","title":"Option 2: Environment Variable","text":"<pre><code>export TRITON_AUGMENT_ENABLE_AUTOTUNE=1\npython train.py\n</code></pre>"},{"location":"auto-tuning/#auto-tuning-details","title":"Auto-Tuning Details","text":""},{"location":"auto-tuning/#auto-tuned-kernel","title":"Auto-Tuned Kernel","text":"<p>The fused kernel (<code>fused_augment</code>) is auto-tuned across these parameters:</p> <ul> <li>BLOCK_SIZE: Number of elements processed per thread block (256, 512, 1024, 2048)</li> <li>num_warps: Thread group size for parallelism (2, 4, 8)</li> <li>num_stages: Memory pipeline depth for memory-compute overlap (2, 3, 4)</li> </ul> <p>Triton tests 12 configurations and caches the fastest one for your specific workload.</p>"},{"location":"auto-tuning/#fixed-kernels","title":"Fixed Kernels","text":"<p>Simple operations use <code>BLOCK_SIZE=1024</code> for optimal performance on most GPUs: - <code>adjust_brightness</code> - <code>adjust_saturation</code> - <code>normalize</code></p> <p>These don't need auto-tuning as they're simple memory-bound operations.</p>"},{"location":"auto-tuning/#how-it-works","title":"How It Works","text":"<ol> <li>First run: Auto-tuning tests 12 configurations and caches the best one (5-10 seconds)</li> <li>Subsequent runs: Uses cached optimal configuration (zero overhead)</li> <li>Per GPU + size: Cache is specific to your GPU model and total elements (N\u00d7C\u00d7H\u00d7W)</li> </ol>"},{"location":"auto-tuning/#cache-warm-up-when-auto-tuning-is-enabled","title":"Cache Warm-Up (When Auto-Tuning is Enabled)","text":"<p>To avoid auto-tuning delays during training, warm up the cache once:</p>"},{"location":"auto-tuning/#cli","title":"CLI","text":"<pre><code># Enable auto-tuning and warm up\nTRITON_AUGMENT_ENABLE_AUTOTUNE=1 python -m triton_augment.warmup \\\n    --batch-sizes 64,128 \\\n    --image-sizes 320,384,640\n</code></pre>"},{"location":"auto-tuning/#python-api","title":"Python API","text":"<pre><code>import triton_augment as ta\n\n# Enable and warm up\nta.enable_autotune()\nta.warmup_cache(\n    batch_sizes=(64, 128),\n    image_sizes=(320, 384, 640)\n)\n</code></pre> <p>Important Notes</p> <ul> <li>Auto-tuning must be ENABLED for warmup to test multiple configs</li> <li>Without auto-tuning, warmup just compiles the default config</li> <li>Auto-tuning is size-specific: A cache for 224\u00d7224 won't help with 320\u00d7320</li> <li>Always use your actual training dimensions</li> </ul>"},{"location":"auto-tuning/#when-auto-tuning-is-disabled-default","title":"When Auto-Tuning is Disabled (Default)","text":"<ul> <li>Uses a single well-tuned default configuration</li> <li>Zero auto-tuning overhead</li> <li>No need to warm up cache</li> <li>Good performance on most GPUs</li> <li>First use still takes ~1-2 seconds to compile (but no testing/selection)</li> </ul>"},{"location":"auto-tuning/#performance-impact","title":"Performance Impact","text":""},{"location":"auto-tuning/#expected-improvements-when-enabled","title":"Expected Improvements (When Enabled)","text":"<p>Auto-tuning typically provides: - 5-15% speedup on most operations - Best gains on:   - Larger images (512\u00d7512+)   - Larger batch sizes (64+)   - Data center GPUs (A100, H100) - Minimal gains on:   - Small images (&lt; 128\u00d7128)   - Small batches (&lt; 16)   - Consumer GPUs (RTX 30xx series)</p>"},{"location":"auto-tuning/#trade-offs","title":"Trade-offs","text":"Aspect Disabled (Default) Enabled First-run speed Fast (~1-2 sec) Slow (~5-10 sec) Steady-state performance Good (95-98%) Optimal (100%) Cache warmup needed No Yes (recommended) Best for Most users, rapid iteration Production, max performance"},{"location":"auto-tuning/#benchmarking-withwithout-auto-tuning","title":"Benchmarking With/Without Auto-Tuning","text":"<p>The recommended workflow is simple: benchmark the default config first, then enable auto-tuning and benchmark again.</p>"},{"location":"auto-tuning/#using-benchmark-scripts","title":"Using Benchmark Scripts","text":"<p>Run Default Config First!</p> <p>Always benchmark without <code>--autotune</code> first, then with <code>--autotune</code>. Once auto-tuning runs, it caches the config and you can't go back without clearing cache.</p> <pre><code># Step 1: Benchmark default config first\npython examples/benchmark.py\n\n# Step 2: Then benchmark with auto-tuning\npython examples/benchmark.py --autotune\n</code></pre> <p>Or the comprehensive benchmark: <pre><code>python examples/benchmark_triton.py --autotune\n</code></pre></p>"},{"location":"auto-tuning/#standard-benchmarking-workflow-custom-code","title":"Standard Benchmarking Workflow (Custom Code)","text":"<pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Step 1: Benchmark default config (auto-tuning is disabled by default), run it first!\nprint(\"Benchmarking default config...\")\ntime_default = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Default config: {time_default:.3f} ms\")\n\n# Step 2: Enable auto-tuning and benchmark\nprint(\"\\nEnabling auto-tuning...\")\nta.enable_autotune()\n\n# First call triggers auto-tuning (takes 5-10 seconds, only once)\nprint(\"Running auto-tuning (this will take ~5-10 seconds)...\")\n_ = transform(img)\n\n# Now benchmark with optimal config\nprint(\"Benchmarking auto-tuned config...\")\ntime_tuned = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Auto-tuned config: {time_tuned:.3f} ms\")\n\n# Compare\nspeedup = time_default / time_tuned\nprint(f\"\\nSpeedup: {speedup:.2f}x\")\n</code></pre>"},{"location":"auto-tuning/#for-reproducible-comparisons-google-colab","title":"For Reproducible Comparisons (Google Colab)","text":"<p>If you need truly isolated benchmarks on Google Colab (where cache may persist across runtime restarts), use two separate colab notebooks:</p> <p>Notebook 1 - Default Config: <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\n# Auto-tuning is disabled by default\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Default config: {time_ms:.3f} ms\")\n</code></pre></p> <p>Notebook 2 - Auto-Tuned Config: <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\n# Enable auto-tuning in fresh notebook\nta.enable_autotune()\n\nimg = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = ta.TritonFusedAugment(\n    crop_size=224,\n    brightness=(0.8, 1.2),\n    saturation=(0.5, 1.5),\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Trigger auto-tuning\n_ = transform(img)\n\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\nprint(f\"Auto-tuned config: {time_ms:.3f} ms\")\n</code></pre></p> <p>Colab Cache Behavior</p> <p>Google Colab's cache directory (<code>~/.triton/cache</code>) may persist across runtime restarts within the same session. Using separate notebooks ensures completely independent benchmarks.</p>"},{"location":"auto-tuning/#benchmarking-on-sharedcloud-services","title":"Benchmarking on Shared/Cloud Services","text":"<p>Instability on Colab, Kaggle, and Cloud GPUs</p> <p>If you're benchmarking on Google Colab, Kaggle Notebooks, or other shared cloud services, you may see unstable or inconsistent results.</p>"},{"location":"auto-tuning/#why-benchmarks-can-be-unstable","title":"Why Benchmarks Can Be Unstable","text":"<p>Shared GPU services can cause significant performance variability:</p> <ol> <li>Shared Physical GPU - Multiple users on the same GPU compete for resources</li> <li>Variable GPU Allocation - You might get different GPU models between sessions</li> <li>Thermal Throttling - GPU performance degrades when hot from other users' workloads</li> <li>Background Processes - Cloud platform monitoring and management overhead</li> <li>Network I/O - Data transfers can interfere with kernel execution timing</li> </ol>"},{"location":"auto-tuning/#symptoms-of-instability","title":"Symptoms of Instability","text":"<p>You might see: - Wildly varying benchmark times (e.g., 0.5ms one run, 200ms the next) - Incorrect speedups (e.g., 0.00x or negative speedups) - Different results between runs with identical code - Auto-tuning picking suboptimal configs due to noisy measurements</p>"},{"location":"auto-tuning/#best-practices-for-stable-benchmarks","title":"Best Practices for Stable Benchmarks","text":"<p>If you must benchmark on shared services:</p> <ol> <li> <p>Run multiple iterations and take the median:    <pre><code>from triton.testing import do_bench\n\n# do_bench already uses median of multiple runs\ntime_ms = do_bench(lambda: transform(img), warmup=25, rep=100)\n</code></pre></p> </li> <li> <p>Warm up thoroughly before benchmarking:    <pre><code># Warm up: compile kernels and stabilize GPU state\nfor _ in range(10):\n    _ = transform(img)\ntorch.cuda.synchronize()\n\n# Now benchmark\ntime_ms = do_bench(lambda: transform(img))\n</code></pre></p> </li> <li> <p>Use a dedicated session - Close other notebooks/tabs using the GPU</p> </li> <li> <p>Restart runtime if results seem anomalous</p> </li> <li> <p>Run at off-peak times - Early morning or late night (timezone-dependent)</p> </li> <li> <p>Compare trends, not absolute numbers - Look for consistent relative speedups</p> </li> </ol>"},{"location":"auto-tuning/#for-production-benchmarks","title":"For Production Benchmarks","text":"<p>For reliable, production-grade benchmarks:</p> <ul> <li>Use dedicated GPU instances (AWS P3/P4, GCP A2, Azure NC-series)</li> <li>Lock GPU clocks to prevent throttling (requires root):   <pre><code>sudo nvidia-smi -lgc 1410,1410  # Lock to max clock\n</code></pre></li> <li>Isolate the GPU - No other processes using it</li> <li>Multiple runs - Run benchmarks 5-10 times and report mean \u00b1 std dev</li> </ul>"},{"location":"auto-tuning/#recommendation","title":"Recommendation","text":"<ul> <li>For most users: Keep auto-tuning disabled (default)</li> <li>Faster startup</li> <li>Good performance out-of-the-box</li> <li> <p>No cache management needed</p> </li> <li> <p>For production/max performance: Enable auto-tuning</p> </li> <li>Warm up cache during deployment</li> <li>Squeeze out last 5-15% performance</li> <li>Worth it for long training runs</li> </ul>"},{"location":"batch-behavior/","title":"Batch Behavior &amp; Different Parameters Per Sample","text":"<p>Different Parameters Per Sample by Default</p> <p>Triton-Augment applies different random parameters to each image in a batch by default!</p>"},{"location":"batch-behavior/#default-behavior-different-parameters-per-sample","title":"Default Behavior: Different Parameters Per Sample","text":"<pre><code>import torch\nimport triton_augment as ta\n\n# Each image gets DIFFERENT random augmentation\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2\n)\n\nresult = transform(batch)  # 32 different random augmentations! \u2705\n</code></pre> <p>How it works: - Random parameters are sampled per-image (32 different crop positions, flip decisions, color factors) - All processed in ONE kernel launch on GPU - Fast batch processing + individual randomness = best of both worlds! \ud83d\ude80</p>"},{"location":"batch-behavior/#controlling-randomness-same_on_batch-flag","title":"Controlling Randomness: <code>same_on_batch</code> Flag","text":"<p>All transform classes support the <code>same_on_batch</code> parameter:</p>"},{"location":"batch-behavior/#different-parameters-per-sample-default","title":"Different Parameters Per Sample (Default)","text":"<pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=False  # Default\n)\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\nresult = transform(batch)  # Each image: different crop, flip, brightness\n</code></pre>"},{"location":"batch-behavior/#batch-wide-parameters-same-for-all","title":"Batch-Wide Parameters (Same for All)","text":"<pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=True  # Same params for all images\n)\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\nresult = transform(batch)  # All images: same crop position, flip, brightness\n</code></pre>"},{"location":"batch-behavior/#video-5d-tensor-support-same_on_frame-flag","title":"Video (5D Tensor) Support: <code>same_on_frame</code> Flag","text":"<p>For video tensors with shape <code>[N, T, C, H, W]</code> (batch, frames, channels, height, width), Triton-Augment supports the <code>same_on_frame</code> parameter to control whether augmentation parameters are shared across frames:</p>"},{"location":"batch-behavior/#consistent-augmentation-across-frames-default","title":"Consistent Augmentation Across Frames (Default)","text":"<pre><code># Video batch: 8 videos \u00d7 16 frames \u00d7 3 channels \u00d7 224\u00d7224\nvideos = torch.rand(8, 16, 3, 224, 224, device='cuda')\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_frame=True  # Default: same augmentation for all frames\n)\n\nresult = transform(videos)  # All 16 frames in each video get same crop/flip/color\n</code></pre> <p>Use when: - \u2705 Video training (consistent augmentation preserves temporal coherence) - \u2705 You want frames in a video to look consistent - \u2705 Similar to Kornia's <code>VideoSequential</code> behavior</p>"},{"location":"batch-behavior/#independent-augmentation-per-frame","title":"Independent Augmentation Per Frame","text":"<pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_frame=False  # Each frame gets different augmentation\n)\n\nresult = transform(videos)  # Each of 16 frames gets different crop/flip/color\n</code></pre> <p>Use when: - \u2705 You want maximum frame diversity - \u2705 Each frame should be augmented independently - \u2705 Similar to processing frames individually</p>"},{"location":"batch-behavior/#combining-same_on_batch-and-same_on_frame","title":"Combining <code>same_on_batch</code> and <code>same_on_frame</code>","text":"<p>For video tensors <code>[N, T, C, H, W]</code>, you can control both batch and frame dimensions:</p> <pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=False,   # Different params per video\n    same_on_frame=True     # Same params for all frames in each video\n)\n\n# Result:\n# - Video 0: frames 0-15 share same augmentation\n# - Video 1: frames 0-15 share same augmentation (different from Video 0)\n# - Video 2: frames 0-15 share same augmentation (different from Videos 0,1)\n# ... and so on\n</code></pre> <p>Parameter combinations for <code>[N, T, C, H, W]</code>: - <code>same_on_batch=False, same_on_frame=False</code>: N\u00d7T different parameters (all independent) - <code>same_on_batch=False, same_on_frame=True</code>: N different parameters (one per video, shared across frames) - <code>same_on_batch=True, same_on_frame=False</code>: T different parameters (one per frame position, shared across videos) - <code>same_on_batch=True, same_on_frame=True</code>: 1 parameter (shared across all videos and frames)</p>"},{"location":"batch-behavior/#when-to-use-each-mode","title":"When to Use Each Mode","text":""},{"location":"batch-behavior/#different-parameters-per-sample-recommended-for-training","title":"Different Parameters Per Sample (Recommended for Training)","text":"<p>\u2705 Use when: - Training neural networks (standard augmentation) - You want maximum data diversity - Each image should be augmented independently</p> <pre><code># Standard training setup\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n    same_on_batch=False  # \u2705 Default, each image different\n)\n\nfor images, labels in train_loader:\n    images = images.cuda()\n    images = transform(images)  # Unique augmentation per image\n    # ... training ...\n</code></pre> <p>Performance: Still fast! One kernel launch processes entire batch with per-image params.</p>"},{"location":"batch-behavior/#batch-wide-parameters-specialized-use-cases","title":"Batch-Wide Parameters (Specialized Use Cases)","text":"<p>\u2705 Use when: - Debugging (easier to see effect of specific parameters) - Specific research requirements - All images should share exact same augmentation</p> <pre><code>transform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    same_on_batch=True  # Same for all images\n)\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\nresult = transform(batch)  # All images: same augmentation \u2705\n</code></pre> <p>Note: For video tensors <code>[N, T, C, H, W]</code>, use <code>same_on_frame=True</code> instead (or in addition) to control frame-level consistency.</p>"},{"location":"batch-behavior/#all-transforms-support-different-parameters-per-sample","title":"All Transforms Support Different Parameters Per Sample","text":"<p>The following transforms all support <code>same_on_batch</code> (and <code>same_on_frame</code> for video tensors):</p> <ul> <li><code>TritonFusedAugment</code> - Complete pipeline (crop, flip, color, normalize)</li> <li><code>TritonRandomCropFlip</code> - Geometric operations only</li> <li><code>TritonColorJitterNormalize</code> - ColorJitter + Normalize</li> <li><code>TritonColorJitter</code> - ColorJitter only</li> <li><code>TritonRandomCrop</code> - Random cropping</li> <li><code>TritonRandomHorizontalFlip</code> - Random flipping</li> <li><code>TritonRandomGrayscale</code> - Random grayscale conversion</li> </ul> <p>Example: <pre><code># Individual transforms also support same_on_batch\ncrop = ta.TritonRandomCrop(112, same_on_batch=False)\nflip = ta.TritonRandomHorizontalFlip(p=0.5, same_on_batch=False)\njitter = ta.TritonColorJitter(brightness=0.2, same_on_batch=False)\n\n# Video transforms support same_on_frame\nvideo_crop = ta.TritonRandomCrop(112, same_on_batch=False, same_on_frame=True)\nvideo_flip = ta.TritonRandomHorizontalFlip(p=0.5, same_on_batch=False, same_on_frame=True)\n</code></pre></p>"},{"location":"batch-behavior/#functional-api-fixed-parameters","title":"Functional API: Fixed Parameters","text":"<p>The functional API (<code>triton_augment.functional</code>) is for deterministic augmentations:</p> <pre><code>import triton_augment.functional as F\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Fixed parameters - same for all images\nresult = F.fused_augment(\n    batch,\n    top=20, left=30,          # Fixed crop position\n    height=112, width=112,\n    flip_horizontal=True,      # Fixed flip decision\n    brightness_factor=1.2,     # Fixed brightness\n    saturation_factor=0.9,     # Fixed saturation\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>If you need per-image fixed parameters, pass tensors:</p> <pre><code># Per-image fixed parameters (different but deterministic)\ntop_offsets = torch.tensor([10, 20, 30, ...], device='cuda')  # [32]\nbrightness = torch.tensor([1.1, 1.2, 1.3, ...], device='cuda')  # [32]\n\nresult = F.fused_augment(\n    batch,\n    top=top_offsets,           # Tensor: per-image positions\n    left=30,                   # Scalar: same for all\n    height=112, width=112,\n    flip_horizontal=True,\n    brightness_factor=brightness,  # Tensor: per-image factors\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre>"},{"location":"batch-behavior/#comparison-with-torchvision","title":"Comparison with torchvision","text":"<p>torchvision doesn't support different parameters per sample on batched tensors:</p> <pre><code>import torchvision.transforms.v2 as tv_transforms\n\nbatch = torch.rand(32, 3, 224, 224, device='cuda')\ntransform = tv_transforms.ColorJitter(brightness=0.2)\n\n# All 32 images get the SAME brightness factor\nresult = transform(batch)\n</code></pre> <p>To get different parameters per sample in torchvision, you must apply transforms before batching (in DataLoader), which processes images sequentially.</p> <p>Triton-Augment advantage: Different parameters per sample with GPU batch processing - best of both worlds! \ud83d\ude80</p>"},{"location":"batch-behavior/#performance-impact","title":"Performance Impact","text":"<p>Different Parameters Per Sample: - \u2705 Same kernel launch time as batch-wide - \u2705 One kernel launch for entire batch - \u2705 Minimal overhead (kernel uses <code>tl.load</code> to fetch per-image params)</p> <p>Batch-Wide Parameters: - \u2705 Slightly faster (no per-image parameter indexing) - \u26a0\ufe0f Less data diversity for training</p> <p>Verdict: Use different parameters per sample (default) for training. The performance difference is negligible (~1-2%), but data diversity is crucial!</p>"},{"location":"batch-behavior/#example-real-training-pipeline","title":"Example: Real Training Pipeline","text":"<pre><code>import torch\nimport triton_augment as ta\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Step 1: Load data on CPU with workers (fast async I/O)\ntrain_dataset = datasets.CIFAR10(\n    './data', train=True,\n    transform=transforms.ToTensor()  # Only ToTensor on CPU\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    num_workers=4,      # \u2705 Async data loading\n    pin_memory=True\n)\n\n# Step 2: GPU augmentation with different parameters per sample\naugment = ta.TritonFusedAugment(\n    crop_size=28,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    mean=(0.4914, 0.4822, 0.4465),\n    std=(0.2470, 0.2435, 0.2616),\n    same_on_batch=False  # \u2705 Each image gets unique augmentation\n)\n\n# Step 3: Training loop\nfor images, labels in train_loader:\n    images, labels = images.cuda(), labels.cuda()\n    images = augment(images)  # \ud83d\ude80 One kernel, all augmentations, per-image random!\n\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    # ... backprop ...\n</code></pre> <p>Result: - \u2705 Fast async CPU data loading (<code>num_workers &gt; 0</code>) - \u2705 Fast GPU batch processing (one kernel) - \u2705 Different parameters per sample (maximum diversity) - \u2705 Best of all worlds! \ud83d\ude80</p>"},{"location":"comparison-other-libs/","title":"Comparison with other GPU-based data transforms library","text":""},{"location":"comparison-other-libs/#triton-augment-vs-dali-vs-kornia","title":"\ud83d\udcca Triton-Augment vs DALI vs Kornia","text":"Feature Triton-Augment (yours) NVIDIA DALI Kornia Primary goal Fast, fused GPU augmentations for training End-to-end input pipeline (decode \u2192 resize \u2192 augment) Differentiable augmentations in pure PyTorch Fused ops \u2714\ufe0f Crop + flip + brightness + contrast + saturation + grayscale + normalize (single kernel) \u26a0\ufe0f Only some fusions (e.g., <code>crop_mirror_normalize</code>); color ops are separate kernels \u274c No fusion \u2014 each op is a separate CUDA/PyTorch op Per-sample random params \u2714\ufe0f Built-in, torchvision-style API \u2714\ufe0f Supported (via feeding random tensors), but more manual \u2714\ufe0f Built-in Ease of use \u2714\ufe0f Simple, torchvision-like \u26a0\ufe0f Steeper learning curve (pipeline graph) \u2714\ufe0f Very easy (just PyTorch ops) Supported ops \u26a0\ufe0f Limited for now (crop, flip, color jitter, normalize, grayscale) \u2714\ufe0f Huge library (decode, resize, warp, color, video, audio) \u2714\ufe0f Wide set (geometry, color, filtering, keypoints) Performance \ud83d\ude80 Very fast for augmentation (1 fused kernel for all ops) \ud83d\ude80 Fast for full pipelines (GPU decode/resize), but augmentation uses multiple kernels (less fusion) \u26a0\ufe0f Moderate (PyTorch kernels, multiple launches) Integration PyTorch training pipelines PyTorch, TensorFlow, JAX PyTorch only CPU preprocessing \u274c None (expects tensors already on GPU) \u2714\ufe0f Hardware-accelerated decode/resize possible \u2714\ufe0f Built on top of PyTorch Autograd support \u274c Not needed (augmentations only) \u274c Most ops are not differentiable \u2714\ufe0f Yes (Kornia is differentiable by design) Production readiness \u26a0\ufe0f Early-stage (fast but limited scope) \u2714\ufe0f Mature, used in industry \u2714\ufe0f Mature"},{"location":"comparison-other-libs/#notes","title":"\ud83d\udcdd Notes","text":"<ul> <li> <p>Triton-Augment is not a replacement for DALI or Kornia.   It\u2019s a small, focused library aimed at speeding up a few high-impact augmentations via kernel fusion.</p> </li> <li> <p>DALI is still the best choice if the bottleneck is decode/resize or you need full data pipeline acceleration. However, for augmentation-only workloads (data already on GPU as tensors), Triton-Augment is faster due to higher kernel fusion.</p> </li> <li> <p>Kornia is best if you need differentiable augmentation or a wide variety of transforms.</p> </li> <li> <p>Our advantage:   For the operations that are supported, our one-kernel design beats both in raw speed and simplicity.</p> </li> <li> <p>Our limitation:   Fewer ops, no CPU pipeline, not designed for everything \u2014 just the common path.</p> </li> </ul>"},{"location":"contrast/","title":"Contrast Implementation","text":"<p>Important</p> <p>This library implements a different contrast algorithm than torchvision for speed and fusion benefits.</p>"},{"location":"contrast/#tldr","title":"TL;DR","text":"<ul> <li><code>F.fused_augment()</code> and transforms class like <code>TritonColorJitter</code>, <code>TritonFusedAugment</code> uses fast contrast (not torchvision-exact)</li> <li>For exact torchvision results, use individual functional <code>F.adjust_contrast()</code></li> <li>Fast contrast is production-proven (same as NVIDIA DALI)</li> </ul>"},{"location":"contrast/#technical-details","title":"Technical Details","text":""},{"location":"contrast/#torchvision-contrast","title":"Torchvision Contrast","text":"<pre><code>grayscale_mean = mean(rgb_to_grayscale(input))\noutput = input * contrast_factor + grayscale_mean * (1 - contrast_factor)\n</code></pre> <p>Problem for fusion: Requires computing the mean of the entire image, which:</p> <ul> <li> <p>Needs an extra kernel launch</p> </li> <li> <p>Breaks the fusion (can't fuse before knowing the mean)</p> </li> <li> <p>Creates a data dependency</p> </li> </ul>"},{"location":"contrast/#fast-contrast","title":"Fast Contrast","text":"<pre><code>output = (input - 0.5) * contrast_factor + 0.5\n</code></pre> <p>Benefits:</p> <ul> <li> <p>No mean computation needed</p> </li> <li> <p>Fully fusible with other operations</p> </li> <li> <p>Single kernel launch for entire pipeline</p> </li> <li> <p>0.5 is a reasonable default anchor point (middle of [0, 1] range)</p> </li> </ul>"},{"location":"contrast/#impact-on-training","title":"Impact on Training","text":"<p>In practice, the difference is minimal:</p> <ul> <li> <p>Models learn to be robust to data augmentation variations</p> </li> <li> <p>The specific contrast formula matters less than having contrast augmentation at all</p> </li> <li> <p>NVIDIA DALI uses this approach in production ML systems worldwide</p> </li> </ul>"},{"location":"contrast/#visual-comparison","title":"Visual Comparison","text":"<p>Visual comparison between torchvision (top), triton-exact (middle) and triton-fast (bottom) at various contrast factors. This figure is generated by <code>python examples/visualize_augmentations.py</code>.</p> <p>Minimal Visual Differences</p> <p>The differences between fast and exact contrast in this sample image are very small and barely perceptible to the human eye. For brighter or darker images, the differences might be more noticeable, but the impact on model training is minimal.</p>"},{"location":"contrast/#contrast-formula-comparison-to-other-libraries","title":"Contrast Formula: Comparison to Other Libraries","text":"Library Formula Type Speed NVIDIA DALI <code>(x - 0.5) * f + 0.5</code> Linear (centered) Fastest \u2705 Triton-Augment (fast) <code>(x - 0.5) * f + 0.5</code> Linear (centered) Fastest \u2705 OpenCV <code>alpha * x + beta</code> Linear Fast Torchvision <code>x * f + mean * (1-f)</code> Linear (mean) Slower Scikit-image <code>1/(1+exp(-gain*(x-cut)))</code> Sigmoid (S-curve) Slowest"},{"location":"contrast/#why-this-formula","title":"Why This Formula?","text":"<p>\u2705 Production-proven: Same as NVIDIA DALI \u2705 Fast &amp; fusible: No mean computation required  </p> <p>If you need exact torchvision reproduction, use <code>F.adjust_contrast()</code> instead of fast mode. See the following ways to produce exact torchvision results.</p>"},{"location":"contrast/#three-equivalent-ways-torchvision-exact","title":"Three Equivalent Ways (Torchvision-Exact)","text":"<p>All three approaches below produce pixel-perfect identical results:</p>"},{"location":"contrast/#1-torchvision","title":"1. Torchvision","text":"<pre><code>import torchvision.transforms.v2.functional as tvF\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nresult = tvF.adjust_brightness(img, 1.2)\nresult = tvF.adjust_contrast(result, 1.1)\nresult = tvF.adjust_saturation(result, 0.9)\nresult = tvF.normalize(result, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n</code></pre> <p>\u23f1\ufe0f Speed: Baseline</p>"},{"location":"contrast/#2-triton-individual-functions-exact","title":"2. Triton Individual Functions (Exact)","text":"<pre><code>import triton_augment.functional as F\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\nresult = F.adjust_brightness(img, 1.2)\nresult = F.adjust_contrast(result, 1.1)        # Torchvision-exact\nresult = F.adjust_saturation(result, 0.9)\nresult = F.normalize(result, \n                     mean=(0.485, 0.456, 0.406),\n                     std=(0.229, 0.224, 0.225))\n</code></pre> <p>\u23f1\ufe0f Speed: Faster (optimized Triton kernels) \u26a1</p>"},{"location":"contrast/#3-triton-contrast-fused-exact-fast","title":"3. Triton Contrast + Fused (Exact + Fast)","text":"<pre><code>import triton_augment.functional as F\n\nimg = torch.rand(1, 3, 224, 224, device='cuda')\n# Apply exact contrast first\nresult = F.adjust_brightness(img, 1.2)\nresult = F.adjust_contrast(result, 1.1)        # Torchvision-exact\n\n# Then fuse remaining ops (no contrast)\nresult = F.fused_augment(\n    result,\n    brightness_factor=1.0,                     # Identity (already applied)\n    contrast_factor=1.0,                       # Identity (already applied)\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>\u23f1\ufe0f Speed: Fast (3 kernel launches) \u26a1\u26a1</p>"},{"location":"contrast/#for-maximum-speed-not-exact","title":"For Maximum Speed (Not Exact)","text":"<p>If you don't need exact torchvision reproduction:</p> <pre><code>import triton_augment.functional as F\n\n# Single fused kernel - fastest!\nresult = F.fused_augment(\n    img,\n    brightness_factor=1.2,\n    contrast_factor=1.1,                       # Fast contrast (different from torchvision)\n    saturation_factor=0.9,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n</code></pre> <p>\u23f1\ufe0f Speed: Fastest (single fused kernel) \ud83d\ude80</p>"},{"location":"float16/","title":"Float16 (Half Precision) Support","text":"<p>Triton-Augment fully supports float16, providing memory savings and potential speedup on modern GPUs.</p>"},{"location":"float16/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nimport triton_augment as ta\n\n# Create float16 images\nimages = torch.rand(32, 3, 224, 224, device='cuda', dtype=torch.float16)\n\n# Apply fused transform (works seamlessly with float16)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # Output is also float16\n</code></pre>"},{"location":"float16/#benefits","title":"Benefits","text":"<p>\ud83d\udcbe Half the memory: Float16 uses 2x less VRAM, enabling: - Larger batch sizes - Higher resolution images - More models in memory</p> <p>\u26a1 Potential speedup: On Tesla T4, we observed ~1.3-1.4x speedup for large images (1024\u00d71024+)</p> <p>\u2705 Maintained accuracy: Data augmentation is robust to lower precision</p>"},{"location":"float16/#benchmark-results-tesla-t4","title":"Benchmark Results (Tesla T4)","text":"<p>Our measurements on Ultimate Fusion (all operations in one kernel):</p> Image Size Float32 Float16 Speedup 256\u00d7256 0.41 ms 0.47 ms 0.88x (slower) 512\u00d7512 0.48 ms 0.47 ms 1.03x 640\u00d7640 0.57 ms 0.49 ms 1.15x 1024\u00d71024 0.93 ms 0.72 ms 1.29x 1280\u00d71280 1.27 ms 0.93 ms 1.36x <p>Conclusion: Float16 provides meaningful speedup for large images (600\u00d7600+), but offers minimal benefit for small images.</p> <p>\ud83d\udca1 Your mileage may vary: Run <code>examples/benchmark_triton.py</code> to measure on your GPU.</p>"},{"location":"float16/#when-to-use-float16","title":"When to Use Float16","text":""},{"location":"float16/#use-float16-when","title":"\u2705 Use Float16 When:","text":"<ul> <li>Training with mixed precision (<code>torch.cuda.amp</code>)</li> <li>Memory constrained: Need to fit larger batches or higher resolution images</li> <li>Large images: 600\u00d7600+ where float16 shows speedup (based on T4 benchmarks)</li> </ul>"},{"location":"float16/#skip-float16-when","title":"\u274c Skip Float16 When:","text":"<ul> <li>Small images: &lt; 512\u00d7512 (minimal or negative speedup on T4)</li> <li>CPU-only training: Float16 is GPU-specific</li> <li>Debugging: Float32 is easier to inspect</li> </ul>"},{"location":"float16/#precision-considerations","title":"Precision Considerations","text":"<p>Float16 results will differ slightly from float32 due to reduced precision. This is expected and acceptable for data augmentation:</p> <ul> <li>Models are robust to small input perturbations</li> <li>Augmentation inherently introduces variation</li> <li>Training with mixed precision is a standard practice</li> </ul>"},{"location":"float16/#usage-example","title":"Usage Example","text":"<p>With mixed precision training:</p> <pre><code>from torch.cuda.amp import autocast, GradScaler\nimport triton_augment as ta\n\ntransform = ta.TritonFusedAugment(...)\nscaler = GradScaler()\n\nfor images, labels in loader:\n    with autocast():  # Images automatically converted to float16\n        images = images.cuda()\n        augmented = transform(images)\n        output = model(augmented)\n        loss = criterion(output, labels)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre> <p>Manual float16 conversion:</p> <pre><code># Convert to float16 for memory savings\nimages = images.half().cuda()\naugmented = transform(images)  # Stays in float16\n</code></pre>"},{"location":"float16/#benchmarking","title":"Benchmarking","text":"<p>Compare float16 vs float32 performance:</p> <pre><code>import torch\nimport triton_augment as ta\nfrom triton.testing import do_bench\n\nbatch = 32\nimg_fp32 = torch.rand(batch, 3, 224, 224, device='cuda', dtype=torch.float32)\nimg_fp16 = img_fp32.half()\n\ntransform = ta.TritonFusedAugment(\n    crop_size=112, brightness=0.2, saturation=0.2\n)\n\n# Benchmark\ntime_fp32 = do_bench(lambda: transform(img_fp32))\ntime_fp16 = do_bench(lambda: transform(img_fp16))\n\nprint(f\"Float32: {time_fp32:.3f} ms\")\nprint(f\"Float16: {time_fp16:.3f} ms\")\nprint(f\"Speedup: {time_fp32/time_fp16:.2f}x\")\n</code></pre>"},{"location":"float16/#why-float16-can-be-faster","title":"Why Float16 Can Be Faster","text":"<p>Float16 benefits come from: 1. Memory bandwidth: Half the data to transfer (2 bytes vs 4 bytes per value) 2. Cache efficiency: More data fits in GPU caches 3. GPU hardware: Modern GPUs have specialized float16 units</p> <p>Note: Speedup varies by GPU architecture and operation complexity. Always benchmark on your specific hardware.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>PyTorch &gt;= 2.0.0</li> <li>Triton &gt;= 2.0.0</li> <li>CUDA-capable GPU</li> </ul>"},{"location":"installation/#installation-from-pypi-recommended","title":"Installation from PyPI (Recommended)","text":"<pre><code>pip install triton-augment\n</code></pre>"},{"location":"installation/#development-installation-from-source","title":"Development Installation (From Source)","text":"<p>For contributors or those who want to modify the code:</p> <pre><code>git clone https://github.com/yuhezhang-ai/triton-augment.git\ncd triton-augment\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#input-requirements","title":"Input Requirements","text":"<p>Input Requirements</p> <ul> <li>Range: Pixel values must be in <code>[0, 1]</code> (use <code>transforms.ToTensor()</code> if loading from PIL)</li> <li>Device: GPU only (CPU tensors are automatically moved to CUDA)</li> <li>Shape: Supports both 3D <code>(C, H, W)</code> and 4D <code>(N, C, H, W)</code> tensors (automatic batching)</li> <li>Dtype: <code>float32</code> or <code>float16</code></li> </ul>"},{"location":"installation/#first-run-behavior","title":"First Run Behavior","text":"<p>On first use, Triton will compile kernels for your GPU (~1-2 seconds per image size with default config). This is normal and only happens once per GPU and image size.</p> <p>Optional: Cache Warm-Up</p> <p>To avoid compilation delays during training, you can optionally warm up the cache after installation:</p> <pre><code>python -m triton_augment.warmup\n</code></pre> <p>For more details and auto-tuning optimization, see the Auto-Tuning Guide.</p>"},{"location":"installation/#what-to-expect","title":"What to expect","text":"<ul> <li>First import: Helpful message about auto-tuning status (can be suppressed with <code>TRITON_AUGMENT_SUPPRESS_FIRST_RUN_MESSAGE=1</code>)</li> <li>First use of each image size: ~1-2 seconds (kernel compilation)</li> <li>Subsequent uses: Instant (kernels are cached)</li> </ul>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Should work without errors\nimg = torch.rand(4, 3, 224, 224, device='cuda')\ntransform = ta.TritonColorJitterNormalize(brightness=0.2)\nresult = transform(img)\nprint(\"\u2705 Installation successful!\")\n</code></pre>"},{"location":"quickstart/","title":"Quick Start Guide","text":""},{"location":"quickstart/#basic-usage-ultimate-fusion-recommended","title":"Basic Usage: Ultimate Fusion (Recommended) \ud83d\ude80","text":"<p>The fastest way to use Triton-Augment - fuse ALL augmentations in a single kernel:</p> <pre><code>import torch\nimport triton_augment as ta\n\n# Create a batch of images on GPU\nimages = torch.rand(32, 3, 224, 224, device='cuda')\n\n# Replace torchvision Compose (7 kernel launches)\n# With Triton-Augment (1 kernel launch - significantly faster!)\ntransform = ta.TritonFusedAugment(\n    crop_size=112,\n    horizontal_flip_p=0.5,\n    brightness=0.2,\n    contrast=0.2,\n    saturation=0.2,\n    grayscale_p=0.1,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\n# Apply transformation\naugmented = transform(images)  # Single kernel launch for ALL operations!\n</code></pre> <p>What it does:</p> <ul> <li>RandomCrop (112\u00d7112)</li> <li>RandomHorizontalFlip (50% probability)</li> <li>ColorJitter (brightness, contrast, saturation)</li> <li>Normalize</li> </ul> <p>Performance: Up to 12x faster on large images (8.1x average on Tesla T4, scales dramatically with image size)</p>"},{"location":"quickstart/#other-fusion-options","title":"Other Fusion Options","text":""},{"location":"quickstart/#pixel-only-fusion","title":"Pixel-Only Fusion","text":"<p>If you don't need geometric operations (crop/flip), use pixel fusion:</p> <pre><code># Fuse color jitter + normalize (single kernel)\ntransform = ta.TritonColorJitterNormalize(\n    brightness=0.2,\n    saturation=0.2,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225)\n)\n\naugmented = transform(images)  # Faster, single fused kernel\n</code></pre>"},{"location":"quickstart/#geometric-only-fusion","title":"Geometric-Only Fusion","text":"<p>If you only need crop + flip:</p> <pre><code># Fuse crop + flip (single kernel)\ntransform = ta.TritonRandomCropFlip(size=112, horizontal_flip_p=0.5)\n\naugmented = transform(images)  # ~1.5-2x faster\n</code></pre>"},{"location":"quickstart/#individual-operations","title":"Individual Operations","text":"<p>For maximum control, use individual operations (fixed parameters):</p> <pre><code>import triton_augment.functional as F\n\nimg = torch.rand(4, 3, 224, 224, device='cuda')\n\n# Geometric operations\ncropped = F.crop(img, top=20, left=30, height=112, width=112)\nflipped = F.horizontal_flip(img)\n\n# Color operations \nbright = F.adjust_brightness(img, 1.2)\nsaturated = F.adjust_saturation(img, 0.9)\nnormalized = F.normalize(img, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n</code></pre> <p>Or use transform classes (random augmentations):</p> <pre><code>import triton_augment as ta\n\n# Individual transforms\ncrop = ta.TritonRandomCrop(112)\nflip = ta.TritonRandomHorizontalFlip(p=0.5)\njitter = ta.TritonColorJitter(brightness=0.2)\nnormalize = ta.TritonNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n</code></pre>"},{"location":"quickstart/#training-integration","title":"Training Integration","text":"<p>Recommended Pattern: Load data on CPU (fast async I/O), augment on GPU (fast batch processing)</p> <pre><code>import torch\nimport triton_augment as ta\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Step 1: CPU data loading with workers\ntrain_dataset = datasets.CIFAR10(\n    './data', train=True,\n    transform=transforms.ToTensor()  # Only ToTensor on CPU\n)\ntrain_loader = DataLoader(\n    train_dataset, batch_size=128,\n    num_workers=4, pin_memory=True  # Fast async loading!\n)\n\n# Step 2: GPU augmentation transform\naugment = ta.TritonFusedAugment(\n    crop_size=28, horizontal_flip_p=0.5,\n    brightness=0.2, saturation=0.2,\n    mean=(0.4914, 0.4822, 0.4465),\n    std=(0.2470, 0.2435, 0.2616)\n)\n\n# Step 3: Apply in training loop\nfor images, labels in train_loader:\n    images, labels = images.cuda(), labels.cuda()\n    images = augment(images)  # All ops in 1 kernel! \ud83d\ude80\n    # ... rest of training ...\n</code></pre> <p>Why This Pattern:</p> <ul> <li>\u2705 Fast async data loading: <code>num_workers &gt; 0</code> for CPU parallelism</li> <li>\u2705 Fast GPU batch processing: All augmentations in 1 fused kernel</li> <li>\u2705 Different parameters per sample: Each image gets different random parameters (default)</li> <li>\u2705 Best of both worlds: CPU for I/O, GPU for compute</li> <li>\u2705 Kernel fusion: No intermediate memory allocations</li> <li>\u2705 Large batch advantage: Speedup increases with batch size</li> </ul> <p>Note: Set <code>same_on_batch=True</code> if you want all images to share the same random parameters.</p> <p>\ud83d\udca1 Pro Tip: Apply Triton-Augment transforms AFTER moving tensors to GPU for maximum performance!</p> <p>Full Examples: See <code>examples/train_mnist.py</code> and <code>examples/train_cifar10.py</code> for complete training scripts with neural networks.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Batch Behavior - Understand random parameter handling</li> <li>Contrast Notes - Fast contrast vs torchvision-exact</li> <li>Auto-Tuning - Optional performance optimization</li> <li>Float16 Support - Use half-precision for 1.3-2x additional speedup</li> <li>API Reference - Complete API documentation</li> </ul>"}]}