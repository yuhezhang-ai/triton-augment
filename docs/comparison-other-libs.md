# Comparison with other GPU-based data transforms library

## ğŸ“Š Triton-Augment vs DALI vs Kornia

| Feature                      | **Triton-Augment** (yours)                                                                  | **NVIDIA DALI**                                                                      | **Kornia**                                          |
| ---------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | --------------------------------------------------- |
| **Primary goal**             | Fast, fused GPU augmentations for training                                                  | End-to-end input pipeline (decode â†’ resize â†’ augment)                                | Differentiable augmentations in pure PyTorch        |
| **Fused ops**                | âœ”ï¸ Crop + flip + brightness + contrast + saturation + grayscale + normalize (single kernel) | âš ï¸ Only some fusions (e.g., `crop_mirror_normalize`); color ops are separate kernels | âŒ No fusion â€” each op is a separate CUDA/PyTorch op |
| **Per-sample random params** | âœ”ï¸ Built-in, torchvision-style API                                                          | âœ”ï¸ Supported (via feeding random tensors), but more manual                           | âœ”ï¸ Built-in                                         |
| **Ease of use**              | âœ”ï¸ Simple, torchvision-like                                                                 | âš ï¸ Steeper learning curve (pipeline graph)                                           | âœ”ï¸ Very easy (just PyTorch ops)                     |
| **Supported ops**            | âš ï¸ Limited for now (crop, flip, color jitter, normalize, grayscale)                         | âœ”ï¸ Huge library (decode, resize, warp, color, video, audio)                          | âœ”ï¸ Wide set (geometry, color, filtering, keypoints) |
| **Performance**              | ğŸš€ Very fast for augmentation (1 fused kernel for all ops)                                  | ğŸš€ Fast for full pipelines (GPU decode/resize), but augmentation uses multiple kernels (less fusion) | âš ï¸ Moderate (PyTorch kernels, multiple launches)    |
| **Integration**              | PyTorch training pipelines                                                                  | PyTorch, TensorFlow, JAX                                                             | PyTorch only                                        |
| **CPU preprocessing**        | âŒ None (expects tensors already on GPU)                                                     | âœ”ï¸ Hardware-accelerated decode/resize possible                                       | âœ”ï¸ Built on top of PyTorch                                          |
| **Autograd support**         | âŒ Not needed (augmentations only)                                                           | âŒ Most ops are not differentiable                                                    | âœ”ï¸ Yes (Kornia is differentiable by design)         |
| **Production readiness**     | âš ï¸ Early-stage (fast but limited scope)                                                     | âœ”ï¸ Mature, used in industry                                                          | âœ”ï¸ Mature                                           |

---

## ğŸ“ Notes

* **Triton-Augment is not a replacement for DALI or Kornia.**
  Itâ€™s a *small, focused* library aimed at speeding up a few high-impact augmentations via kernel fusion.

* **DALI is still the best choice** if the bottleneck is decode/resize or you need full data pipeline acceleration. However, for augmentation-only workloads (data already on GPU as tensors), Triton-Augment is faster due to higher kernel fusion.

* **Kornia is best** if you need differentiable augmentation or a wide variety of transforms.

* **Our advantage**:
  For the operations that are supported, **our one-kernel design beats both** in raw speed and simplicity.

* **Our limitation**:
  Fewer ops, no CPU pipeline, not designed for everything â€” just the common path.

